{"meta":{"title":"天外飞猪的博客","subtitle":null,"description":"人为什么越长大越孤单？\n答:内心中有秘密,无法诉说\n","author":"fbZhu","url":"http://yoursite.com"},"pages":[{"title":"tags","date":"2018-04-04T07:35:47.000Z","updated":"2018-04-04T07:44:50.377Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"kafka消息","slug":"kafka消息","date":"2018-04-11T05:39:08.000Z","updated":"2018-04-11T05:43:20.583Z","comments":true,"path":"2018/04/11/kafka消息/","link":"","permalink":"http://yoursite.com/2018/04/11/kafka消息/","excerpt":"","text":"带着以下两点疑问，进行kafka server的Log管理源码的分析： producer遇到 NOT_LEADER_EXCEPTION 是在何时产生的 消息是如何在ISR中备份的 下面以从上往下的方式对一条消息写入磁盘的全链路进行分析 KafkaApis kafka apis反映出kafka broker server可以提供哪些服务，broker server主要和producer，consumer，controller有交互，搞清这些api就清楚了broker server的所有行为 handleProducerRequest 该方法用于处理Client的producer请求，ApiKeys = 0 从 RequestChannel 中获取请求，然后根据acks规则进行反馈 写入磁盘的动作在replicaManager.appendRecords中完成 ack规则 acks 规则 0 producer不需要等待ack,server收到消息后直接反馈 1 leader成功写入后反馈给producer -1 (kafka-client中配置为&quot;all&quot;) 在ISR中所有replicas都写入消息后才进行反馈 ReplicaManager.appendMessages 先将消息写入leader的log中(正常情况就是当前这个broker) 将消息写到ISR的其他备份中 超时或者ack规则满足时进行反馈操作(执行回调函数：responseCallback) //写到leader的logval localProduceResults = appendToLocalLog(internalTopicsAllowed, entriesPerPartition, requiredAcks)if (delayedRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;//acks=-1时需要创建 delayedProduce实现消息的备份val produceMetadata = ProduceMetadata(requiredAcks, produceStatus)val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback)val producerRequestKeys = entriesPerPartition.keys.map(new TopicPartitionOperationKey(_)).toSeqdelayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)&#125; else &#123;val produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)responseCallback(produceResponseStatus)&#125; ReplicaManager.appendToLocalLog 当topic为内部topic(即__consumer_offsets)，并且不允许往内部类中写消息时，抛出 InvalidTopicException val info = partitionOpt match &#123; case Some(partition) =&gt; partition.appendRecordsToLeader(records, requiredAcks) case None =&gt; throw new UnknownTopicOrPartitionException(&quot;Partition %s doesn&apos;t exist on %d&quot; .format(topicPartition, localBrokerId))&#125; Partition.appendRecordsToLeader判断当前broker是否是parition的leaderdef getReplica(replicaId: Int = localBrokerId): Option[Replica] = Option(assignedReplicaMap.get(replicaId))def leaderReplicaIfLocal: Option[Replica] = leaderReplicaIdOpt.filter(_ == localBrokerId).flatMap(getReplica) assignedReplicaMap：用于存放每个partition对应的leader已经replicas 通过比较leaderId与localBrokerId，判断当前broker是否就是leader 如果不满足，那么将抛出NotLeaderForPartitionException 备份数不满足条件的消息不会写入 val log = leaderReplica.log.get val minIsr = log.config.minInSyncReplicas val inSyncSize = inSyncReplicas.size// acks为all的时候才会进行判断 if (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == -1) &#123; throw new NotEnoughReplicasException(&quot;Number of insync replicas for partition %s is [%d], below required minimum [%d]&quot; .format(topicPartition, inSyncSize, minIsr)) &#125; Log.append 返回的格式为LogAppendInfo,包含第一条及最后一条offset信息 找到该Partition在当前broker上面最新的segment，如果塞不进去 就新建一个segment 将消息添加到segment中 更新segment的LogEndOffset为最新添加消息的lastOffset + 1ByteBufferMessageSetappend方法中传入的消息集的数据结构为ByteBufferMessageSet (注：在0.10.2.0版本后引入新的结构：MemoryRecords)，父类为Messageset,其结构如下图所示： 一个有效的MessageSet的最小长度为12字节 Message的组成Message在magic不同的情况下有不同的结构：迭代器的实现internalIterator是ByteBufferMessageSet实现的迭代器，迭代单位是MessageAndOffset magic的值是由server中Log的配置属性：message.format.version决定的，0.10.0之前的版本的magic值为0，之后的版本为1 迭代的过程也是对消息的有效性的检验过程： ByteBuffer(对应MessageSet)的长度是否&lt; 12 消息体(对应Message)的长度是否 &lt; 魔术为0时Message的头部长度(4+1+1+8+4 = 18) ByteBuffer的长度是否&lt;消息体的长度(否则就表明消息不完整) 满足以上条件的消息一定是无法继续迭代的 由于消息的载体实现的是ByteBuffer，那我们就从Buffer的操作的角度来看看message和offset是如何被迭代取出来的： 假设当前接收到一个新的ByteBuffer，下面进行迭代： 获取buffer的片段(第一次算是拷贝)：topIter = buffer.slice() 获取offset(获取前八个字节)： offset = topIter.getLong() 获取size(获取紧接着的四个字节)：size = topIter.getInt() 获取message(当前position指向message的开始位置，截取后续size大小的就可以得到message)：message = topIter.slice; message.limit(size) 将topIter的position指向下一条MessageSet: topIter.position(topIter.position + size) LogAppendInfo的生成LogAppendInfo主要包含四个属性，用于描述message set: firstOffest：第一条消息的offset: lastOffset：最后一条消息的offset: maxTimestamp：消息里面包含的最大时间戳: offsetOfMaxTimestamp：最大时间戳消息的offset analyzeAndValidateMessageSet方法实现了LogAppendInfo的生成，根据上面提到的迭代器对MessageSet中的消息进行迭代处理，找出并记录offset和timestamp信息，此外也对每条消息进行检验： 每条消息的大小不能超过max.message.bytes所定义的 每条消息必须通过循环冗余校验 消息的进一步校验对消息的进一步校验及转化是在validateMessagesAndAssignOffsets方法中完成的。该方法的参数中涉及到一些概念： 1.topic清理策略log.cleanup.policy配置项控制着消息在segment中持久化的策略，目前有两种策略供选择：delete和compact，默认选项为delte。 delete的策略很好理解，就是当segment时间或者大小到期了就删除。 compact的策略是为了满足系统灾后恢复的需求，该选项是针对topic的，比如存在某个topic：email_topic用于存储用户变更的email信息，key=userId,value=emailAddress，compact操作就是在日志删除过程中保留每个userId最新的数据，如果系统崩溃了也能通过该topic获得用户最新修改的email地址。下面的图很好的诠释了这种操作： 2. 版本与magicValue 由于kafka的版本更新速度比较快，为了能让新的server版本兼容老的client版本以及server的滚动升级的实现，提供了message.format.version配置项定义consumer及Producer的API版本。 不同的API版本对应不同的magicValue，其中0.9.0.X版本之前(包括该版本)的magicValue未0，之后的都为1 当前我们生产环境使用的API版本为0.10.0.0，server的版本为0.10.1.1 3. 解析非压缩消息 producer可以选择是否对消息进行压缩 message.timestamp.type：时间戳类型: CreateTime：消息的创建时间&lt;默认值&gt;: LogAppendTime：添加到log的时间 message.timestamp.difference.max.ms：最大时间间隔,表示收到的消息中的时间戳与当前时间的差的最大容忍值。 对非压缩消息的进一步处理的过程依然是ByteBuffer的操作过程：/*主要目的是获取maxTimestamp和offsetOfMaxTimestamp*/var messagePosition = 0var maxTimestamp = Message.NoTimestampvar offsetOfMaxTimestamp = -1L//将position位置存到mark中buffer.mark()while (messagePosition &lt; sizeInBytes - MessageSet.LogOverhead) &#123; buffer.position(messagePosition) //offsetCounter是server维护的下一个offset的值 buffer.putLong(offsetCounter.getAndIncrement()) val messageSize = buffer.getInt() val messageBuffer = buffer.slice() messageBuffer.limit(messageSize) val message = new Message(messageBuffer) //以上的操作能获取到MessageSet中的一条message if (message.magic &gt; Message.MagicValue_V0) &#123; validateTimestamp(message, now, timestampType, timestampDiffMaxMs) if (message.timestamp &gt; maxTimestamp) &#123; maxTimestamp = message.timestamp offsetOfMaxTimestamp = offsetCounter.value - 1 &#125; &#125; //将buffer的position移到后一条消息的头部 messagePosition += MessageSet.LogOverhead + messageSize&#125;//根据mark恢复positionbuffer.reset() 经过上面的操作，MessageSet中的offset被server重新设置了，并且maxTimestamp之类的信息重新收集了。这些信息在append操作中会被使用：validMessages = validateAndOffsetAssignResult.validatedMessagesappendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestampappendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.offsetOfMaxTimestampappendInfo.lastOffset = offset.value - 1 其中：: validMessages就是处理完后的MessageSet: offset是上面使用到的offsetCounter，即下一个offset值。为什么要-1?因为上面执行了getAndIncrement()操作，因此当前的offset指向的依然是下一个offset值 4.压缩消息的处理 producer的压缩策略必须与broker一致，如果不匹配那么将不会解压缩消息 以下几种情况不会对压缩消息进行解压处理： topic指定了压缩策略，但是发送的消息中没有key(会报错) 消息体与server的magic值不一致 找到合适的segment 一个topic由多个parition组成，每个partition又存在多个segment 每个segment的大小由log.segment.bytes控制,下面是segment大小设置为1024的broker上某个partiton的Log情况：[test-0]$ du -smh *0 00000000000000017699.index4.0K 00000000000000017699.log4.0K 00000000000000017699.timeindex0 00000000000000017724.index4.0K 00000000000000017724.log0 00000000000000017724.timeindex 每个log的命名规则是取该log中 下一个 offset的值(logEndOffset); 以index结尾的offset index文件的作用是将offset映射到物理文件中 timeindex文件将时间戳与segment中的逻辑offset联系起来 满足以下几个条件之一时会创建新的segment： 当前segment容不下最新的消息 当前segment非空，并且达到了log.roll.hours的时间 offset 或者 time index满了 下面是写消息时创建出新的segment时server的输出日志：[2017-04-05 17:14:29,776] DEBUG Rolling new log segment in test-11 (log_size = 1006/1024&#125;, index_size = 0/1310720, time_index_size = 1/873813, inactive_time_ms = 184193/604800000). (kafka.log.Log) 将消息添加到LogSegment中LogSegment的参数主要有：: log：File类型的MessageSet，该Set中包含一个FileChannel，能够从ByteBuffer中读取数据: index: OffsetIndex，逻辑Offset到物理文件位置的索引: timeIndex：TimeIndex，时间戳到物理位置的索引: baseOffset：第一个offset 写消息的操作：将AppendInfo中的消息写入到LogSegment中的FileChannel中def writeFullyTo(channel: GatheringByteChannel): Int = &#123; buffer.mark() var written = 0 while (written &lt; sizeInBytes) written += channel.write(buffer) buffer.reset() written&#125; _size.getAndAdd(written) FileMessageSet的search操作根据offset或者时间戳是从Log中读取消息的常见方法。FileMessageSet提供对应的两个方法：searchForOffsetWithSize和searchForTimestamp。前者是从FileMessageSet的给定位置往后搜索第一个大于等于目标offset的消息，返回的是Offset&amp;Position，后者是从给定位置往后搜索第一个时间戳大于等于目标时间戳的消息，返回Timestamp&amp;Offset。 下图描述的是从0开始搜索offset≥1003的消息： 写OffsetIndexOffsetIndex中并不会记录所有Offset的映射关系，写入Index的时机由index.interval.bytes参数(default：4096)控制，当segment中积累的消息数量大于该参数时，会将此次写入segment中的MessageSet的第一个消息的offset写入OffsetIndex中：if(bytesSinceLastIndexEntry &gt; indexIntervalBytes) &#123; index.append(firstOffset, physicalPosition) timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp) bytesSinceLastIndexEntry = 0&#125;bytesSinceLastIndexEntry += messages.sizeInBytes index.append的具体操作如下：if (_entries == 0 || offset &gt; _lastOffset) &#123; mmap.putInt((offset - baseOffset).toInt) mmap.putInt(position) _entries += 1 _lastOffset = offset&#125; 上面的操作是将offset相对这个Segmet的baseOffset的偏移值以及物理地址填入到OffsetIndex中定义的MappedByteBuffer。 之所以使用相对偏移值是出于节省存储空间的考虑，相对偏移值只需要4位空间就能存储，而MessageSet中的offset占8位。 写TimeIndex在每次执行append操作时，TimeIndex记录的是最大时间戳及其对应的offset的索引。if (timestamp &gt; lastEntry.timestamp) &#123; mmap.putLong(timestamp) mmap.putInt((offset - baseOffset).toInt) _entries += 1&#125; 写入Buffer中的是时间戳以及相对偏移量 更新LogEndOffset上面的分析中提到过每个Log都维护了一个记录下一个offset的变量——nextOffsetMetadata,该变量是LogOffsetMetadata类型的：: messageOffset：绝对偏移值: segmentBaseOffset：LogSegment的baseOffset值: relativePositionInSegment：LogSegment的大小(字节数)，根据字节数可以定位到这条消息在segment中的物理位置 每次append操作都会执行：nextOffsetMetadata = new LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size.toInt)其中： messageOffset为 AppendInfo.lastOffset+1 activeSegment是当前可用的segment activeSegment.size由_size.get()，这个_size正是在上面往segment中写消息时进行更新的，每次增加的值是写入channel中的字节数。 下面这张图生动描述了这个更新的操作： 绿色的代表activeSegment，当前的nextOffsetMetadata为10(指的是messageOffset) 写入10条消息后，nextOffsetMetadata更新为21 再次写入10条消息后，更新为31 再来10条消息，无法写入，执行roll操作新建一个segment,messageOffset值未改变，不过activeSegment变化了 将10条消息写入新的segment中，更新为41","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"Kafka Coordinator实现细节","slug":"Kafka-Coordinator实现细节","date":"2018-04-04T07:29:55.000Z","updated":"2018-04-04T07:44:07.656Z","comments":true,"path":"2018/04/04/Kafka-Coordinator实现细节/","link":"","permalink":"http://yoursite.com/2018/04/04/Kafka-Coordinator实现细节/","excerpt":"","text":"GroupCoordinator 每个kafka server在启动的时候会创建一个GroupCoordinator用于管理group以及consumer的offset fetch/commit 在创建GroupCoordinator实例时不仅需要brokerId、group以及offset config，还需要传入replicaManager，其作用是 GroupMetadataManagerGroupMetadataManager是GroupCoordinator最重要的组成部分，其作用是管理group的元信息(状态、成员、提交的offset信息)以及作为coordinator的broker所分配到的分区信息，主要成员有4个： groupMetadataCache：存储group与GroupMetadata的cache loadingPartitions：”__consumer_offsets”中正在被当前coordinator加载的分区 ownedPartitions：”__consumer_offsets”中分配到当前coordinator的分区(即该broker是这些分区的leader) scheduler：删除过期offset以及group元数据的定时任务(执行间隔由offsets.retention.check.interval.ms参数控制，默认为10分钟) __consumer_offsets__consumer_offsets是用于存储消费者消费信息的topic，存储的消息由两部分组成 一部分是offset信息(kafka.coordinator.OffsetsMessageFormatter类型)的： [groupId,topic,partition]::[OffsetMetadata[offset,metadata],CommitTime ExprirationTime] 另一部分是group信息(kafka.coordinator.GroupMetadataMessageFormatter类型): groupId::[groupId,Some(consumer),groupState,Map(memberId -&gt; [memberId,clientId,clientHost,sessionTimeoutMs], ...-&gt;[]...)] group分配到哪个分区的策略在kafka cosumer中介绍过了。 作为一个特殊的topic，consumer_offsets也有replica的概念，并且其replica factor与其他topic保持一致。consumer_offsets上每个分区都对应一个leader，作为leader的broker上的GroupCoordinator会记录着分区上记录着的group以及offset信息。当leader(__consumer_offsets分布的leader)发生变化时，新的leader需要加载对应分区上的group以及offset信息。 server在处理LeaderAndIsrRequest时会对__consumer_offsets的分区做出入境 操作：def onLeadershipChange(updatedLeaders: Iterable[Partition], updatedFollowers: Iterable[Partition]) &#123; updatedLeaders.foreach &#123; partition =&gt; if (partition.topic == Topic.GroupMetadataTopicName) coordinator.handleGroupImmigration(partition.partitionId) &#125; updatedFollowers.foreach &#123; partition =&gt; if (partition.topic == Topic.GroupMetadataTopicName) coordinator.handleGroupEmigration(partition.partitionId) &#125;&#125; 小实验“G3”这个group根据hash映射到分区2上，当前的ISR为：Topic: __consumer_offsets Partition: 2 Leader: 1 Replicas: 1,3,2 Isr: 2,3,1 接下来，关闭broker1，导致broker3成为了新的leader。 broker3执行入境 操作，加载分区2上面的的offset以及group信息：[Group Metadata Manager]: Loading offsets and group metadata from [__consumer_offsets,2][Group Metadata Manager]: Loaded group metadata for group G3 with generation 1[Group Metadata Manager]: Loaded group metadata for group G3 with generation 2[Group Metadata Manager]: Loaded group metadata for group G3 with generation 3[Group Metadata Manager]: Loaded group metadata for group G3 with generation 4[Group Metadata Manager]: Loaded offset [OffsetMetadata[3448,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-0.[Group Metadata Manager]: Loaded offset [OffsetMetadata[3441,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-3.[Group Metadata Manager]: Loaded offset [OffsetMetadata[3257,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-5.[Group Metadata Manager]: Loaded offset [OffsetMetadata[3382,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-2.[Group Metadata Manager]: Loaded offset [OffsetMetadata[3397,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-4.[Group Metadata Manager]: Loaded offset [OffsetMetadata[3163,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-1.[GroupCoordinator 3]: Loading group metadata for G3 with generation 4[Group Metadata Manager]: Finished loading offsets from [__consumer_offsets,2] in 21 milliseconds. 在loadGroupsForPartition方法中通过使用Map确保加载每个group最新generation的信息 在执行入境操作之前，分区2被添加到loadingPartitions中，表示coordinator正在加载该分区里面的信息，这个阶段如果有groupId在loadingPartitions之内的消费请求进来，是无法响应的；处理完后，分区2被添加到ownedPartitions中。 保存group元数据方法定义如下：def prepareStoreGroup(group: GroupMetadata, groupAssignment: Map[String, Array[Byte]], responseCallback: Errors =&gt; Unit): Option[DelayedStore] groupAssignment是group中member的分区分配 返回值是DelayedStore，这并不是一个DO类型的延迟任务，只适用于存放消息的临时媒介，方便后续往replicas中写Log用的。 生成消息第一步是生成能往Log(确切的说是写入__consumer_offsets中该group对应的partition中)中写入的ByteBufferMessageSet(消息写入磁盘及备份实现分析),key为groupId,value为member以及sessionTimeout，即上面所说的group信息。 设置appendLog回调函数该回调函数作为参数传入到ReplicaManager.replicaMessages，对append操作的结果进行处理，最主要的就是状态的转换：case Errors.UNKNOWN_TOPIC_OR_PARTITION | Errors.NOT_ENOUGH_REPLICAS | Errors.NOT_ENOUGH_REPLICAS_AFTER_APPEND =&gt; Errors.GROUP_COORDINATOR_NOT_AVAILABLEcase Errors.NOT_LEADER_FOR_PARTITION =&gt; Errors.NOT_COORDINATOR_FOR_GROUPcase Errors.REQUEST_TIMED_OUT =&gt; Errors.REBALANCE_IN_PROGRESScase Errors.MESSAGE_TOO_LARGE | Errors.RECORD_LIST_TOO_LARGE | Errors.INVALID_FETCH_SIZE =&gt; Errors.UNKNOWN 执行storeGroup回调函数在doSyncGroup中定义了一个用于处理leader的SyncGroupRequest的回调函数：group synchronized &#123; if (group.is(AwaitingSync) &amp;&amp; generationId == group.generationId) &#123; if (error != Errors.NONE) &#123; resetAndPropagateAssignmentError(group, error) maybePrepareRebalance(group) &#125; else &#123; setAndPropagateAssignment(group, assignment) group.transitionTo(Stable) &#125; &#125;&#125; 因为在等待回调函数被执行的过程中，可能会有新的member加入，这样的话就无法保证group的状态以及generation不会变化的。 往replicas写group数据调用GroupMetadataManager的store方法，将DelayedStore中的messageSet以及回调函数传入到replicaManager的appendMessages方法中replicaManager.appendMessages( config.offsetCommitTimeoutMs.toLong, config.offsetCommitRequiredAcks, true, // allow appending to internal offset topic delayedStore.messageSet, delayedStore.callback) 保存offset commitprepareStoreOffsets方法与prepareStoreGroup基本相像：def prepareStoreOffsets(group: GroupMetadata, consumerId: String, generationId: Int, offsetMetadata: immutable.Map[TopicPartition, OffsetAndMetadata], responseCallback: immutable.Map[TopicPartition, Short] =&gt; Unit): Option[DelayedStore] 返回的依然是个DelayedStore。组装的MessageSet中,key为[groupId, topic, partition]，value为OffsetMetadata，写入的partition由groupId确定。 offsetCommit回调函数 GroupMetadata为offset commit创建了两个Cache：offsets以及pendingOffsetCommits，consumer提交的offset先存放到pending中，然后根据一定的状态来决定是否移到offsets中。 如果offsetCommit执行结束后group依旧存活那个根据是否有错误对cache执行不同的操作： 没有错误码，那么就将offset写入到offsets中，并从pendingOffsetCommits中移除； 有错误，那么仅仅将offset从pendingOffsetCommits中移除； offset以及group元数据的清理工作GroupMetadataManager在启动时会开启一个定时执行的清理线程：”delete-expired-group-metadata”，该线程的主要工作是清理__consumer_offsets中失效的offset以及可删除的group信息。 remove expire offset每个offset commit提交到server时，都会根据配置的保存时间来设置其失效时间，超过该时间的将会被清除掉。 首先，从cache中筛选出可清楚的offset集合：val expiredOffsets = offsets.filter &#123; case (topicPartition, offset) =&gt; offset.expireTimestamp &lt; startMs &amp;&amp; !pendingOffsetCommits.contains(topicPartition) &#125; offsets --= expiredOffsets.keySet expiredOffsets 执行完删除操作过后判断group是否可以判定为DEAD： group不包含member group的两个offsetCache都为空 立墓碑 当offset被移除或者group进入DEAD状态，都会在__consumer_offsets中留下一个墓碑。 对于group而言： 如果当前有member存在，那么其存在于__consumer_offsets中的数据是这样的： G6::[G6,Some(consumer),Stable,Map(client1-1ee1d482-c9fa-4617-860a-98d3a3d5a836 -&gt; [client1-1ee1d482-c9fa-4617-860a-98d3a3d5a836,client1,/10.45.48.129,120000])] 如果member为空，但是状态不是DEAD： G6::[G6,None,Empty,Map()] 如果group被该清理线程认为DEAD，该group信息不仅从groupMetadataCache中移除，还会在__cosnumer_offsets中留下一座墓碑： G6::NULL offset的立墓碑操作与group类似。 consumer与Coordinator的连接过程确定coordinator 在kafka cosumer中提到过consumer寻找coordinator的过程。 选择将哪个节点作为coordinator其实是由consumer client决定的，确切的说是向已连接的节点中随机选择一个最空闲的节点发送GroupCoordinatorRequest。(这里随机是指往一个空闲的随机broker发送请求，收到的response中分配到的coordinator是根据group找对__consumer_offsets对应分区的Leader)这里空闲的定义是：NetworkClient中处于inflight状态的请求数量少，下面是consumer寻找到coordinator的日志：[AbstractCoordinator] Sending coordinator request for group G6 to broker 10.45.4.10:9092[AbstractCoordinator] Received group coordinator response ClientResponse(receivedTimeMs=1495510047579, disconnected=false, request=ClientRequest(callback=..., request=RequestSend(header=&#123;api_key=10,api_version=0,correlation_id=0,client_id=client111&#125;, body=&#123;group_id=G6&#125;), createdTimeMs=1495510047472, sendTimeMs=1495510047577), responseBody=&#123;error_code=0,coordinator=&#123;node_id=1,host=10.45.4.9,port=9092&#125;&#125;) handleGroupCoordinatorRequestserver在接收到GroupCoordinatorRequest后： 根据groupId找到对应__consumer_offsets上的分区P 找到P对应的leader作为coordinator handleJoinGroup校验 简单校验： coordinator是否处于工作状态 groupId是否有效 coordinator是否负责该group sessionTimeoutMs是否合理 groupId是否在loadingPartitions中，如果在的话表明正在Rebalance中。 member校验：客户端ConsumerCoordinator发送的JoinGroupRequest 中的memberId永远都是空的，也就是说memberId是由server端进行设置的。如果Request中group是已知(存在于GroupMetadataManager.groupMetadataCache)的并且memberId非空，那么Server将拒绝这个请求。 响应请求 group有不同的状态，在不同的状态下有相应的响应joinGroupRequest方法 Deadgroup处于Dead状态表明该group中没有了成员，并且其GroupMetadata已被该coordinator移除,这个状态下对任何请求都是返回UnknownMemberIdException PreparingRebalance根据Request中memberId是否为空有两套处理逻辑： memberId为空：执行addMember操作 //新建memberId，格式为：clientId-UUID val memberId = clientId + &quot;-&quot; + group.generateMemberIdSuffixval member = new MemberMetadata(memberId, group.groupId, clientId, clientHost, rebalanceTimeoutMs, sessionTimeoutMs, protocolType, protocols)member.awaitingJoinCallback = callbackgroup.add(member.memberId, member)maybePrepareRebalance(group)member memberId不为空：执行updateMember操作 member.supportedProtocols = protocolsmember.awaitingJoinCallback = callbackmaybePrepareRebalance(group) 以上两个操作都不会触发PrepareRebalance操作，因为当前已经是PreparingReblance状态了。 AwaitingSync 这个状态表明coordinator已经发送了JoinGroupResponse了，正在等待leader发送分区分配的SyncGroupRequest. 这个时候如果收到一个memberId为空的JoinGroupRequest，表明group中有新的成员加入，除了要创建member信息添加到GroupMetadata中之外，还需要prepareRebalance，并将状态重新置为PreparingReblance 如果收到的memberId不为空，有两种情况： 该成员未收到之前发送过的JoinGroupResponse，这种情况就重新发送一个Response，leader和member分配不会改变的 这次的Request中改变了分区分配策略，因此需要执行updateMember操作，并且还需要执行PrepareRebalance操作将状态重新置为PreparingReblance。 Empty or Stable处理三种可能的joinGroup情形： 收到memberId为空的请求，group有新成员加入，执行addMember以及PrepareRebalce操作。 收到leader的joinGroup请求或者请求中的分配策略发生变化(何种场景下会leader会重发joinGroup请求嘞？分区变化时consumer是如何rejoin的) 其他情况(followers发送的没有内容变化的Join请求)说明followers可能没收到Response，因此重发Response。 handleSyncGroup在校验阶段如果发现该coordinator并不负责该group，则会反馈NotCoordinatorForGroupException。 能够正常响应SyncGroup请求的group状态为AwaitingSync和Stable，其中AwaitingSync状态下就是执行保存group数据 handleLeaveGroup主要有三步操作 1、从心跳DOP中移除心跳DOP(heartbeatPurgatory)中保存的是DelayedHeartbeat，该DO操作用于侦测member是否存活,成员变量包括group、member已经超时时间。当member要离开group时需要将该member对应的DO操作完成并移除掉。 2、从group中移除将member从其对应的GroupMetadata中的members中移除，然后根据当前group状态进行对应的处理： Stable or Empty：触发PrepareRebalance PreparingRebalance：complete掉joinPurgatory中该group的DJ操作 3、组装反馈信息其实consumer对LeaveGroupResponse不是很关心，因为不会重发。 handleFetchOffsetsconsumer是从coordinator维护的offset cache ：offsets中获取group已提交的offset信息的。 FetchRequest中包含group以及topic-partition信息，据此coordinator进行反馈： 如果group不存在 or group状态为DEAD，则返回的PartitionData中offset为-1(代表InvalidOffset) 如果未指定topic以及partition，那么就将offsets中所有的[topic-partition, commit offset]数据都反馈给consunmer 如果指定topic-partition，就将offsets中对应的数据反馈，找不到就反馈-1 Helper分区变化时consumer是如何rejoin的？消费者在长连接的时候，增加了分区的topic，client是如何感知到的呢，下面是实验过程记录： 10:33:08 开启consumer [AbstractCoordinator] Sending coordinator request for group G6 to broker 10.45.4.10:9092 (id: -2 rack: null)[AbstractCoordinator] Discovered coordinator 10.45.4.9:9092 (id: 2147483646 rack: null) for group G6.[ConsumerCoordinator] Revoking previously assigned partitions [] for group G6[ConcurrentMessageListenerContainer] partitions revoked:[][AbstractCoordinator] (Re-)joining group G6[AbstractCoordinator] Sending JoinGroup (&#123;group_id=G6,session_timeout=120000,member_id=,protocol_type=consumer,group_protocols=[&#123;protocol_name=range,protocol_metadata=java.nio.HeapByteBuffer[pos=0 lim=18 cap=18]&#125;]&#125;) to coordinator 10.45.4.9:9092 (id: 2147483646 rack: null)[AbstractCoordinator] Received successful join group response for group G6: [AbstractCoordinator] Successfully joined group G6 with generation 5[ConcurrentMessageListenerContainer] partitions assigned:[newOne-8, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3] 10:34:17 controller检测到partitoin变化： [AddPartitionsListener on 2]: Partition modification triggered &#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;8&quot;:[2,1],&quot;4&quot;:[1,3],&quot;11&quot;:[2,1],&quot;9&quot;:[3,2],&quot;5&quot;:[2,1],&quot;10&quot;:[1,3],&quot;6&quot;:[3,2],&quot;1&quot;:[1,2],&quot;0&quot;:[3,1],&quot;2&quot;:[2,3],&quot;7&quot;:[1,3],&quot;3&quot;:[3,2]&#125;&#125; for path /brokers/topics/newOne (kafka.controller.PartitionStateMachine$PartitionModificationsListener) 10:38:10 client rejoin [ConsumerCoordinator] Revoking previously assigned partitions [newOne-8, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3] for group G6[ConcurrentMessageListenerContainer] partitions revoked:[newOne-8, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3][AbstractCoordinator] &quot;(Re-)joining group G6&quot;[AbstractCoordinator] Sending JoinGroup (&#123;group_id=G6,session_timeout=120000,member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,protocol_type=consumer,group_protocols=[&#123;protocol_name=range,protocol_metadata=java.nio.HeapByteBuffer[pos=0 lim=18 cap=18]&#125;]&#125;) to coordinator 10.45.4.9:9092 (id: 2147483646 rack: null)[AbstractCoordinator] Received successful join group response for group G6: &#123;error_code=0,generation_id=6,group_protocol=range,leader_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,members=[&#123;member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=18 cap=18]&#125;]&#125;[AbstractCoordinator] Sending leader SyncGroup for group G6 to coordinator 10.45.4.9:9092 (id: 2147483646 rack: null): &#123;group_id=G6,generation_id=6,member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,group_assignment=[&#123;member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,member_assignment=java.nio.HeapByteBuffer[pos=0 lim=70 cap=70]&#125;]&#125;[AbstractCoordinator] Successfully joined group G6 with generation 6[ConsumerCoordinator] Setting newly assigned partitions [newOne-8, newOne-9, newOne-10, newOne-11, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3] for group G6[ConcurrentMessageListenerContainer] partitions assigned:[newOne-8, newOne-9, newOne-10, newOne-11, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3] 那么consumer到底是如何触发rejoin的呢，这个时间间隔有何讲究？ 一开始自己关注的点是rejoinNeeded什么时候被置为true 有三种情况下会被置为true： SyncGroupResponse中存在ERROR HeartbeatResponse中存在REBALANCE_IN_PROGRESS、ILLEGAL_GENERATION或UNKNOWN_MEMBER_ID的ERROR consumer leave group 然而从日志来看并没有出现错误，所以注意力转移到needRejoin方法中的其他判断条件：return subscriptions.partitionsAutoAssigned() &amp;&amp; (super.needRejoin() || subscriptions.partitionAssignmentNeeded()); partitionsAutoAssigned的条件是一直满足的，partitionAssignmentNeeded被置为true的场景有点多，排查起来比较费时费力。这时有个新的现象进入眼帘：经过多次试验后发现rejoin距离分区调整的时间间隔最长不超过5分钟！。而metadata.max.age.ms这个配置参数的默认值刚好是5分钟： 该配置项是client端强制刷新metadata的最长时间间隔。 因为kafka集群出现broker或者partition变化的时候是不会通知客户端的，因此客户端需要定期的去获取metadata的值。 客户端判断是否需要刷新metadata的方法：public synchronized long timeToNextUpdate(long nowMs) &#123; long timeToExpire = needUpdate ? 0 : Math.max(this.lastSuccessfulRefreshMs + this.metadataExpireMs - nowMs, 0); long timeToAllowUpdate = this.lastRefreshMs + this.refreshBackoffMs - nowMs; return Math.max(timeToExpire, timeToAllowUpdate);&#125; 其中metadataExpireMs就是5分钟【默认】。 ConsumerCoordinator为metadata添加了一个Listener监听其更新操作：private void addMetadataListener() &#123; this.metadata.addListener(new Metadata.Listener() &#123; @Override public void onMetadataUpdate(Cluster cluster) &#123; ... // check if there are any changes to the metadata which should trigger a rebalance if (subscriptions.partitionsAutoAssigned()) &#123; MetadataSnapshot snapshot = new MetadataSnapshot(subscriptions, cluster); if (!snapshot.equals(metadataSnapshot)) &#123; metadataSnapshot = snapshot; subscriptions.needReassignment(); &#125; &#125; &#125; &#125;);&#125; needReassignment()方法将needsPartitionAssignment置为true,这正是partitionAssignmentNeeded()方法所需要的。 how to prepare rebalance 在kafka cosumer中做个一个实验：consumer多次关闭重连后，partition将会在较长时间后才能分配到。GroupCoordinator做了什么才导致这样的现象发生嘞？ prepareReblance的代码不长，但是要搞懂到底做了些什么着实不易：private def prepareRebalance(group: GroupMetadata) &#123; // if any members are awaiting sync, cancel their request and have them rejoin if (group.is(AwaitingSync)) resetAndPropagateAssignmentError(group, Errors.REBALANCE_IN_PROGRESS) group.transitionTo(PreparingRebalance) info(&quot;Preparing to restabilize group %s with old generation %s&quot;.format(group.groupId, group.generationId)) val rebalanceTimeout = group.rebalanceTimeoutMs val delayedRebalance = new DelayedJoin(this, group, rebalanceTimeout) val groupKey = GroupKey(group.groupId) joinPurgatory.tryCompleteElseWatch(delayedRebalance, Seq(groupKey))&#125; DelayedOperation And DelayedOperationPurgatorypurgatory wiki DelayedJoin(简称DJ)是DelayedOperation(简称DO)的子类，DelayedOperationPurgatory(简称DOP)用于记录DO，并将超时的DO执行expired操作? DO用于执行延迟任务，参数只有一个超时时间，目前已有的实现类有： 比如DelayedFetch的操作允许Fetch等待一定数量的消息或者达到超时时间后再返回。 当DO完成给定的操作后，会调用onComplete方法(该方法需要子类实现)并且只会被调用一次, isComplete方法将返回true(通过原子变量AtomicBoolean实现)； forceComplete和tryComplete方法等能触发onComplete，其中前者已经在DO中实现了：def forceComplete(): Boolean = &#123; if (completed.compareAndSet(false, true)) &#123; // cancel the timeout timer cancel() onComplete() true &#125; else &#123; false &#125;&#125; 该方法将原子变量强行换成true(如果当前为false的话)，然后调用onComplete操作；后者需要子类实现：在判断是否达到complete条件后再调用forceComplete。 safeTryComplete是TryComplete的线程安全版： def safeTryComplete(): Boolean = &#123; synchronized &#123; tryComplete() &#125;&#125; 如果DO超时，将调用onExpiration操作(0.10.1.1版本中，DJ仍未实现该方法) WatchersDOP形象的定义了Watchers这个类用于存放和“观察”DO，使用到的数据结构是ConcurrentLinkedQueue。 watch：该方法将DO添加到队列中 tryComplteWatched：遍历队列，将已完成的DO从队列中移除，调用未完成DO的safeTryComplte方法，并记录在该方法中完成的DO数量。 purgeCompleted：该方法只是将已完成的移除掉，并返回移除的数量。 每个Watchers都与一些key关联(HashMap)，定义为watchersForKey Key的类型没有限制， 当key对应的DO全部完成后，key以及对应的Watchers一起从Map中移除。 其他参数及属性 timeoutTimer brokerId purgeInterval：清理基准线，当DOP中已完成的DO数量达到该基准线后开始清理操作 reaperenbaled：是否允许清除DO watchersForKey：存放watcher与对应的key expirationReaper：超时DO清道夫 tryCompleteElseWatch该方法将一个DO塞进Watchers中并与多个Key进行关联。如果每次与Key进行关联时都执行一次tryComplete操作成本很大。kafka选择了一个折中的策略，保证在该方法内最多调用两次tryComplete方法： 执行第一次tryComplete操作，成功就返回 遍历keys，如果DO未完成就将key与Watchers进行绑定，添加到watchersForKey 执行第二次tryComplete，成功就返回 依然未成功的话就添加到timeoutTimer中(进入这里面的DO将怎么处理？怎么才能再次触发onComplete操作呢？) 放到Timer中的DO在超时的时候会执行TimerTask#run()方法 DelayedOperation基础了TimerTask类，覆盖了父类的run方法： override def run(): Unit = &#123; if (forceComplete()) onExpiration()&#125; DelayedJoinGroupCoordinator中实现了DJ作为DO的各个方法，在分析这些方法前需要关注的是GroupMetadata 与 MemberMetadata的两个属性： notYetRejoinedMembers AND awaitingJoinCallback awaitingJoinCallback是Mebmer的属性，初始为null;当member所在的group处于PreparingReblance状态下，member向coordinator发送了JoinGroup请求，那么该字段用于存放将JoinGroupResult反馈的回调方法，在重平衡结束后或者coordinator执行出境操作等会将该字段重新置为Null notYetRejoinedMembers存放该group中没有awaitingJoinCallback的member，在Rebalance期间，该集合中存储的是那些没有发送JoinGroup请求的member。 tryComplete如果group中所有的成员都发送了JoinGroupRequest 就调用forceComplete方法(DO中的方法)：def tryCompleteJoin(group: GroupMetadata, forceComplete: () =&gt; Boolean) = &#123; group synchronized &#123; if (group.notYetRejoinedMembers.isEmpty) forceComplete() else false &#125;&#125; 留个问题 如果某个member迟迟不发送JoinGroup请求的话，那总不能永久等待吧？member在什么情况下会被移出group呢？(关键点在于DelayedHeartbeat的onExpireHeartbeat方法) onExpireHeartbeat如果coordinator听不到member的心跳：member.latestHeartbeat + member.sessionTimeoutMs &gt; heartbeatDeadline heartbeat的超时时间由consumer的session.timeout.ms控制【默认为100s】那么就会将member从group中移除：private def onMemberFailure(group: GroupMetadata, member: MemberMetadata) &#123; trace(\"Member %s in group %s has failed\".format(member.memberId, group.groupId)) group.remove(member.memberId) group.currentState match &#123; case Dead | Empty =&gt; case Stable | AwaitingSync =&gt; maybePrepareRebalance(group) case PreparingRebalance =&gt; joinPurgatory.checkAndComplete(GroupKey(group.groupId)) &#125;&#125; 如果当前group处于PreparingRebalance状态，那么将会将查下是否可以complete join操作。那么前面两个问题已经明朗了： 添加到DOP的timeoutTimer中的DO只是放入一个定时器内，超时后移除，DO能否在超时前执行onComplete操作完全靠外部触发； 如果某个group存在多个member，如果其中存在member非正常退出(即没有执行unsubscribe操作)，那么coordinator必须依赖心跳超时来检查该member是否dead，在此期间内的joinGroup请求无法立即得到响应。 onCompleteJoin疑惑 该方法的第一步看不懂：// remove any members who haven't joined the group yetgroup.notYetRejoinedMembers.foreach &#123; failedMember =&gt; group.remove(failedMember.memberId) // TODO: cut the socket connection to the client&#125; 能够执行onCompleteJoin说明notYetRejoinedMembers已经是空的了，这里的移除操作感觉多余了？？？？ JoinGroupResult：val joinResult = JoinGroupResult( members=if (member.memberId == group.leaderId) &#123; group.currentMemberMetadata &#125; else &#123; Map.empty &#125;, memberId=member.memberId, generationId=group.generationId, subProtocol=group.protocol, leaderId=group.leaderId, errorCode=Errors.NONE.code) group的leaderId采取先来后到的原则，新来的memberId作为leaderId。 对于非正常重启一个consumer所遇到的长时间等待可以用下图加深理解：","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"test_blog","slug":"test-blog","date":"2018-04-04T03:22:14.000Z","updated":"2018-04-04T07:28:05.135Z","comments":true,"path":"2018/04/04/test-blog/","link":"","permalink":"http://yoursite.com/2018/04/04/test-blog/","excerpt":"","text":"this is titlethis is the second title//this is codepublic static void main(String[] args) &#123; System.out.println(\"hello world\");&#125;","categories":[],"tags":[{"name":"test","slug":"test","permalink":"http://yoursite.com/tags/test/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-04-04T02:16:19.237Z","updated":"2018-04-04T02:16:19.239Z","comments":true,"path":"2018/04/04/hello-world/","link":"","permalink":"http://yoursite.com/2018/04/04/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Controller工作原理","slug":"Controller工作原理","date":"2017-04-28T02:55:56.000Z","updated":"2018-04-11T05:51:20.688Z","comments":true,"path":"2017/04/28/Controller工作原理/","link":"","permalink":"http://yoursite.com/2017/04/28/Controller工作原理/","excerpt":"","text":"[TOC] start up controller 引入Controller的目的是为了减小ZK的压力以及降低整个分布式系统的复杂度。 注册监听向zk注册监听两个实例： SessionExpirationListener：如果session时效了，zk会负责重连的，kafka这边不需要处理 LeaderChangeListener：监听集群的leader(即controller)发生改变，对于原leader需要执行resign的操作(onControllerResignation)将Leadership上交: de-register listeners：IsrChangeNotificationListener、ReassignedPartitionsListener、PreferredReplicaElectionListener、ReassignedPartitionsIsrChangeListeners 关闭的功能有：删除topic、自动重平衡管理、replica与partition的状态机等 start Elector 首先要判断下zookeeper下面是否存在永久节点：/controller，该节点存储着controller的信息，如：{&quot;version&quot;:1,&quot;brokerid&quot;:2,&quot;timestamp&quot;:&quot;1493345638688&quot;} 抢注leader 从/controller目录下获取controllerId，成功获取到就返回amILeader。这样就能确保如果有broker注册controller(向zk的/controller目录下面创建临时节点)成功，其他broker就不再尝试注册 抢注leader：使用ZKCheckedEphemeral注册临时节点，下面是controller(broker)关闭的情况下，broker2注册controller成功的输出日志：[2017-04-28 10:13:58,689] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)[2017-04-28 10:13:58,692] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)[2017-04-28 10:13:58,693] INFO 2 successfully elected as leader (kafka.server.ZookeeperLeaderElector)[2017-04-28 10:13:59,592] INFO New leader is 2 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener) onControllerFailover在抢注成功后，该broker需要担负起Leader的职责，包括以下几个方面： 1. 改朝换代读取 /controller_epoch 的值，获取当前controller的纪元(朝代)，然后增加1 2. 注册ZK监听程序对ZK上的节点进行监听： /admin/reassign_partitions：监听partition重分配的动作 isr_change_notification：该节点用于通知parition的ISR变化 /admin/preferred_replica_election: 以上都是通过controller注册的，下面是通过特定的状态机向ZK注册监听的： PartitionStateMachine：描述parition的状态机 /brokers/topics：TopicChangeListener /admin/delete_topics：DeleteTopicsListener replicaStateMachine：描述replicas的状态机 /brokers/ids：BrokerChangeListener 最后，对每个topic添加PartitionModificationsListener,zk路径为：/brokers/topics/topic_name 3. 初始化controller上下文收集：brokers、topics、partitions-leader、ISR等信息；开启ControllerChannelManager和partitionStateMachine 4. 进行分区重分配和分区leader选举操作详见 partition reassign PreferredReplicaElection 5. 发送metadata数据发送数据到集群上所有的broker，包括live和shutdown的 6. 开启分区重平衡线程分区的重平衡是否开启由参数auto.leader.rebalance.enable控制，默认开启；线程的执行间隔由leader.imbalance.check.interval.seconds控制，默认为300 所有触发Elect的情况partition reassign在Kafka-manager上对topic——testReassign(包含3个分区)进行手动partition分配： 这是重分配之前的分区情况： 调整0和1的分区Replicas： 执行Reassign Partitions，观察日志： PartitionsReassignedListener感知到了ZK上/admin/reassign_partitions节点上内容发生了变化[PartitionsReassignedListener on 2]: Partitions reassigned listener fired for path /admin/reassign_partitions. Record partitions to be reassigned &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;testReassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,&#123;&quot;topic&quot;:&quot;testReassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;,&#123;&quot;topic&quot;:&quot;testReassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,1]&#125;]&#125; 由于分区2未进行重分配，其原来的replicas与现在的一致，因此无视该Reassign请求：kafka.common.KafkaException: Partition [testReassign,2] to be reassigned is already assigned to replicas 1,2. Ignoring request for partition reassignment 接下来对分区0和1进行重分配，就拿1来说：[Controller 2]: Handling reassignment of partition [testReassign,1] to new replicas 2,3 [Controller 2]: New replicas 2,3 for partition [testReassign,1] being reassigned not yet caught up with the leader [Controller 2]: Updated path /brokers/topics/testReassign with &#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;2&quot;:[1,2],&quot;0&quot;:[2,3],&quot;1&quot;:[2,3,1]&#125;&#125; for replica assignment [Controller 2]: Updated assigned replicas for partition [testReassign,1] being reassigned to 2,3,1 [[Controller 2]: Updating leader epoch for partition [testReassign,1]. [Controller 2]: Updated leader epoch for partition [testReassign,1] to 1 [Replica state machine on controller 2]: Invoking state change to NewReplica for replicas [Topic=testReassign,Partition=1,Replica=2] [Controller 2]: Waiting for new replicas 2,3 for partition [testReassign,1] being reassigned to catch up with the leader 疑问：往/brokers/topics/testReassign中写入的为啥是[2,3,1]而不是[2,3]呢？留个坑：在KafkaController.onPartitionReassignment方法中找答案吧~该方法在多处被调用 partition state machine下图简单描述了partition的4种状态： 状态机通过一个map类型的partitionState来存放所有分区的当前状态。 start up此时cluster的controller已经产生，controller通过读取ZK的partition和ISR节点信息判断每个partition的当前状态。 具体是判断每个分区对应的LeaderIsrAndControllerEpoch信息与当前的Epoch是否吻合： 如果吻合，则是OnLine状态 存在但不吻合，则是OffLine状态 不存在信息，则是New状态 划分好了当前的状态后，下面要将new和offline的转化成Online 如何转化呢？当然是选举leader咯，关键代码在OfflinePartitionLeaderSelector这个类里，下面这个表是选举时遇到的几种情况ISR|replicas|result—|—|—(1,2)|(1,2)|leader = 1null|(2,3)|unclean enabled ? leader=2 : NoReplicaOnlineExceptionnull|null|NoReplicaOnlineException 如当前有个repilca设置为1的topic：”jjhtest”，当前leader与isr的分配为： 接下来我重启broker3，会发生什么呢？ 由于每个分区的replicas都是固定的(不考虑手动分配)，关闭broker3将导致分区1,4,7的replicas中无可用的broker，这3个分区将无法分配到broker，进入offline状态 如果分区的replica&gt;1，在shutDownBroker方法中：broker作为leader的分区状态转移是online ==&gt; online，leader选举的selector为：ControlledShutdownPartitionLeaderSelector broker3恢复后，以上3个分区由offline状态向online转变，触发leader的选举 选举的过程： partitionReplicaAssignment该变量是个Mutable Map，存放的是每个分区对应的replicas信息。这个信息有两个来源： TopicChangeListener监听ZK上的topic变化，比如新增了topic，那么就会从ZK获取分区的replicas分配信息 PartitionModificationsListener监听parition的变化，比如新增了分区，会从ZK获取新增分区的replicas信息。 electLeaderForPartition当partition的状态由offline或者online ==&gt; online时，会通过不同的PartitionLeaderSelector选举leader(LeaderSelector缩写为LS)： 每次成功执行了leader select，都会更新leader cache(controllerContext.partitionLeadershipInfo) preferred replica election 在controller启动的过程中会执行一次preferredReplicaElection(简称PRE)，并创建一个间隔为leader.imbalance.check.interval.seconds(default:300)的定时任务执行PartitionRebalance操作，该操作会触发PRE kafka平衡策略的实现依赖于以下几方面的因素： 在不考虑手动分配分区的情况下，每个分区分配的Replicas是写死的，包括顺序都是固定的。 在出现broker挂掉的情况下，leader的重新选举能保证消息不丢失(未触发脏选举)，而PRE(首选备份选举)能保证broker恢复后分区的leader能均衡分布在集群上。 小实验有一个分区数为6，备份数为2的topic：”TheOne”，当前的分配状态为： 关闭broker3后分区4和5的leader重新选举： broker3恢复后，又重新加入到了各个topic的ISR中(IsrChangeNotificationListener被触发)： 一段时间后，触发PRE： [Controller 1]: Starting preferred replica leader election for partitions [TheOne,5] (kafka.controller.KafkaController)[Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [TheOne,5] (kafka.controller.PartitionStateMachine)[PreferredReplicaPartitionLeaderSelector]: Current leader 1 for partition [TheOne,5] is not the preferred replica. Trigerring preferred replica leader election (kafka.controller.PreferredReplicaPartitionLeaderSelector)[Controller 1]: Partition [TheOne,5] completed preferred replica leader election. New leader is 3 (kafka.controller.KafkaController) leader &amp; ISR又恢复了初始状态，达到了均衡。 PRE的选举操作由preferredReplicaPartitionLeaderSelector完成，状态为online ==&gt; online 思考：动态扩展broker如何实现？动态扩展broker能否实现分区的均衡扩展呢？ partition reblance 分区重平衡的间隔上面提到过了，默认为5分钟。 触发重平衡由失衡率(imbalanceRatio)决定:$$ ratio = Sum(partitonsNotLeaded)/Sum(partitonsShouldLeaded)$$当ratio大于leader.imbalance.per.broker.percentage(默认10%)时会触发重平衡 DEBUG: topics not in preferred replica Map() (kafka.controller.KafkaController)TRACE: leader imbalance ratio for broker 2 is 0.000000 (kafka.controller.KafkaController)DEBUG: topics not in preferred replica Map() (kafka.controller.KafkaController)TRACE: leader imbalance ratio for broker 1 is 0.000000 (kafka.controller.KafkaController)DEBUG: topics not in preferred replica Map([TheOne,4] -&gt; List(3, 2), [TheOne,5] -&gt; List(3, 1),...) (kafka.controller.KafkaController)TRACE: leader imbalance ratio for broker 3 is 0.951613 (kafka.controller.KafkaController) 重平衡的过程主要是执行PRE Replica state machine与partition相比多了3种状态 create topic新建topic涉及到partition以及replica状态的改变： NoExistent ==&gt; NewInvoking state change to NewPartition for partitions [TT,6],...(kafka.controller.PartitionStateMachine)Invoking state change to NewReplica for replicas [Topic=TT,Partition=6,Replica=2],[Topic=TT,Partition=6,Replica=3],... (kafka.controller.ReplicaStateMachine) 在新建topic后，ZK就会为该topic分配replicas New ==&gt; Online#partition state machineInvoking state change to OnlinePartition for partitions [TT,6],...Live assigned replicas for partition [TT,6] are: [List(2, 3)]Initializing leader and isr for partition [TT,6] to (Leader:2,ISR:2,3,LeaderEpoch:0,ControllerEpoch:30)#replica state machineInvoking state change to OnlineReplica for replicas [Topic=TT,Partition=6,Replica=2],[Topic=TT,Partition=6,Replica=3],... ReplicaDeletionStarted该状态一般在进行手动分区分配时产生的，以parition reassign实验试验中的分区1为例：replicas：[3,1] ==&gt; [2,3] broker2成为NewReplica：Invoking state change to NewReplica for replicas [Topic=testReassign,Partition=1,Replica=2] broker2进入OnlineReplica：Invoking state change to OnlineReplica for replicas [Topic=testReassign,Partition=1,Replica=2] broker1成为OfflineReplica，从ISR中移除： Invoking state change to OfflineReplica for replicas [Topic=testReassign,Partition=1,Replica=1]Removing replica 1 from ISR 3,1 for partition [testReassign,1] broker1进入ReplicaDeletionStarted状态:Invoking state change to ReplicaDeletionStarted for replicas [Topic=testReassign,Partition=1,Replica=1] broker1进入ReplicaDeletionSuccessful ：nvoking state change to ReplicaDeletionSuccessful for replicas [Topic=testReassign,Partition=1,Replica=1] broker1进入NonExistentReplica：Invoking state change to NonExistentReplica for replicas [Topic=testReassign,Partition=1,Replica=1] ZKListener 详解 下面对Controller中使用到的一些监听ZK节点及子节点变化的listener进行梳理 listener 监听的节点 用处 处理逻辑 BrokerChangeListener /brokers/ids 子节点 在ReplicaStateMachine中使用 BCL ISRChangeNotificationListener /isr_change_notification 子节点 Controller用于监听各个partition的ISR变化 ICNL PartitionModificationsListener /brokers/topics/topic_name 监听对应topic分区的变化(只允许增加) PML LeaderChangeListener /controller 监听集群leader的变化 注册监听 Broker Change Listener/brokers/ids目录下有多个以broker_id命名的子节点，每个子节点上存储着与broker相关的信息，如：&#123;&quot;jmx_port&quot;:9999,&quot;timestamp&quot;:&quot;1493886226618&quot;,&quot;endpoints&quot;:[&quot;PLAINTEXT://ip:9092&quot;],&quot;host&quot;:&quot;ip&quot;,&quot;version&quot;:3,&quot;port&quot;:9092&#125; 当这些子节点的内容发生变化时，controller感知到后会统计出变化的情况： broker2 shutdown：Newly added brokers: , deleted brokers: 2, all live brokers: 1,3 broker2 startup： Newly added brokers: 2, deleted brokers: , all live brokers: 1,2,3 delete brokershutdownBroker 该方法与KafkaServer中的shutdown方法的区别在于前者用于处理ControlledShutdown请求，将分区的职责转移出去，而后者是关闭brokerServer的一堆服务。 需要处理的分区为：$ List(partition) = { partition | id ∈ Replicas(partition) &amp;&amp; replicaFactor(partition) &gt; 1 } $ 对于作为leader的分区，执行leader的重选举(ControlledShutdownLS) 对于作为普通replica的分区： 关闭ReplicaRequest,如broker3是TheOne-1的replica，在关闭broker3时controller会向3发送关闭replica请求的请求：The stop replica request (delete = false) sent to broker 3 is [Topic=TheOne,Partition=1,Replica=3](kafka.controller.ControllerBrokerRequestBatch) broker3收到StopReplica请求后调用ReplicaManager的stopReplicas移除了相关的replica的Fetcher：[ReplicaFetcherManager on broker 3] Removed fetcher for partitions TheOne-1 将broker3的Replica状态置为OfflineReplica onBrokerFailure步骤如下： 在shutdownBroker的处理的分区中有一项要求是：replicaFactor &gt; 1，而对于replicaFactor = 1的分区来说，如果其leader所在的broker关闭了，那么该分区的状态将置为OfflinePartition。筛选的依据是其leader是否为关闭的broker(其他类型的分区在上面的流程中要么leader重新选举过了，要么只是改变ISR) 尝试使用OfflinePartitionLS将offline和new类型的partition转化为Online 集体性的将该broker的replica的状态置为OfflineReplica 感觉重复操作了，因为shutdownBroker阶段也有这操作。不同点在于shutdownBroker只处理replica&gt;1分区的replica状态。 如果第一步操作未执行，那么将向其他broker发送一个UpdateMetadataRequest onBrokerStartup主要处理的依然是分区、replica、ISR以及leader的转化： 发送UpdateMetadataRequest（具体原因不明） 将该broker的replica状态置为OnlineReplica 尝试使用OfflinePartitionLS将offline和new类型的partition转化为Online，这一步的主要目的是为了将onBrokerFailue阶段呗置为OffLinePartition的分区恢复成Online：[OfflinePartitionLeaderSelector]: No broker in ISR is alive for [jjhtest,1]. Pick the leader from the alive assigned replicas: 3 [OfflinePartitionLeaderSelector]: No broker in ISR is alive for [jjhtest,1]. Elect leader 3 from live brokers 3. There is potential data loss. [OfflinePartitionLeaderSelector]: Selected new leader and ISR {&quot;leader&quot;:3,&quot;leader_epoch&quot;:23,&quot;isr&quot;:[3]} for offline partition [jjhtest,1] 判断是否需要执行分区重分配的操作 ISR Change Notification Listener集群正常运行时/isr_change_notification节点下面是没有子节点的，当ISR发生变化的时候，如重启某个broker，那些ISR原来包含该broker的分区需要调整ISR，因此会创建这些partition的子节点。 下面是broker3挂掉后，TheOne这个topic的ISR： 在broker3恢复后，ISNL感知到了某些partition的ISR发生了变化，进行下面3步操作： 根据子节点的内容更新对应parition的leader以及ISR cache (从ZK上获取) 向集群所有节点发送MetadataRequest：DEBUG Sending MetadataRequest to Brokers:ArrayBuffer(1, 2, 3) for TopicAndPartitions:Set([TheOne, 2], [TheOne, 4],...) 删除子节点 Partition Modifications Listener Contorller为每个topic都设置了一个PartitionModificationsListener，用于监听新增分区的操作 下面是将TheOne的分区数增加3，PML所感知到的信息：[AddPartitionsListener on 1]: Partition modification trigered &#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;8&quot;:[3,2],&quot;4&quot;:[3,2],&quot;5&quot;:[3,1],&quot;6&quot;:[1,3],&quot;1&quot;:[1,3],&quot;0&quot;:[1,2],&quot;2&quot;:[2,3],&quot;7&quot;:[2,1],&quot;3&quot;:[2,1]&#125;&#125; for path /brokers/topics/TheOne[AddPartitionsListener on 1]: New partitions to be added Map([TheOne,7] -&gt; List(2, 1), [TheOne,6] -&gt; List(1, 3), [TheOne,8] -&gt; List(3, 2)) 新增了Partition，ZK已经为这些partition分配了replicas，在ZK上每个topic的分区上存储着的信息如：{&quot;controller_epoch&quot;:30,&quot;leader&quot;:3,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[3,2]} 拿到这些消息后，Controller要做的第一件事就是将新增的分区的replicas信息添加到partitionReplicaAssignment中，接下来才是处理这些新建的分区的状态、leader等，这个操作与create topic的处理如出一辙，区别在于前者是对新增的分区进行处理，后者是对这个topic的所有分区进行处理","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"ISR消息管理","slug":"ISR消息管理","date":"2017-04-24T09:09:57.000Z","updated":"2018-04-11T05:50:57.882Z","comments":true,"path":"2017/04/24/ISR消息管理/","link":"","permalink":"http://yoursite.com/2017/04/24/ISR消息管理/","excerpt":"","text":"[TOC] 疑问 leader是在何时更新Highwater的？ kafka-manager上出现Lag为负值是什么原因造成的？ Log中的消息被删除时，ISR之间是如何协调的？ 下面所有的讨论都是基于一个包含3个broker的kafka集群而言的 Replicaleader vs followers如果将所有的topic的replicas设置为2(_consumer_offsets除外)，那么对于每个partition而言其Log存在于两个broker上，其中一个作为leader，另一个作为followers 下图是一个包含3个分区的topic的replicas以及leader分布情况： leader与followers的职责之前在kafka-consumer剖析的文章中介绍过high watermark(简称HW)以及log end offset(简称LEO)的概念。作为leader，其主要职责是： 将消息及offset写入Local log &amp; offset 在followers完全备份了消息后(leader应该是会收到通知)，更新HW leader并不需要维护LEO的值，因为在Log中有一个nextOffsetMetadata属性就是每个Log维护的最新offset的信息 反观followers，其职责则体现在与leader的交互上： 备份消息，然后通知leader 更新LEO的值 创建Replica因为每个partition都有对应的replicas，所以创建replica的操作是在Partition中执行的，具体的方法是getOrCreateReplica：if (isReplicaLocal(replicaId)) &#123; //创建Log文件 val config = LogConfig.fromProps(...) val log = logManager.createLog(TopicAndPartition(topic, partitionId), config) //获取checkPoint文件 val checkpoint = replicaManager.highWatermarkCheckpoints(log.dir.getParentFile.getAbsolutePath) val offsetMap = checkpoint.read val offset = offsetMap.getOrElse(TopicAndPartition(topic, partitionId), 0L).min(log.logEndOffset) val localReplica = new Replica(replicaId, this, time, offset, Some(log)) addReplicaIfNotExists(localReplica)&#125; else &#123; val remoteReplica = new Replica(replicaId, this, time) addReplicaIfNotExists(remoteReplica)&#125;getReplica(replicaId).get 其中： getReplica与addReplicaIfNotExists都对assignedReplicaMap进行操作 checkpoint里维护着该log.dir下面所有topic-parition与offset的对应关系 log.dir是Log文件所在目录，log.dir.getParentFile就是server.properties中定义的log.dirs ReplicaManager 创建Replica的调用路径是ReplicaManager.becomeLeaderOrFollower-&gt;makeFollowers。ReplicaManager是broker范畴的，管理着broker上面所有的partition OffsetCheckPoint在多磁盘的环境中，log.dirs会定义在多个磁盘上，这样就能将Partiton分布在不同的磁盘上。 每个磁盘中的Log目录下都维护着3个OffsetCheckPoint文件：: recovery-point-offset-checkpoint: replication-offset-checkpoint：维护parition与offset信息: cleaner-offset-checkpoint replication-offset-checkpoint的内容如下：021__consumer_offsets 16 0__consumer_offsets 49 124admin.benchmark 9 24564634test 13 16709theOne 18 0__consumer_offsets 4 6... 其中： 第一行是CurrentVersion的值，默认是0 第二行是当前Log目录下面分区的数量 后面紧跟着的21行是分区以及对应的offset信息，格式为：$topic $partition $offset 在ReplicaManager中会将所有log目录下面的replication-offset-checkpoint组成一个名为highWatermarkCheckpoints的Map:val highWatermarkCheckpoints = config.logDirs.map(dir =&gt; (new File(dir).getAbsolutePath, new OffsetCheckpoint(new File(dir,ReplicaManager.HighWatermarkFilename)))).toMap 其更新操作是由一个定时任务控制的，每隔replica.high.watermark.checkpoint.interval.ms(默认5s)的时间会将所有log目录下的raplica的HW值写入到对应的replication-offset-checkpoint文件中。该定时任务是在becomeLeaderOrFollower方法中被开启的。 appendMessages在Log分析曾提到过appendMessages方法，该方法内先将消息写入local(即Leader的Log)，然后判断ack是否=-1来决定是否需要创建延迟的producer请求(不立即反馈，需要等待备份完成，由delayedProducePurgatory进行管理)。该请求是从leader发往followers的，内容包含： delayedMs：即producer配置的request.timeout requiredOffset: Leader在写入消息后的InextOffsetMetadata值(用LogAppendInfo.lastOffset + 1表示) 在ack = -1的策略下，leader必须在所有的followers都将消息备份的情况下，才会向producer发送反馈，而判断消息是否已完整备份的方法是Partition.checkEnoughReplicasReachOffset：def checkEnoughReplicasReachOffset(requiredOffset: Long): (Boolean, Errors) = &#123; leaderReplicaIfLocal() match &#123; case Some(leaderReplica) =&gt; val curInSyncReplicas = inSyncReplicas //对ISR中所有replicas的LEO校验是否已跟上leader的步伐(包括leader) def numAcks = curInSyncReplicas.count &#123; r =&gt; if (!r.isLocal) if (r.logEndOffset.messageOffset &gt;= requiredOffset) &#123; true &#125; else false else true &#125; val minIsr = leaderReplica.log.get.config.minInSyncReplicas if (leaderReplica.highWatermark.messageOffset &gt;= requiredOffset) &#123; if (minIsr &lt;= curInSyncReplicas.size) (true, Errors.NONE) else (true, Errors.NOT_ENOUGH_REPLICAS_AFTER_APPEND) &#125; else (false, Errors.NONE) case None =&gt; (false, Errors.NOT_LEADER_FOR_PARTITION) &#125;&#125; 第一个返回参数表示是否有足够多的replicas达到了requiredOffset 如果HW &lt; requiredOffset，表明leader还未收到足够多的replicas的response HW &gt;= requiredOffset表明HW已经更新过了，此时如果ISR的数量小于misISR，那么就报NOT_ENOUGH_REPLICAS_AFTER_APPEND错 如果当前ReplicaManger发现local不再是leader了，说明这个broker出现问题了，报NOT_LEADER_FOR_PARTITION错 Increment HWHW是由leader维护的，其更新时机主要有两种： ISR发生变化，包括： 新的broker成为leader shrink ISR expand ISR Replicas已备份完新的消息，判别的方法有两种： ISR中Replicas中的最小LEO值大于HW HW的LogOffsetMetadata存在一个老的log segment中，表明上一次Producer请求已处理完成(即消息已经在完成备份) ShrinkIsrbecomeLeaderOrFollowerReplcaFetcher启动过程ReplicaManger相关日志记录以broker3启动日志为例：在Log加载、处理完后，首先是unblock操作：[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 producer requests. (kafka.server.ReplicaManager)[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 producer requests. (kafka.server.ReplicaManager)[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 fetch requests. (kafka.server.ReplicaManager)[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 fetch requests. (kafka.server.ReplicaManager) 应该是解封在关闭阶段被阻塞的一些request 下面开启ReplicaFetcherThread：[2017-04-25 13:35:57,367] INFO [ReplicaFetcherThread-0-1], Starting (kafka.server.ReplicaFetcherThread)[2017-04-25 13:35:57,384] INFO [ReplicaFetcherThread-3-1], Starting (kafka.server.ReplicaFetcherThread)[2017-04-25 13:35:57,384] INFO [ReplicaFetcherThread-1-2], Starting (kafka.server.ReplicaFetcherThread)[2017-04-25 13:35:57,398] INFO [ReplicaFetcherThread-2-2], Starting (kafka.server.ReplicaFetcherThread)[2017-04-25 13:35:57,406] INFO [ReplicaFetcherThread-1-1], Starting (kafka.server.ReplicaFetcherThread)[2017-04-25 13:35:57,414] INFO [ReplicaFetcherThread-0-2], Starting (kafka.server.ReplicaFetcherThread)[2017-04-25 13:35:57,423] INFO [ReplicaFetcherThread-2-1], Starting (kafka.server.ReplicaFetcherThread)[2017-04-25 13:35:57,435] INFO [ReplicaFetcherThread-3-2], Starting (kafka.server.ReplicaFetcherThread) 数字编号的含义是什么呢？？ 接下来是添加Fetcher请求，向Remote Broker请求那些本地不是leader的分区的数据。摘取片段如下：INFO [ReplicaFetcherManager on broker 3] Added fetcher for partitions List([topic1-8, initOffset 6538 to broker BrokerEndPoint(1,10.45.4.9,9092)] ,[topic2-9, initOffset 0 to broker BrokerEndPoint(1,10.45.4.9,9092)]...) ReplicaFetcherThread 每个ReplicaManager不仅要维护一个producer的炼狱(delayedProducePurgatory)，还维护了一个fetcher的炼狱(delayedFetchPurgatory)。fetch这个操作不仅仅是由consumer触发的，followers备份消息也是通过向leader fetch实现的。 ReplicaFetcherManager ReplicaFetcherManager管理着ReplicaFetcherThread的创建和关闭工作。 该manager的命名格式为：“ReplicaFetcherManager on broker $ID” thread fetcher的数量由num.replica.fetchers控制：(31 * topic.hashCode() + partitionId) % numFetchers，当该参数设置为4，那么 $fetcherId∈[0,1,2,3]$，对于3节点replicas=2的集群，每个broker需要$4*2=8$个fetcherThread：val partitionsPerFetcher = partitionAndOffsets.groupBy&#123; case(topicAndPartition, brokerAndInitialOffset) =&gt; BrokerAndFetcherId(brokerAndInitialOffset.broker, getFetcherId(topicAndPartition.topic, topicAndPartition.partition))&#125; 上面的代码中将partition-broker这个map按照broker-&gt;fetcherId的对应关系进行分组。 fetcherThread的命名方式为：ReplicaFetcherThread-fetcherId-brokerId ReplicaFetcherThread是个定时执行的线程，执行间隔由replica.fetch.backoff.ms控制，默认为1s 每个ReplicaFethcerThread的fetch目标是一个broker上的若干topic-partition 工作流程 processPartitionData当前：: broker2作为test-1的follower: 其LEO值为17314: leader的HW=17314: FetchRequest中的fetchOffset = 17314 收到一条消息： [ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17314 for partition test-1. Received 45 messages and leader hw 17314 对log执行append操作 [ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17315 after appending 45 bytes of messages for partition test-1 与appendMessagesToLeader时不相同的是，follower执行append操作不会再进行offset配置操作，用得就是leader传过来的offset 设置follower的HW值[ReplicaFetcherThread-3-1], Follower 2 set replica high watermark for partition [test,1] to 17314 1s后再次fetch到消息：[ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17315 for partition test-1. Received 45 messages and leader hw 17315[ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17316 after appending 45 bytes of messages for partition test-1[ReplicaFetcherThread-3-1], Follower 2 set replica high watermark for partition [test,1] to 17315 由上面的日志可以注意到： LEO的值由17314 =&gt; 17315，这是因为前面执行的Log.append操作使得nextOffsetMetadata的值+1 leader的HW值变成了17315,这是由于leader检测到了followers(就是broker2)LEO的最小值已经变成了17315，所以更新了HW值,具体实现在Increment HW中有介绍 handleOffsetOutOfRange 当ReplicaFether的fetchOffset不在leader的offset之内(即大于最大值或者小于最小值)，那么就会收到OffsetOutOfRangeException 假设当前test-1这个partition的leader为1，新增broker2作为follower broker2在catch up的过程中broker1挂了，broker2被选为leader。(需要unclean.leader.election.enable配置为1，才能允许这样的脏选举) 情况一、Replica fetchOffset &gt; leader LEObroker1在恢复后，成为了follower，向leader发送ReplicaFetcherRequest，其中fetchOffset=7，该值大于leader的LEO，所以需要将broker1上该分区的Log执行truncate操作使得LEO值与leader保持一致。 值得注意的是，这样的truncate操作后，follower与leader的消息并不完全一致的，上例中，offset为3和4的消息就是不一致的 情况二、Replica fetchOffset &lt; leader start offset broker1在恢复之前，broker2添加了很多消息，并且也删除了一个消息，导致其最小的offset大于broker1中的LEO，这种情况下，broker1中的所有消息都没有意义了(因为ISR中的leader不再维护这些消息)，所以就删除掉所有的segment，然后fetch leader的第一条消息 Kafka_0.10.1.1版本对于收到OffsetOutOfRangeException时follower的LEO处于leader开始与LEO之间的情况没有对策。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]}]}