<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="bxXYqd7tQuaxEZXTyg2jG5HJvQhRp0bpb5KzceDpPsU" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="人为什么越长大越孤单？ 答:内心中有秘密,无法诉说">
<meta property="og:type" content="website">
<meta property="og:title" content="天外飞猪的博客">
<meta property="og:url" content="https://flytoair.github.io/index.html">
<meta property="og:site_name" content="天外飞猪的博客">
<meta property="og:description" content="人为什么越长大越孤单？ 答:内心中有秘密,无法诉说">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="天外飞猪的博客">
<meta name="twitter:description" content="人为什么越长大越孤单？ 答:内心中有秘密,无法诉说">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://flytoair.github.io/"/>





  <title>天外飞猪的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">天外飞猪的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2018/04/11/表连接最优方案/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/11/表连接最优方案/" itemprop="url">表连接方案</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-11T14:07:20+08:00">
                2018-04-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/04/11/表连接最优方案/" class="leancloud_visitors" data-flag-title="表连接方案">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>表连接是集合运算</p>
</blockquote>
<h1 id="嵌套循环连接"><a href="#嵌套循环连接" class="headerlink" title="嵌套循环连接"></a>嵌套循环连接</h1><p>最传统、普遍、重要的连接方式<br><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/22587721.jpg" alt=""><br>根据执行计划的执行顺序，解析下该连接方式的<strong>执行步骤</strong></p>
<ol>
<li>在列account_num的索引中，从满足查询条件account_num&gt;’A000000139’的范围中读取第一个索引行。</li>
<li>利用account_num中的rowid从表ge_balance_account中读取对应的数据行，此时，所读取的数据行中所有列都将获得常量值，并利用查询条件site_type=29对所读取结果进行检验，如果满足条件则执行下一阶段的操作；否则，返回步骤1重新开始处理下一个索引行。</li>
<li>利用表a的site_name列的常量值去表s中site_name列的索引中寻找对应的索引行，如果没找到对应的行，匹配失败，返回步骤1重新处理下一个索引行；成功，则返回一条匹配记录。</li>
</ol>
<h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><ul>
<li>次序性。按次序处理驱动表查询范围中的每一行数据，按次序执行表连接。</li>
<li>先行性。依据优先读取的表中查询范围的大小，决定所需要处理的数据量。</li>
<li>选择性。即使为WHERE条件中所使用到的列都创建了索引，也不意味着这些索引全都会被使用。</li>
</ul>
<h2 id="应用准则"><a href="#应用准则" class="headerlink" title="应用准则"></a>应用准则</h2><ul>
<li>适合用于范围扫描方式。可以实现快速响应。</li>
<li>如果执行表连接的某一边表只有在获得了对方所提供的执行结果后才能缩减自身的查询范围，则必须选择嵌套循环表连接方式。</li>
<li>驱动结果集数据量较大时，执行效率不高，主要是因为会发生大量的随机读取。</li>
</ul>
<h1 id="排序合并连接"><a href="#排序合并连接" class="headerlink" title="排序合并连接"></a>排序合并连接</h1><p><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/9243702.jpg" alt=""></p>
<p>##执行步骤 </p>
<ol>
<li>从表s的site_type列索引中读取满足查询条件s.site_type=29的数据行后，并按照site_name的值对数据进行排序。</li>
<li>从表a中读取满足查询条件的数据行后，也按照site_name的值进行排序。</li>
<li>对两个排序后的数据按照site_name相等的要求进行合并</li>
</ol>
<h2 id="特征-1"><a href="#特征-1" class="headerlink" title="特征"></a>特征</h2><ul>
<li>并行性。根据自身的查询条件读取条件并排序。</li>
<li>独立性。不需要从其他集合中获取处理结果。</li>
<li>必须按照全局扫描的方式进行处理。</li>
<li>与表的连接方向无关</li>
<li>在很大程度上减少随机读取次数</li>
</ul>
<h2 id="应用准则-1"><a href="#应用准则-1" class="headerlink" title="应用准则"></a>应用准则</h2><ol>
<li>在只能按照全局扫描方式来处理的SQL中，可以使用该方法</li>
<li>在不需要获取对应表中任何常量值也可以充分地实现缩减查询范围的条件下，使用该方法很有效</li>
<li>不适合OLTP类型的系统，因为排序是非常昂贵的操作。</li>
</ol>
<p>随机读取的代价：在最坏的情况下，为了读取某一行的数据，需要从磁盘将整个表格数据读入。<br>排序的代价：对内存造成了很大的负担</p>
<h1 id="哈希连接"><a href="#哈希连接" class="headerlink" title="哈希连接"></a>哈希连接</h1><p><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/18746506.jpg" alt=""></p>
<h2 id="术语及基本概念"><a href="#术语及基本概念" class="headerlink" title="术语及基本概念"></a>术语及基本概念</h2><p><strong>Hash Area</strong><br>内存空间，存储包括：位图矢量（向量）、哈希表和分区表。<br>位图矢量：就是Build Input中的值的集合（集合的唯一性），主要用于过滤操作（如果在Probe Input过程中所读取的数据行不存在于位图向量中，则没必要为其在分期中分配空间）<br>哈希表中存储各个分区的位置信息。</p>
<p><strong>分区(Partition)</strong></p>
<p><strong>聚簇(Cluster)</strong><br>在哈希聚簇的时候，把具有相同哈希值的行存储在统一聚簇中。（那么跟分区又有什么区别呢？ 书中有个比喻，如果将柜子比作分区，那么cluster就是柜子中的抽屉），好吧，这个意思就是：第一次哈希得到的是分区值（即映射到分区上），第二次哈希得到的值是映射到分区内的聚簇上。</p>
<p><strong>Build Input</strong><br>提前执行的读取准备操作</p>
<p><strong>Probe Input</strong><br>后来读取的操作</p>
<p><strong>In-Memory哈希连接</strong><br>能将Build Input全部加载到Hash Area的情况<br>指将Build Input全部存储在内存中并未其创建哈希表，在扫描Probe Input的同时实现连接。</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/23430760.jpg" alt=""></p>
<ol>
<li>根据统计信息选择结果集较小的作为Build Input</li>
<li>确定分区数fan-out</li>
<li>经过第一次hash运算，确定所在的分区</li>
<li>第二次hash运算，得到hash_value_2</li>
<li>根据hash_value_2创建哈希表，并将对应的列存入相应分区的聚簇中</li>
<li>根据连接列的值创建位图向量</li>
<li>按照上面的步骤对表中所有数据对象进行处理</li>
<li>从现在开始从Probe Input中读取满足查询条件的数据</li>
<li>第一次hash运算，并利用位图向量对Probe Input对象进行过滤，若没有通过过滤，则返回并重新读取下一个对象</li>
<li>对于通过的对象进行第二次hash运算，利用hash表读取相关分区和聚簇找到相应的行，找不到，重新读取下一个</li>
<li>执行连接操作，将结果发送到运输单位</li>
<li>反复8~11的操作</li>
<li>运输单位被填满后直接返回结果</li>
<li>按照以上步骤对Probe Input进行处理，直到结束为止。</li>
</ol>
<p>疑问：<del>位图向量中到底存储的是什么？是连接列的值呢，还是经过第一次哈希得到的值呢?</del> OK，还是统一确定为连接列的唯一值好了</p>
<h2 id="特征-2"><a href="#特征-2" class="headerlink" title="特征"></a>特征</h2><ul>
<li>不需要使用索引</li>
<li>允许实现局部范围扫描</li>
</ul>
<h1 id="延迟哈希连接"><a href="#延迟哈希连接" class="headerlink" title="延迟哈希连接"></a>延迟哈希连接</h1><p>不能完全加载，需要将超出的部分存储在磁盘中<br><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/43171176.jpg" alt=""><br>前6步基本一致</p>
<ol start="7">
<li>如果超过了Hash Area的范围，则将分区的地址信息存储在分区表中，并将超出的部分移到磁盘上对应的分区上，后面如果寻找到分区并利用地址信息，就再次将磁盘      上的数据加载到内存中实现连接操作</li>
<li>处理Bulid Input直到结束为止</li>
<li>从Probe Input的查询范围内读取数据，进行第一次哈希运算，并利用位图向量对结果进行过滤</li>
<li>对通过过滤的进行第二次哈希运算，如果对应的Build Input对象存在于内存中则读取哈希表进行连接操作，否则，将Probe Input对象存储在其所属的分区中</li>
<li>将无法实现连接的分区存储在磁盘上</li>
<li>按照以上方式对Probe Input的对象进行连续处理</li>
<li>内存中的连接操作处理完毕，接下来，利用分区表中的地址信息从磁盘上将没有被连接的分区对载入到内存中</li>
<li>从重新载入到内存中的各个分区中选择一个最小的集合为其创建哈希表。实现角色互换</li>
<li>对重新确定的Probe Input进行扫描，利用哈希表进行连接。按照这种方式对磁盘中剩余的所有对象进行处理，直到结束为止</li>
</ol>
<h2 id="特征-3"><a href="#特征-3" class="headerlink" title="特征"></a>特征</h2><ul>
<li>延迟哈希连接主要被使用在需要处理大量数据的批处理应用程序中。</li>
<li>哈希连接能弥补sort merge join最大的弱点(对海量数据执行排序操作所需要付出的代价过大)</li>
<li>利用位图向量，对另一个集合进行过滤，这一点与嵌套循环连接很相似。</li>
<li>延迟哈希无法实现局部范围扫描</li>
<li>一般而言，指定的Hash Area大小基本上是排序区域大小的1.5倍</li>
<li>在处理非海量数据的情况下，当额外要求对连接列的值进行排序操作时排序合并更有效</li>
</ul>
<h1 id="半连接"><a href="#半连接" class="headerlink" title="半连接"></a>半连接</h1><ul>
<li>在使用了子查询的时候为了实现子查询与主查询之间的连接而使用的一种广义表连接</li>
<li>子查询可以无条件的继承主查询的所有属性，反之不成立。（主查询所具有的各个列可以被使用在子查询中）</li>
<li>结果集合始终与主查询的集合类型相同</li>
</ul>
<h2 id="嵌套循环型半连接"><a href="#嵌套循环型半连接" class="headerlink" title="嵌套循环型半连接"></a>嵌套循环型半连接</h2><p><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/27165107.jpg" alt=""><br>为了维护主查询集合类型的完整性而附加性地增加了SORT(UNIQUE)操作</p>
<p> 在这里，子查询被优先执行，因此可以将子查询定义为“<strong>提供者</strong>”，其执行结果将以常量值的形式提供给主查询Where条件中的连接列。</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/59878132.jpg" alt=""></p>
<p>这个SQL的执行计划中，优先执行的是主查询，然后利用其执行结果与子查询进行了连接。因此在这里可以将自查询定义为“<strong>检验者</strong>”，对主查询的执行结果进行了检验。<br>实现这一转变的关键在于</p>
<ul>
<li>主查询中添加了查询条件<code>t.site_type=30</code></li>
<li>在子查询中添加了<code>a.site_id = t.id</code>这一句看上去重复的连接条件，这个连接条件可以从逻辑上确保子查询不可能被优先执行</li>
</ul>
<h2 id="过滤型半连接"><a href="#过滤型半连接" class="headerlink" title="过滤型半连接"></a>过滤型半连接</h2><p><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/15051616.jpg" alt=""></p>
<h2 id="哈希型半连接"><a href="#哈希型半连接" class="headerlink" title="哈希型半连接"></a>哈希型半连接</h2><p><img src="http://olt6kofv9.bkt.clouddn.com/18-2-12/37084461.jpg" alt=""></p>
<p>限制条件：</p>
<ol>
<li>在子查询中只能使用一个表</li>
<li>在子查询中再次嵌套使用子查询时无法使用哈希连接</li>
<li>连接条件只能是相等</li>
<li>在查询中不能使用Group BY,Connect by, rownum等限制条件</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2018/04/11/kafka消息/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/11/kafka消息/" itemprop="url">kafka消息</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-11T13:39:08+08:00">
                2018-04-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/04/11/kafka消息/" class="leancloud_visitors" data-flag-title="kafka消息">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>带着以下两点疑问，进行kafka server的Log管理源码的分析：</p>
<ul>
<li><strong>producer遇到 <em>NOT_LEADER_EXCEPTION</em> 是在何时产生的</strong></li>
<li><strong>消息是如何在ISR中备份的</strong></li>
</ul>
<p>下面以从上往下的方式对一条消息写入磁盘的全链路进行分析</p>
<h1 id="KafkaApis"><a href="#KafkaApis" class="headerlink" title="KafkaApis"></a>KafkaApis</h1><blockquote>
<p>kafka apis反映出kafka broker server可以提供哪些服务，<br>broker server主要和producer，consumer，controller有交互，搞清这些api就清楚了broker server的所有行为</p>
</blockquote>
<h2 id="handleProducerRequest"><a href="#handleProducerRequest" class="headerlink" title="handleProducerRequest"></a>handleProducerRequest</h2><ul>
<li>该方法用于处理Client的producer请求，<code>ApiKeys = 0</code></li>
<li>从 <em>RequestChannel</em> 中获取请求，然后根据<strong>acks</strong>规则进行反馈</li>
<li>写入磁盘的动作在<code>replicaManager.appendRecords</code>中完成</li>
</ul>
<h2 id="ack规则"><a href="#ack规则" class="headerlink" title="ack规则"></a>ack规则</h2><table>
<thead>
<tr>
<th>acks</th>
<th>规则</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>producer不需要等待ack,server收到消息后直接反馈</td>
</tr>
<tr>
<td>1</td>
<td>leader成功写入后反馈给producer</td>
</tr>
<tr>
<td>-1 (kafka-client中配置为<code>&quot;all&quot;</code>)</td>
<td>在ISR中所有replicas都写入消息后才进行反馈</td>
</tr>
</tbody>
</table>
<h1 id="ReplicaManager-appendMessages"><a href="#ReplicaManager-appendMessages" class="headerlink" title="ReplicaManager.appendMessages"></a>ReplicaManager.appendMessages</h1><ul>
<li>先将消息写入leader的log中(正常情况就是当前这个broker)</li>
<li>将消息写到ISR的其他备份中</li>
<li>超时或者ack规则满足时进行反馈操作(执行回调函数：<code>responseCallback</code>)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//写到leader的log</span><br><span class="line">val localProduceResults = appendToLocalLog(internalTopicsAllowed, entriesPerPartition, requiredAcks)</span><br><span class="line"></span><br><span class="line">if (delayedRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;</span><br><span class="line">//acks=-1时需要创建 delayedProduce实现消息的备份</span><br><span class="line">val produceMetadata = ProduceMetadata(requiredAcks, produceStatus)</span><br><span class="line">val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback)</span><br><span class="line">val producerRequestKeys = entriesPerPartition.keys.map(new TopicPartitionOperationKey(_)).toSeq</span><br><span class="line">delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">val produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</span><br><span class="line">responseCallback(produceResponseStatus)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="ReplicaManager-appendToLocalLog"><a href="#ReplicaManager-appendToLocalLog" class="headerlink" title="ReplicaManager.appendToLocalLog"></a>ReplicaManager.appendToLocalLog</h1><blockquote>
<p>当topic为内部topic(即<code>__consumer_offsets</code>)，并且不允许往内部类中写消息时，抛出 <em>InvalidTopicException</em></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val info = partitionOpt match &#123;</span><br><span class="line">  case Some(partition) =&gt;</span><br><span class="line">    partition.appendRecordsToLeader(records, requiredAcks)</span><br><span class="line"></span><br><span class="line">  case None =&gt; throw new UnknownTopicOrPartitionException(&quot;Partition %s doesn&apos;t exist on %d&quot;</span><br><span class="line">    .format(topicPartition, localBrokerId))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Partition-appendRecordsToLeader"><a href="#Partition-appendRecordsToLeader" class="headerlink" title="Partition.appendRecordsToLeader"></a>Partition.appendRecordsToLeader</h1><h3 id="判断当前broker是否是parition的leader"><a href="#判断当前broker是否是parition的leader" class="headerlink" title="判断当前broker是否是parition的leader"></a>判断当前broker是否是parition的leader</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def getReplica(replicaId: Int = localBrokerId): Option[Replica] = Option(assignedReplicaMap.get(replicaId))</span><br><span class="line"></span><br><span class="line">def leaderReplicaIfLocal: Option[Replica] =</span><br><span class="line">  leaderReplicaIdOpt.filter(_ == localBrokerId).flatMap(getReplica)</span><br></pre></td></tr></table></figure>
<ul>
<li>assignedReplicaMap：用于存放每个partition对应的leader已经replicas</li>
<li>通过比较leaderId与localBrokerId，判断当前broker是否就是leader</li>
<li>如果不满足，那么将抛出<em>NotLeaderForPartitionException</em></li>
</ul>
<h3 id="备份数不满足条件的消息不会写入"><a href="#备份数不满足条件的消息不会写入" class="headerlink" title="备份数不满足条件的消息不会写入"></a>备份数不满足条件的消息不会写入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> val log = leaderReplica.log.get</span><br><span class="line"> val minIsr = log.config.minInSyncReplicas</span><br><span class="line"> val inSyncSize = inSyncReplicas.size</span><br><span class="line">// acks为all的时候才会进行判断</span><br><span class="line"> if (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == -1) &#123;</span><br><span class="line">   throw new NotEnoughReplicasException(&quot;Number of insync replicas for partition %s is [%d], below required minimum [%d]&quot;</span><br><span class="line">     .format(topicPartition, inSyncSize, minIsr))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h1 id="Log-append"><a href="#Log-append" class="headerlink" title="Log.append"></a>Log.append</h1><ul>
<li>返回的格式为<code>LogAppendInfo</code>,包含第一条及最后一条offset信息</li>
<li>找到该Partition在当前broker上面最新的segment，如果<em>塞不进去</em> 就新建一个segment</li>
<li>将消息添加到segment中</li>
<li>更新segment的<code>LogEndOffset</code>为最新添加消息的<code>lastOffset</code> + 1<h2 id="ByteBufferMessageSet"><a href="#ByteBufferMessageSet" class="headerlink" title="ByteBufferMessageSet"></a>ByteBufferMessageSet</h2>append方法中传入的消息集的数据结构为<code>ByteBufferMessageSet</code> (<em>注：在0.10.2.0版本后引入新的结构：MemoryRecords</em>)，父类为Messageset,其结构如下图所示：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-4-13/72686806-file_1492065684931_d216.png" alt=""><blockquote>
<p>一个有效的MessageSet的最小长度为12字节</p>
</blockquote>
<h3 id="Message的组成"><a href="#Message的组成" class="headerlink" title="Message的组成"></a>Message的组成</h3>Message在magic不同的情况下有不同的结构：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-4-13/20229045-file_1492067234752_15050.png" alt=""><h3 id="迭代器的实现"><a href="#迭代器的实现" class="headerlink" title="迭代器的实现"></a>迭代器的实现</h3><code>internalIterator</code>是ByteBufferMessageSet实现的迭代器，迭代单位是<code>MessageAndOffset</code></li>
</ul>
<blockquote>
<p>magic的值是由server中Log的配置属性：<code>message.format.version</code>决定的，0.10.0之前的版本的magic值为0，之后的版本为1</p>
</blockquote>
<p>迭代的过程也是对消息的有效性的检验过程：</p>
<ul>
<li>ByteBuffer(对应MessageSet)的长度是否&lt; 12 </li>
<li>消息体(对应Message)的长度是否 &lt; 魔术为0时Message的头部长度(4+1+1+8+4 = 18)</li>
<li>ByteBuffer的长度是否&lt;消息体的长度(否则就表明消息不完整)</li>
</ul>
<p><strong>满足以上条件的消息一定是无法继续迭代的</strong></p>
<blockquote>
<p>由于消息的载体实现的是ByteBuffer，那我们就从Buffer的操作的角度来看看message和offset是如何被迭代取出来的：</p>
</blockquote>
<p>假设当前接收到一个新的ByteBuffer，下面进行迭代：</p>
<ol>
<li>获取buffer的片段(第一次算是拷贝)：<em>topIter = buffer.slice()</em></li>
<li>获取offset(获取前八个字节)： <em>offset = topIter.getLong()</em></li>
<li>获取size(获取紧接着的四个字节)：<em>size = topIter.getInt()</em></li>
<li>获取message(当前position指向message的开始位置，截取后续size大小的就可以得到message)：<em>message = topIter.slice; message.limit(size)</em></li>
<li>将topIter的position指向下一条MessageSet: <em>topIter.position(topIter.position + size)</em></li>
</ol>
<h3 id="LogAppendInfo的生成"><a href="#LogAppendInfo的生成" class="headerlink" title="LogAppendInfo的生成"></a>LogAppendInfo的生成</h3><p>LogAppendInfo主要包含四个属性，用于描述message set<br>: firstOffest：第一条消息的offset<br>: lastOffset：最后一条消息的offset<br>: maxTimestamp：消息里面包含的最大时间戳<br>: offsetOfMaxTimestamp：最大时间戳消息的offset</p>
<p><code>analyzeAndValidateMessageSet</code>方法实现了LogAppendInfo的生成，根据上面提到的迭代器对MessageSet中的消息进行迭代处理，找出并记录offset和timestamp信息，此外也对每条消息进行检验：</p>
<ol>
<li>每条消息的大小不能超过<code>max.message.bytes</code>所定义的</li>
<li>每条消息必须通过循环冗余校验</li>
</ol>
<h3 id="消息的进一步校验"><a href="#消息的进一步校验" class="headerlink" title="消息的进一步校验"></a>消息的进一步校验</h3><p>对消息的进一步校验及转化是在<code>validateMessagesAndAssignOffsets</code>方法中完成的。该方法的参数中涉及到一些概念：</p>
<h4 id="1-topic清理策略"><a href="#1-topic清理策略" class="headerlink" title="1.topic清理策略"></a>1.topic清理策略</h4><p><code>log.cleanup.policy</code>配置项控制着消息在segment中持久化的策略，目前有两种策略供选择：<strong>delete</strong>和<strong>compact</strong>，默认选项为delte。</p>
<ul>
<li>delete的策略很好理解，就是当segment时间或者大小到期了就删除。</li>
<li><p>compact的策略是为了满足系统灾后恢复的需求，该选项是针对topic的，比如存在某个topic：<em>email_topic</em>用于存储用户变更的email信息，key=userId,value=emailAddress，<strong>compact</strong>操作就是在日志删除过程中保留每个userId最新的数据，如果系统崩溃了也能通过该topic获得用户最新修改的email地址。下面的图很好的诠释了这种操作：<br><img src="http://kafka.apache.org/0102/images/log_compaction.png" width="530px"></p>
<h4 id="2-版本与magicValue"><a href="#2-版本与magicValue" class="headerlink" title="2. 版本与magicValue"></a>2. 版本与magicValue</h4></li>
<li><p>由于kafka的版本更新速度比较快，为了能让新的server版本兼容老的client版本以及server的滚动升级的实现，提供了<code>message.format.version</code>配置项定义consumer及Producer的API版本。</p>
</li>
<li>不同的API版本对应不同的magicValue，其中0.9.0.X版本之前(包括该版本)的magicValue未0，之后的都为1<blockquote>
<p>当前我们生产环境使用的API版本为<em>0.10.0.0</em>，server的版本为<em>0.10.1.1</em></p>
</blockquote>
</li>
</ul>
<h4 id="3-解析非压缩消息"><a href="#3-解析非压缩消息" class="headerlink" title="3. 解析非压缩消息"></a>3. 解析非压缩消息</h4><blockquote>
<p>producer可以选择是否对消息进行压缩</p>
</blockquote>
<p>message.timestamp.type：时间戳类型<br>: CreateTime：消息的创建时间&lt;<strong>默认值</strong>&gt;<br>: LogAppendTime：添加到log的时间</p>
<p>message.timestamp.difference.max.ms：最大时间间隔,表示收到的消息中的时间戳与当前时间的差的最大容忍值。</p>
<p>对非压缩消息的进一步处理的过程依然是ByteBuffer的操作过程：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/*主要目的是获取maxTimestamp和offsetOfMaxTimestamp*/</span><br><span class="line">var messagePosition = 0</span><br><span class="line">var maxTimestamp = Message.NoTimestamp</span><br><span class="line">var offsetOfMaxTimestamp = -1L</span><br><span class="line">//将position位置存到mark中</span><br><span class="line">buffer.mark()</span><br><span class="line">while (messagePosition &lt; sizeInBytes - MessageSet.LogOverhead) &#123;</span><br><span class="line">  buffer.position(messagePosition)</span><br><span class="line">  //offsetCounter是server维护的下一个offset的值</span><br><span class="line">  buffer.putLong(offsetCounter.getAndIncrement())</span><br><span class="line">  val messageSize = buffer.getInt()</span><br><span class="line">  val messageBuffer = buffer.slice()</span><br><span class="line">  messageBuffer.limit(messageSize)</span><br><span class="line">  val message = new Message(messageBuffer)</span><br><span class="line">  //以上的操作能获取到MessageSet中的一条message</span><br><span class="line">  if (message.magic &gt; Message.MagicValue_V0) &#123;</span><br><span class="line">    validateTimestamp(message, now, timestampType, timestampDiffMaxMs)</span><br><span class="line">    if (message.timestamp &gt; maxTimestamp) &#123;</span><br><span class="line">      maxTimestamp = message.timestamp</span><br><span class="line">      offsetOfMaxTimestamp = offsetCounter.value - 1</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  //将buffer的position移到后一条消息的头部</span><br><span class="line">  messagePosition += MessageSet.LogOverhead + messageSize</span><br><span class="line">&#125;</span><br><span class="line">//根据mark恢复position</span><br><span class="line">buffer.reset()</span><br></pre></td></tr></table></figure></p>
<p>经过上面的操作，MessageSet中的offset被server重新设置了，并且maxTimestamp之类的信息重新收集了。这些信息在append操作中会被使用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">validMessages = validateAndOffsetAssignResult.validatedMessages</span><br><span class="line">appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</span><br><span class="line">appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.offsetOfMaxTimestamp</span><br><span class="line">appendInfo.lastOffset = offset.value - 1</span><br></pre></td></tr></table></figure></p>
<p>其中：<br>: <code>validMessages</code>就是处理完后的MessageSet<br>: <code>offset</code>是上面使用到的offsetCounter，即下一个offset值。为什么要<strong>-1</strong>?因为上面执行了getAndIncrement()操作，因此当前的<code>offset</code>指向的依然是下一个offset值</p>
<h4 id="4-压缩消息的处理"><a href="#4-压缩消息的处理" class="headerlink" title="4.压缩消息的处理"></a>4.压缩消息的处理</h4><blockquote>
<p>producer的压缩策略必须与broker一致，如果不匹配那么将不会解压缩消息</p>
</blockquote>
<p>以下几种情况不会对压缩消息进行解压处理：</p>
<ol>
<li>topic指定了压缩策略，但是发送的消息中没有key(会报错)</li>
<li>消息体与server的magic值不一致</li>
<li></li>
</ol>
<hr>
<h2 id="找到合适的segment"><a href="#找到合适的segment" class="headerlink" title="找到合适的segment"></a>找到合适的segment</h2><ul>
<li>一个topic由多个parition组成，每个partition又存在多个segment</li>
<li>每个segment的大小由<code>log.segment.bytes</code>控制,下面是segment大小设置为<strong>1024</strong>的broker上某个partiton的Log情况：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[test-0]$ du -smh *</span><br><span class="line">0	    00000000000000017699.index</span><br><span class="line">4.0K	00000000000000017699.log</span><br><span class="line">4.0K	00000000000000017699.timeindex</span><br><span class="line">0	    00000000000000017724.index</span><br><span class="line">4.0K	00000000000000017724.log</span><br><span class="line">0	    00000000000000017724.timeindex</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<ul>
<li>每个log的命名规则是取该log中 <strong>下一个</strong> offset的值(<code>logEndOffset</code>);</li>
<li>以index结尾的offset index文件的作用是将offset映射到物理文件中</li>
<li>timeindex文件将时间戳与segment中的逻辑offset联系起来</li>
</ul>
</blockquote>
<p>满足以下几个条件之一时会创建新的segment：</p>
<ol>
<li>当前segment容不下最新的消息</li>
<li>当前segment非空，并且达到了<code>log.roll.hours</code>的时间</li>
<li>offset 或者 time index满了</li>
</ol>
<p>下面是写消息时创建出新的segment时server的输出日志：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2017-04-05 17:14:29,776] DEBUG Rolling new log segment in test-11 (log_size = 1006/1024&#125;, index_size = 0/1310720, time_index_size = 1/873813, inactive_time_ms = 184193/604800000). (kafka.log.Log)</span><br></pre></td></tr></table></figure></p>
<h3 id="将消息添加到LogSegment中"><a href="#将消息添加到LogSegment中" class="headerlink" title="将消息添加到LogSegment中"></a>将消息添加到LogSegment中</h3><p>LogSegment的参数主要有：<br>: log：File类型的MessageSet，该Set中包含一个FileChannel，能够从ByteBuffer中读取数据<br>: index: OffsetIndex，逻辑Offset到物理文件位置的索引<br>: timeIndex：TimeIndex，时间戳到物理位置的索引<br>: baseOffset：第一个offset</p>
<h4 id="写消息的操作："><a href="#写消息的操作：" class="headerlink" title="写消息的操作："></a>写消息的操作：</h4><p>将AppendInfo中的消息写入到LogSegment中的FileChannel中<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def writeFullyTo(channel: GatheringByteChannel): Int = &#123;</span><br><span class="line">  buffer.mark()</span><br><span class="line">  var written = 0</span><br><span class="line">  while (written &lt; sizeInBytes)</span><br><span class="line">    written += channel.write(buffer)</span><br><span class="line">  buffer.reset()</span><br><span class="line">  written</span><br><span class="line">&#125;</span><br><span class="line"> _size.getAndAdd(written)</span><br></pre></td></tr></table></figure></p>
<h4 id="FileMessageSet的search操作"><a href="#FileMessageSet的search操作" class="headerlink" title="FileMessageSet的search操作"></a><strong>FileMessageSet的search操作</strong></h4><p>根据offset或者时间戳是从Log中读取消息的常见方法。FileMessageSet提供对应的两个方法：<code>searchForOffsetWithSize</code>和<code>searchForTimestamp</code>。前者是从FileMessageSet的给定位置往后搜索第一个大于等于目标offset的消息，返回的是Offset&amp;Position，后者是从给定位置往后搜索第一个时间戳大于等于目标时间戳的消息，返回Timestamp&amp;Offset。</p>
<p>下图描述的是从0开始搜索offset≥1003的消息：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-4-18/34749506-file_1492481684531_13fee.png" alt=""></p>
<h4 id="写OffsetIndex"><a href="#写OffsetIndex" class="headerlink" title="写OffsetIndex"></a>写OffsetIndex</h4><p>OffsetIndex中并不会记录所有Offset的映射关系，写入Index的时机由<code>index.interval.bytes</code>参数(default：4096)控制，当segment中积累的消息数量大于该参数时，会将此次写入segment中的MessageSet的第一个消息的offset写入OffsetIndex中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if(bytesSinceLastIndexEntry &gt; indexIntervalBytes) &#123;</span><br><span class="line">  index.append(firstOffset, physicalPosition)</span><br><span class="line">  timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</span><br><span class="line">  bytesSinceLastIndexEntry = 0</span><br><span class="line">&#125;</span><br><span class="line">bytesSinceLastIndexEntry += messages.sizeInBytes</span><br></pre></td></tr></table></figure></p>
<p>index.append的具体操作如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (_entries == 0 || offset &gt; _lastOffset) &#123;</span><br><span class="line">    mmap.putInt((offset - baseOffset).toInt)</span><br><span class="line">    mmap.putInt(position)</span><br><span class="line">    _entries += 1</span><br><span class="line">    _lastOffset = offset</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的操作是将offset相对这个Segmet的baseOffset的偏移值以及物理地址填入到OffsetIndex中定义的<code>MappedByteBuffer</code>。</p>
<blockquote>
<p>之所以使用相对偏移值是出于节省存储空间的考虑，相对偏移值只需要4位空间就能存储，而MessageSet中的offset占8位。</p>
</blockquote>
<h4 id="写TimeIndex"><a href="#写TimeIndex" class="headerlink" title="写TimeIndex"></a>写TimeIndex</h4><p>在每次执行append操作时，TimeIndex记录的是最大时间戳及其对应的offset的索引。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (timestamp &gt; lastEntry.timestamp) &#123;</span><br><span class="line">  mmap.putLong(timestamp)</span><br><span class="line">  mmap.putInt((offset - baseOffset).toInt)</span><br><span class="line">  _entries += 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>写入Buffer中的是时间戳以及<strong>相对偏移量</strong></p>
<h2 id="更新LogEndOffset"><a href="#更新LogEndOffset" class="headerlink" title="更新LogEndOffset"></a>更新LogEndOffset</h2><p>上面的分析中提到过每个Log都维护了一个记录下一个offset的变量——<code>nextOffsetMetadata</code>,该变量是LogOffsetMetadata类型的：<br>: messageOffset：绝对偏移值<br>: segmentBaseOffset：LogSegment的baseOffset值<br>: relativePositionInSegment：LogSegment的大小(字节数)，根据字节数可以定位到这条消息在segment中的物理位置</p>
<p>每次append操作都会执行：<br><code>nextOffsetMetadata = new LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size.toInt)</code><br>其中：</p>
<ul>
<li>messageOffset为 <em>AppendInfo.lastOffset+1</em></li>
<li>activeSegment是当前可用的segment</li>
<li>activeSegment.size由<code>_size.get()</code>，这个_size正是在上面往segment中写消息时进行更新的，每次增加的值是写入channel中的字节数。</li>
</ul>
<p>下面这张图生动描述了这个更新的操作：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-4-17/96991802-file_1492421401594_f659.png" alt=""></p>
<ul>
<li>绿色的代表<em>activeSegment</em>，当前的<em>nextOffsetMetadata</em>为10(指的是messageOffset)</li>
<li>写入10条消息后，<em>nextOffsetMetadata</em>更新为21</li>
<li>再次写入10条消息后，更新为31</li>
<li>再来10条消息，无法写入，执行roll操作新建一个segment,messageOffset值未改变，不过activeSegment变化了</li>
<li>将10条消息写入新的segment中，更新为41</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2018/04/04/Kafka-Coordinator实现细节/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/04/Kafka-Coordinator实现细节/" itemprop="url">Kafka Coordinator实现细节</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-04T15:29:55+08:00">
                2018-04-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/04/04/Kafka-Coordinator实现细节/" class="leancloud_visitors" data-flag-title="Kafka Coordinator实现细节">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="GroupCoordinator"><a href="#GroupCoordinator" class="headerlink" title="GroupCoordinator"></a>GroupCoordinator</h1><blockquote>
<p>每个kafka server在启动的时候会创建一个GroupCoordinator用于管理group以及consumer的offset fetch/commit</p>
</blockquote>
<p>在创建GroupCoordinator实例时不仅需要brokerId、group以及offset config，还需要传入<strong>replicaManager</strong>，其作用是</p>
<h2 id="GroupMetadataManager"><a href="#GroupMetadataManager" class="headerlink" title="GroupMetadataManager"></a>GroupMetadataManager</h2><p>GroupMetadataManager是GroupCoordinator最重要的组成部分，其作用是管理group的元信息(状态、成员、提交的offset信息)以及作为coordinator的broker所分配到的分区信息，主要成员有4个：</p>
<ol>
<li>groupMetadataCache：存储group与GroupMetadata的cache</li>
<li>loadingPartitions：”__consumer_offsets”中正在被当前coordinator加载的分区</li>
<li>ownedPartitions：”__consumer_offsets”中分配到当前coordinator的分区(即该broker是这些分区的leader)</li>
<li>scheduler：删除过期offset以及group元数据的定时任务(执行间隔由<code>offsets.retention.check.interval.ms</code>参数控制，默认为10分钟)</li>
</ol>
<h3 id="consumer-offsets"><a href="#consumer-offsets" class="headerlink" title="__consumer_offsets"></a><strong>__consumer_offsets</strong></h3><p>__consumer_offsets是用于存储消费者消费信息的topic，存储的消息由两部分组成</p>
<ul>
<li><p>一部分是offset信息(<em>kafka.coordinator.OffsetsMessageFormatter</em>类型)的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[groupId,topic,partition]::[OffsetMetadata[offset,metadata],CommitTime ExprirationTime]</span><br></pre></td></tr></table></figure>
</li>
<li><p>另一部分是group信息(<em>kafka.coordinator.GroupMetadataMessageFormatter</em>类型):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">groupId::[groupId,Some(consumer),groupState,Map(memberId -&gt; [memberId,clientId,clientHost,sessionTimeoutMs], ...-&gt;[]...)]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>group分配到哪个分区的策略在<a href="http://blog.leanote.com/post/zfb050/Kafka-consumer" target="_blank" rel="noopener">kafka cosumer</a>中介绍过了。</p>
<p>作为一个特殊的topic，<strong>consumer_offsets也有replica的概念，<del>并且其replica factor与其他topic保持一致</del>。</strong>consumer_offsets上每个分区都对应一个leader，作为leader的broker上的GroupCoordinator会记录着分区上记录着的group以及offset信息。当leader(__consumer_offsets分布的leader)发生变化时，新的leader需要加载对应分区上的group以及offset信息。</p>
<p>server在处理<code>LeaderAndIsrRequest</code>时会对__consumer_offsets的分区做<em>出入境</em> 操作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def onLeadershipChange(updatedLeaders: Iterable[Partition], updatedFollowers: Iterable[Partition]) &#123;</span><br><span class="line">        updatedLeaders.foreach &#123; partition =&gt;</span><br><span class="line">          if (partition.topic == Topic.GroupMetadataTopicName)</span><br><span class="line">            coordinator.handleGroupImmigration(partition.partitionId)</span><br><span class="line">        &#125;</span><br><span class="line">        updatedFollowers.foreach &#123; partition =&gt;</span><br><span class="line">          if (partition.topic == Topic.GroupMetadataTopicName)</span><br><span class="line">            coordinator.handleGroupEmigration(partition.partitionId)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="小实验"><a href="#小实验" class="headerlink" title="小实验"></a><strong>小实验</strong></h4><p>“G3”这个group根据hash映射到分区2上，当前的ISR为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Topic: __consumer_offsets	Partition: 2	Leader: 1	Replicas: 1,3,2	Isr: 2,3,1</span><br></pre></td></tr></table></figure></p>
<p>接下来，关闭broker1，导致broker3成为了新的leader。</p>
<p>broker3执行<em>入境</em> 操作，加载分区2上面的的offset以及group信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Group Metadata Manager]: Loading offsets and group metadata from [__consumer_offsets,2]</span><br><span class="line">[Group Metadata Manager]: Loaded group metadata for group G3 with generation 1</span><br><span class="line">[Group Metadata Manager]: Loaded group metadata for group G3 with generation 2</span><br><span class="line">[Group Metadata Manager]: Loaded group metadata for group G3 with generation 3</span><br><span class="line">[Group Metadata Manager]: Loaded group metadata for group G3 with generation 4</span><br><span class="line">[Group Metadata Manager]: Loaded offset [OffsetMetadata[3448,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-0.</span><br><span class="line">[Group Metadata Manager]: Loaded offset [OffsetMetadata[3441,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-3.</span><br><span class="line">[Group Metadata Manager]: Loaded offset [OffsetMetadata[3257,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-5.</span><br><span class="line">[Group Metadata Manager]: Loaded offset [OffsetMetadata[3382,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-2.</span><br><span class="line">[Group Metadata Manager]: Loaded offset [OffsetMetadata[3397,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-4.</span><br><span class="line">[Group Metadata Manager]: Loaded offset [OffsetMetadata[3163,NO_METADATA],CommitTime 1495503059311,ExpirationTime 1495506659311] for newOne-1.</span><br><span class="line">[GroupCoordinator 3]: Loading group metadata for G3 with generation 4</span><br><span class="line">[Group Metadata Manager]: Finished loading offsets from [__consumer_offsets,2] in 21 milliseconds.</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>在<code>loadGroupsForPartition</code>方法中通过使用Map确保加载每个group最新generation的信息</p>
</blockquote>
<p>在执行入境操作之前，分区2被添加到<code>loadingPartitions</code>中，表示coordinator正在加载该分区里面的信息，这个阶段如果有groupId在loadingPartitions之内的消费请求进来，是无法响应的；<br>处理完后，分区2被添加到<code>ownedPartitions</code>中。</p>
<h3 id="保存group元数据"><a href="#保存group元数据" class="headerlink" title="保存group元数据"></a><span id="storeGroup">保存group元数据</span></h3><p>方法定义如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def prepareStoreGroup(group: GroupMetadata,</span><br><span class="line">                        groupAssignment: Map[String, Array[Byte]],</span><br><span class="line">                        responseCallback: Errors =&gt; Unit): Option[DelayedStore]</span><br></pre></td></tr></table></figure></p>
<ul>
<li>groupAssignment是group中member的分区分配</li>
<li>返回值是<strong>DelayedStore</strong>，这并不是一个<strong>DO</strong>类型的延迟任务，只适用于存放消息的临时媒介，方便后续往replicas中写Log用的。</li>
</ul>
<h4 id="生成消息"><a href="#生成消息" class="headerlink" title="生成消息"></a><strong>生成消息</strong></h4><p>第一步是生成能往Log(<strong>确切的说是写入__consumer_offsets中该group对应的partition中</strong>)中写入的<em>ByteBufferMessageSet</em>(<a href="http://leanote.com/blog/post/58e4bd26ab64413a3300ed80" target="_blank" rel="noopener">消息写入磁盘及备份实现分析</a>),key为groupId,value为member以及sessionTimeout，即上面所说的group信息。</p>
<h4 id="设置appendLog回调函数"><a href="#设置appendLog回调函数" class="headerlink" title="设置appendLog回调函数"></a><strong>设置appendLog回调函数</strong></h4><p>该回调函数作为参数传入到<code>ReplicaManager.replicaMessages</code>，对append操作的结果进行处理，最主要的就是状态的转换：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">case Errors.UNKNOWN_TOPIC_OR_PARTITION</span><br><span class="line">     | Errors.NOT_ENOUGH_REPLICAS</span><br><span class="line">     | Errors.NOT_ENOUGH_REPLICAS_AFTER_APPEND =&gt;</span><br><span class="line">  Errors.GROUP_COORDINATOR_NOT_AVAILABLE</span><br><span class="line"></span><br><span class="line">case Errors.NOT_LEADER_FOR_PARTITION =&gt;</span><br><span class="line">  Errors.NOT_COORDINATOR_FOR_GROUP</span><br><span class="line"></span><br><span class="line">case Errors.REQUEST_TIMED_OUT =&gt;</span><br><span class="line">  Errors.REBALANCE_IN_PROGRESS</span><br><span class="line"></span><br><span class="line">case Errors.MESSAGE_TOO_LARGE</span><br><span class="line">     | Errors.RECORD_LIST_TOO_LARGE</span><br><span class="line">     | Errors.INVALID_FETCH_SIZE =&gt;</span><br><span class="line">  Errors.UNKNOWN</span><br></pre></td></tr></table></figure></p>
<h4 id="执行storeGroup回调函数"><a href="#执行storeGroup回调函数" class="headerlink" title="执行storeGroup回调函数"></a><strong>执行storeGroup回调函数</strong></h4><p>在<code>doSyncGroup</code>中定义了一个用于处理leader的SyncGroupRequest的回调函数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">group synchronized &#123;</span><br><span class="line">  if (group.is(AwaitingSync) &amp;&amp; generationId == group.generationId) &#123;</span><br><span class="line">    if (error != Errors.NONE) &#123;</span><br><span class="line">      resetAndPropagateAssignmentError(group, error)</span><br><span class="line">      maybePrepareRebalance(group)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      setAndPropagateAssignment(group, assignment)</span><br><span class="line">      group.transitionTo(Stable)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>因为在等待回调函数被执行的过程中，可能会有新的member加入，这样的话就无法保证group的状态以及generation不会变化的。</p>
<h4 id="往replicas写group数据"><a href="#往replicas写group数据" class="headerlink" title="往replicas写group数据"></a><strong>往replicas写group数据</strong></h4><p>调用<code>GroupMetadataManager</code>的store方法，将<strong>DelayedStore</strong>中的messageSet以及回调函数传入到replicaManager的appendMessages方法中<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">replicaManager.appendMessages(</span><br><span class="line">  config.offsetCommitTimeoutMs.toLong,</span><br><span class="line">  config.offsetCommitRequiredAcks,</span><br><span class="line">  true, // allow appending to internal offset topic</span><br><span class="line">  delayedStore.messageSet,</span><br><span class="line">  delayedStore.callback)</span><br></pre></td></tr></table></figure></p>
<h3 id="保存offset-commit"><a href="#保存offset-commit" class="headerlink" title="保存offset commit"></a>保存offset commit</h3><p><code>prepareStoreOffsets</code>方法与<code>prepareStoreGroup</code>基本相像：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def prepareStoreOffsets(group: GroupMetadata,</span><br><span class="line">                        consumerId: String,</span><br><span class="line">                        generationId: Int,</span><br><span class="line">                        offsetMetadata: immutable.Map[TopicPartition, OffsetAndMetadata],</span><br><span class="line">                        responseCallback: immutable.Map[TopicPartition, Short] =&gt; Unit): Option[DelayedStore]</span><br></pre></td></tr></table></figure></p>
<p>返回的依然是个<strong>DelayedStore</strong>。<br>组装的MessageSet中,key为[groupId, topic, partition]，value为OffsetMetadata，写入的partition由groupId确定。</p>
<h4 id="offsetCommit回调函数"><a href="#offsetCommit回调函数" class="headerlink" title="offsetCommit回调函数"></a><strong>offsetCommit回调函数</strong></h4><blockquote>
<p><code>GroupMetadata</code>为offset commit创建了两个<em>Cache</em>：<code>offsets</code>以及<code>pendingOffsetCommits</code>，consumer提交的offset先存放到pending中，然后根据一定的状态来决定是否移到offsets中。</p>
</blockquote>
<p>如果offsetCommit执行结束后group依旧存活那个根据是否有错误对cache执行不同的操作：</p>
<ol>
<li>没有错误码，那么就将offset写入到<code>offsets</code>中，并从<code>pendingOffsetCommits</code>中移除；</li>
<li>有错误，那么仅仅将offset从<code>pendingOffsetCommits</code>中移除；</li>
</ol>
<h3 id="offset以及group元数据的清理工作"><a href="#offset以及group元数据的清理工作" class="headerlink" title="offset以及group元数据的清理工作"></a>offset以及group元数据的清理工作</h3><p><code>GroupMetadataManager</code>在启动时会开启一个定时执行的清理线程：”delete-expired-group-metadata”，该线程的主要工作是清理<strong>__consumer_offsets</strong>中失效的offset以及可删除的group信息。</p>
<h4 id="remove-expire-offset"><a href="#remove-expire-offset" class="headerlink" title="remove expire offset"></a><strong>remove expire offset</strong></h4><p>每个offset commit提交到server时，都会根据配置的保存时间来设置其失效时间，超过该时间的将会被清除掉。</p>
<p>首先，从cache中筛选出可清楚的offset集合：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val expiredOffsets = offsets.filter &#123;</span><br><span class="line">     case (topicPartition, offset) =&gt; offset.expireTimestamp &lt; startMs &amp;&amp; !pendingOffsetCommits.contains(topicPartition)</span><br><span class="line">   &#125;</span><br><span class="line">   offsets --= expiredOffsets.keySet</span><br><span class="line">   expiredOffsets</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>执行完删除操作过后判断group是否可以判定为<strong>DEAD</strong>：</p>
<ol>
<li>group不包含member</li>
<li>group的两个offsetCache都为空</li>
</ol>
</blockquote>
<h4 id="立墓碑"><a href="#立墓碑" class="headerlink" title="立墓碑"></a><strong>立墓碑</strong></h4><blockquote>
<p>当offset被移除或者group进入<strong>DEAD</strong>状态，都会在__consumer_offsets中留下一个<strong>墓碑</strong>。</p>
</blockquote>
<p>对于group而言：</p>
<ul>
<li><p>如果当前有member存在，那么其存在于__consumer_offsets中的数据是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">G6::[G6,Some(consumer),Stable,Map(client1-1ee1d482-c9fa-4617-860a-98d3a3d5a836 -&gt; [client1-1ee1d482-c9fa-4617-860a-98d3a3d5a836,client1,/10.45.48.129,120000])]</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果member为空，但是状态不是<strong>DEAD</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">G6::[G6,None,Empty,Map()]</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果group被该清理线程认为<strong>DEAD</strong>，该group信息不仅从<code>groupMetadataCache</code>中移除，还会在__cosnumer_offsets中留下一座墓碑：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">G6::NULL</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>offset的立墓碑操作与group类似。</p>
<h1 id="consumer与Coordinator的连接过程"><a href="#consumer与Coordinator的连接过程" class="headerlink" title="consumer与Coordinator的连接过程"></a>consumer与Coordinator的连接过程</h1><h2 id="确定coordinator"><a href="#确定coordinator" class="headerlink" title="确定coordinator"></a>确定coordinator</h2><blockquote>
<p>在<a href="http://blog.leanote.com/post/zfb050/Kafka-consumer" target="_blank" rel="noopener">kafka cosumer</a>中提到过consumer寻找coordinator的过程。</p>
</blockquote>
<p>选择将哪个节点作为coordinator其实是由consumer client决定的，确切的说是向已连接的节点中<strong>随机</strong>选择一个最<strong>空闲</strong>的节点发送<code>GroupCoordinatorRequest</code>。(<strong>这里随机是指往一个空闲的随机broker发送请求，收到的response中分配到的coordinator是根据group找对__consumer_offsets对应分区的Leader</strong>)这里空闲的定义是：<em>NetworkClient</em>中处于inflight状态的请求数量少，下面是consumer寻找到coordinator的日志：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[AbstractCoordinator] Sending coordinator request for group G6 to broker 10.45.4.10:9092</span><br><span class="line">[AbstractCoordinator] Received group coordinator response ClientResponse(receivedTimeMs=1495510047579, disconnected=false, request=ClientRequest(callback=..., request=RequestSend(header=&#123;api_key=10,api_version=0,correlation_id=0,client_id=client111&#125;, body=&#123;group_id=G6&#125;), createdTimeMs=1495510047472, sendTimeMs=1495510047577), responseBody=&#123;error_code=0,coordinator=&#123;node_id=1,host=10.45.4.9,port=9092&#125;&#125;)</span><br></pre></td></tr></table></figure></p>
<h3 id="handleGroupCoordinatorRequest"><a href="#handleGroupCoordinatorRequest" class="headerlink" title="handleGroupCoordinatorRequest"></a>handleGroupCoordinatorRequest</h3><p>server在接收到<em>GroupCoordinatorRequest</em>后：</p>
<ul>
<li>根据groupId找到对应__consumer_offsets上的分区<strong>P</strong></li>
<li>找到<strong>P</strong>对应的leader作为coordinator</li>
</ul>
<h2 id="handleJoinGroup"><a href="#handleJoinGroup" class="headerlink" title="handleJoinGroup"></a>handleJoinGroup</h2><h3 id="校验"><a href="#校验" class="headerlink" title="校验"></a>校验</h3><ul>
<li>简单校验：</li>
</ul>
<ol>
<li>coordinator是否处于工作状态</li>
<li>groupId是否有效</li>
<li>coordinator是否负责该group</li>
<li>sessionTimeoutMs是否合理</li>
<li>groupId是否在<code>loadingPartitions</code>中，如果在的话表明正在Rebalance中。</li>
</ol>
<ul>
<li>member校验：<br>客户端<code>ConsumerCoordinator</code>发送的<em>JoinGroupRequest</em> 中的memberId永远都是空的，也就是说memberId是由server端进行设置的。如果Request中group是已知(存在于<code>GroupMetadataManager.groupMetadataCache</code>)的并且memberId非空，那么Server将拒绝这个请求。</li>
</ul>
<h3 id="响应请求"><a href="#响应请求" class="headerlink" title="响应请求"></a>响应请求</h3><blockquote>
<p>group有不同的状态，在不同的状态下有相应的响应joinGroupRequest方法</p>
</blockquote>
<h4 id="Dead"><a href="#Dead" class="headerlink" title="Dead"></a><strong>Dead</strong></h4><p>group处于Dead状态表明该group中没有了成员，并且其GroupMetadata已被该coordinator移除,这个状态下对任何请求都是返回<code>UnknownMemberIdException</code></p>
<h4 id="PreparingRebalance"><a href="#PreparingRebalance" class="headerlink" title="PreparingRebalance"></a><strong>PreparingRebalance</strong></h4><p>根据Request中memberId是否为空有两套处理逻辑：</p>
<ul>
<li><p>memberId为空：执行<code>addMember</code>操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//新建memberId，格式为：clientId-UUID </span><br><span class="line">val memberId = clientId + &quot;-&quot; + group.generateMemberIdSuffix</span><br><span class="line">val member = new MemberMetadata(memberId, group.groupId, clientId, clientHost, rebalanceTimeoutMs,</span><br><span class="line">  sessionTimeoutMs, protocolType, protocols)</span><br><span class="line">member.awaitingJoinCallback = callback</span><br><span class="line">group.add(member.memberId, member)</span><br><span class="line">maybePrepareRebalance(group)</span><br><span class="line">member</span><br></pre></td></tr></table></figure>
</li>
<li><p>memberId不为空：执行<code>updateMember</code>操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">member.supportedProtocols = protocols</span><br><span class="line">member.awaitingJoinCallback = callback</span><br><span class="line">maybePrepareRebalance(group)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>以上两个操作都不会触发<code>PrepareRebalance</code>操作，因为当前已经是PreparingReblance状态了。</p>
</blockquote>
<h4 id="AwaitingSync"><a href="#AwaitingSync" class="headerlink" title="AwaitingSync"></a><strong>AwaitingSync</strong></h4><blockquote>
<p>这个状态表明coordinator已经发送了JoinGroupResponse了，正在等待leader发送分区分配的<strong>SyncGroupRequest</strong>.</p>
</blockquote>
<p>这个时候如果收到一个memberId为空的JoinGroupRequest，表明group中有新的成员加入，除了要创建member信息添加到GroupMetadata中之外，还需要<a href="#PR">prepareRebalance</a>，并将状态重新置为<em>PreparingReblance</em></p>
<p>如果收到的memberId不为空，有两种情况：</p>
<ol>
<li>该成员未收到之前发送过的JoinGroupResponse，这种情况就重新发送一个Response，leader和member分配不会改变的</li>
<li>这次的Request中改变了分区分配策略，因此需要执行<code>updateMember</code>操作，并且还需要执行<code>PrepareRebalance</code>操作将状态重新置为<em>PreparingReblance</em>。</li>
</ol>
<h4 id="Empty-or-Stable"><a href="#Empty-or-Stable" class="headerlink" title="Empty or Stable"></a><strong>Empty or Stable</strong></h4><p>处理三种可能的joinGroup情形：</p>
<ol>
<li>收到memberId为空的请求，group有新成员加入，执行<code>addMember</code>以及<code>PrepareRebalce</code>操作。</li>
<li>收到leader的joinGroup请求或者请求中的分配策略发生变化(<em>何种场景下会leader会重发joinGroup请求嘞？<a href="#rejoin">分区变化时consumer是如何rejoin的</a></em>)</li>
<li>其他情况(followers发送的没有内容变化的Join请求)说明followers可能没收到Response，因此重发Response。</li>
</ol>
<h2 id="handleSyncGroup"><a href="#handleSyncGroup" class="headerlink" title="handleSyncGroup"></a>handleSyncGroup</h2><p>在校验阶段如果发现该coordinator并不负责该group，则会反馈<code>NotCoordinatorForGroupException</code>。</p>
<p>能够正常响应SyncGroup请求的group状态为<strong>AwaitingSync</strong>和<strong>Stable</strong>，其中<strong>AwaitingSync</strong>状态下就是执行<a href="#storeGroup">保存group数据</a></p>
<h2 id="handleLeaveGroup"><a href="#handleLeaveGroup" class="headerlink" title="handleLeaveGroup"></a>handleLeaveGroup</h2><p>主要有三步操作</p>
<h3 id="1、从心跳DOP中移除"><a href="#1、从心跳DOP中移除" class="headerlink" title="1、从心跳DOP中移除"></a>1、从心跳<strong>DOP</strong>中移除</h3><p>心跳DOP(heartbeatPurgatory)中保存的是<strong>DelayedHeartbeat</strong>，该<strong>DO</strong>操作用于侦测member是否存活,成员变量包括group、member已经超时时间。当member要离开group时需要将该member对应的<br><strong>DO</strong>操作完成并移除掉。</p>
<h3 id="2、从group中移除"><a href="#2、从group中移除" class="headerlink" title="2、从group中移除"></a>2、从group中移除</h3><p>将member从其对应的<code>GroupMetadata</code>中的<em>members</em>中移除，然后根据当前group状态进行对应的处理：</p>
<ul>
<li><strong>Stable</strong> or <strong>Empty</strong>：触发<strong>PrepareRebalance</strong></li>
<li><strong>PreparingRebalance</strong>：complete掉joinPurgatory中该group的<strong>DJ</strong>操作</li>
</ul>
<h3 id="3、组装反馈信息"><a href="#3、组装反馈信息" class="headerlink" title="3、组装反馈信息"></a>3、组装反馈信息</h3><p>其实consumer对LeaveGroupResponse不是很关心，因为不会重发。</p>
<h2 id="handleFetchOffsets"><a href="#handleFetchOffsets" class="headerlink" title="handleFetchOffsets"></a>handleFetchOffsets</h2><p>consumer是从coordinator维护的offset cache ：<code>offsets</code>中获取group已提交的offset信息的。</p>
<p><code>FetchRequest</code>中包含group以及topic-partition信息，据此coordinator进行反馈：</p>
<ul>
<li>如果group不存在 <em>or</em> group状态为<strong>DEAD</strong>，则返回的<code>PartitionData</code>中offset为-1(代表<strong>InvalidOffset</strong>)</li>
<li>如果未指定topic以及partition，那么就将<code>offsets</code>中所有的[topic-partition, commit offset]数据都反馈给consunmer</li>
<li>如果指定topic-partition，就将<code>offsets</code>中对应的数据反馈，找不到就反馈-1</li>
</ul>
<hr>
<h1 id="Helper"><a href="#Helper" class="headerlink" title="Helper"></a>Helper</h1><h2 id="分区变化时consumer是如何rejoin的？"><a href="#分区变化时consumer是如何rejoin的？" class="headerlink" title="分区变化时consumer是如何rejoin的？"></a><span id="rejoin">分区变化时consumer是如何rejoin的？</span></h2><p>消费者在长连接的时候，增加了分区的topic，client是如何感知到的呢，下面是实验过程记录：</p>
<ul>
<li><p>10:33:08 开启consumer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[AbstractCoordinator] Sending coordinator request for group G6 to broker 10.45.4.10:9092 (id: -2 rack: null)</span><br><span class="line">[AbstractCoordinator] Discovered coordinator 10.45.4.9:9092 (id: 2147483646 rack: null) for group G6.</span><br><span class="line">[ConsumerCoordinator] Revoking previously assigned partitions [] for group G6</span><br><span class="line">[ConcurrentMessageListenerContainer] partitions revoked:[]</span><br><span class="line">[AbstractCoordinator] (Re-)joining group G6</span><br><span class="line">[AbstractCoordinator] Sending JoinGroup (&#123;group_id=G6,session_timeout=120000,member_id=,protocol_type=consumer,group_protocols=[&#123;protocol_name=range,protocol_metadata=java.nio.HeapByteBuffer[pos=0 lim=18 cap=18]&#125;]&#125;) to coordinator 10.45.4.9:9092 (id: 2147483646 rack: null)</span><br><span class="line">[AbstractCoordinator] Received successful join group response for group G6: </span><br><span class="line">[AbstractCoordinator] Successfully joined group G6 with generation 5</span><br><span class="line">[ConcurrentMessageListenerContainer] partitions assigned:[newOne-8, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3]</span><br></pre></td></tr></table></figure>
</li>
<li><p>10:34:17 controller检测到partitoin变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[AddPartitionsListener on 2]: Partition modification triggered &#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;8&quot;:[2,1],&quot;4&quot;:[1,3],&quot;11&quot;:[2,1],&quot;9&quot;:[3,2],&quot;5&quot;:[2,1],&quot;10&quot;:[1,3],&quot;6&quot;:[3,2],&quot;1&quot;:[1,2],&quot;0&quot;:[3,1],&quot;2&quot;:[2,3],&quot;7&quot;:[1,3],&quot;3&quot;:[3,2]&#125;&#125; for path /brokers/topics/newOne (kafka.controller.PartitionStateMachine$PartitionModificationsListener)</span><br></pre></td></tr></table></figure>
</li>
<li><p>10:38:10 client rejoin</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ConsumerCoordinator] Revoking previously assigned partitions [newOne-8, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3] for group G6</span><br><span class="line">[ConcurrentMessageListenerContainer] partitions revoked:[newOne-8, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3]</span><br><span class="line">[AbstractCoordinator] &quot;(Re-)joining group G6&quot;</span><br><span class="line">[AbstractCoordinator] Sending JoinGroup (&#123;group_id=G6,session_timeout=120000,member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,protocol_type=consumer,group_protocols=[&#123;protocol_name=range,protocol_metadata=java.nio.HeapByteBuffer[pos=0 lim=18 cap=18]&#125;]&#125;) to coordinator 10.45.4.9:9092 (id: 2147483646 rack: null)</span><br><span class="line">[AbstractCoordinator] Received successful join group response for group G6: &#123;error_code=0,generation_id=6,group_protocol=range,leader_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,members=[&#123;member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=18 cap=18]&#125;]&#125;</span><br><span class="line">[AbstractCoordinator] Sending leader SyncGroup for group G6 to coordinator 10.45.4.9:9092 (id: 2147483646 rack: null): &#123;group_id=G6,generation_id=6,member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,group_assignment=[&#123;member_id=client111-39aa3f5d-cdf2-492f-bf41-4affe1a00421,member_assignment=java.nio.HeapByteBuffer[pos=0 lim=70 cap=70]&#125;]&#125;</span><br><span class="line">[AbstractCoordinator] Successfully joined group G6 with generation 6</span><br><span class="line">[ConsumerCoordinator] Setting newly assigned partitions [newOne-8, newOne-9, newOne-10, newOne-11, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3] for group G6</span><br><span class="line">[ConcurrentMessageListenerContainer] partitions assigned:[newOne-8, newOne-9, newOne-10, newOne-11, newOne-4, newOne-5, newOne-6, newOne-7, newOne-0, newOne-1, newOne-2, newOne-3]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>那么consumer到底是如何触发rejoin的呢，这个时间间隔有何讲究？</p>
</blockquote>
<p>一开始自己关注的点是<code>rejoinNeeded</code>什么时候被置为true<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-25/78440956.jpg" alt=""></p>
<p>有三种情况下会被置为true：</p>
<ol>
<li>SyncGroupResponse中存在ERROR</li>
<li>HeartbeatResponse中存在REBALANCE_IN_PROGRESS、ILLEGAL_GENERATION或UNKNOWN_MEMBER_ID的ERROR</li>
<li>consumer leave group</li>
</ol>
<p>然而从日志来看并没有出现错误，所以注意力转移到<code>needRejoin</code>方法中的其他判断条件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">return subscriptions.partitionsAutoAssigned() &amp;&amp;</span><br><span class="line">        (super.needRejoin() || subscriptions.partitionAssignmentNeeded());</span><br></pre></td></tr></table></figure></p>
<p>partitionsAutoAssigned的条件是一直满足的，<code>partitionAssignmentNeeded</code>被置为true的场景有点多，排查起来比较费时费力。这时有个新的现象进入眼帘：<strong>经过多次试验后发现rejoin距离分区调整的时间间隔最长不超过5分钟！</strong>。而<code>metadata.max.age.ms</code>这个配置参数的默认值刚好是5分钟：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-25/33077532.jpg" alt=""></p>
<p>该配置项是client端强制刷新metadata的最长时间间隔。</p>
<blockquote>
<p>因为kafka集群出现broker或者partition变化的时候是不会通知客户端的，因此客户端需要定期的去获取metadata的值。</p>
</blockquote>
<p>客户端判断是否需要刷新metadata的方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public synchronized long timeToNextUpdate(long nowMs) &#123;</span><br><span class="line">    long timeToExpire = needUpdate ? 0 : Math.max(this.lastSuccessfulRefreshMs + this.metadataExpireMs - nowMs, 0);</span><br><span class="line">    long timeToAllowUpdate = this.lastRefreshMs + this.refreshBackoffMs - nowMs;</span><br><span class="line">    return Math.max(timeToExpire, timeToAllowUpdate);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中metadataExpireMs就是5分钟【默认】。</p>
<p>ConsumerCoordinator为metadata添加了一个Listener监听其更新操作：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addMetadataListener</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.metadata.addListener(<span class="keyword">new</span> Metadata.Listener() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onMetadataUpdate</span><span class="params">(Cluster cluster)</span> </span>&#123;</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment">// check if there are any changes to the metadata which should trigger a rebalance</span></span><br><span class="line">            <span class="keyword">if</span> (subscriptions.partitionsAutoAssigned()) &#123;</span><br><span class="line">                MetadataSnapshot snapshot = <span class="keyword">new</span> MetadataSnapshot(subscriptions, cluster);</span><br><span class="line">                <span class="keyword">if</span> (!snapshot.equals(metadataSnapshot)) &#123;</span><br><span class="line">                    metadataSnapshot = snapshot;</span><br><span class="line">                    subscriptions.needReassignment();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>needReassignment()</code>方法将<code>needsPartitionAssignment</code>置为true,这正是<code>partitionAssignmentNeeded()</code>方法所需要的。</p>
<h2 id="how-to-prepare-rebalance"><a href="#how-to-prepare-rebalance" class="headerlink" title=" how to prepare rebalance"></a><span id="PR"> how to prepare rebalance</span></h2><blockquote>
<p>在<a href="http://blog.leanote.com/post/zfb050/Kafka-consumer" target="_blank" rel="noopener">kafka cosumer</a>中做个一个实验：consumer多次关闭重连后，partition将会在较长时间后才能分配到。GroupCoordinator做了什么才导致这样的现象发生嘞？</p>
</blockquote>
<p><code>prepareReblance</code>的代码不长，但是要搞懂到底做了些什么着实不易：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private def prepareRebalance(group: GroupMetadata) &#123;</span><br><span class="line">  // if any members are awaiting sync, cancel their request and have them rejoin</span><br><span class="line">  if (group.is(AwaitingSync))</span><br><span class="line">    resetAndPropagateAssignmentError(group, Errors.REBALANCE_IN_PROGRESS)</span><br><span class="line"></span><br><span class="line">  group.transitionTo(PreparingRebalance)</span><br><span class="line">  info(&quot;Preparing to restabilize group %s with old generation %s&quot;.format(group.groupId, group.generationId))</span><br><span class="line"></span><br><span class="line">  val rebalanceTimeout = group.rebalanceTimeoutMs</span><br><span class="line">  val delayedRebalance = new DelayedJoin(this, group, rebalanceTimeout)</span><br><span class="line">  val groupKey = GroupKey(group.groupId)</span><br><span class="line">  joinPurgatory.tryCompleteElseWatch(delayedRebalance, Seq(groupKey))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="DelayedOperation-And-DelayedOperationPurgatory"><a href="#DelayedOperation-And-DelayedOperationPurgatory" class="headerlink" title="DelayedOperation And DelayedOperationPurgatory"></a>DelayedOperation And DelayedOperationPurgatory</h3><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/Purgatory+Redesign+Proposal" target="_blank" rel="noopener">purgatory wiki</a></p>
<blockquote>
<p><strong>DelayedJoin</strong>(简称<strong>DJ</strong>)是<strong>DelayedOperation</strong>(简称<strong>DO</strong>)的子类，<strong>DelayedOperationPurgatory</strong>(简称<strong>DOP</strong>)用于记录<strong>DO</strong>，并将超时的<strong>DO</strong>执行expired操作?</p>
</blockquote>
<p><strong>DO</strong>用于执行延迟任务，参数只有一个超时时间，目前已有的实现类有：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-25/40124304.jpg" alt=""></p>
<p>比如<strong>DelayedFetch</strong>的操作允许Fetch等待一定数量的消息或者达到超时时间后再返回。</p>
<ul>
<li>当<strong>DO</strong>完成给定的操作后，会调用<code>onComplete</code>方法(该方法需要子类实现)并且只会被调用一次, <code>isComplete</code>方法将返回true(通过原子变量<code>AtomicBoolean</code>实现)；</li>
<li><code>forceComplete</code>和<code>tryComplete</code>方法等能触发<code>onComplete</code>，其中前者已经在<strong>DO</strong>中实现了：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def forceComplete(): Boolean = &#123;</span><br><span class="line">  if (completed.compareAndSet(false, true)) &#123;</span><br><span class="line">    // cancel the timeout timer</span><br><span class="line">    cancel()</span><br><span class="line">    onComplete()</span><br><span class="line">    true</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    false</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>该方法将原子变量强行换成true(如果当前为false的话)，然后调用<code>onComplete</code>操作；后者需要子类实现：在判断是否达到complete条件后再调用forceComplete。</p>
<ul>
<li><p><code>safeTryComplete</code>是<code>TryComplete</code>的线程安全版：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def safeTryComplete(): Boolean = &#123;</span><br><span class="line">  synchronized &#123;</span><br><span class="line">    tryComplete()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果<strong>DO</strong>超时，将调用<code>onExpiration</code>操作(0.10.1.1版本中，<strong>DJ</strong>仍未实现该方法)</p>
</li>
</ul>
<h4 id="Watchers"><a href="#Watchers" class="headerlink" title="Watchers"></a><strong>Watchers</strong></h4><p><strong>DOP</strong>形象的定义了<code>Watchers</code>这个类用于存放和“观察”<strong>DO</strong>，使用到的数据结构是<code>ConcurrentLinkedQueue</code>。</p>
<ul>
<li><code>watch</code>：该方法将<strong>DO</strong>添加到队列中</li>
<li><code>tryComplteWatched</code>：遍历队列，将已完成的<strong>DO</strong>从队列中移除，调用未完成<strong>DO</strong>的<code>safeTryComplte</code>方法，并记录在该方法中完成的<strong>DO</strong>数量。</li>
<li><code>purgeCompleted</code>：该方法只是将已完成的移除掉，并返回移除的数量。</li>
<li>每个Watchers都与一些<strong>key</strong>关联(<code>HashMap</code>)，定义为<em>watchersForKey</em><ul>
<li>Key的类型没有限制，</li>
<li>当key对应的DO全部完成后，key以及对应的Watchers一起从Map中移除。</li>
</ul>
</li>
</ul>
<p><img src="http://olt6kofv9.bkt.clouddn.com/18-3-26/38785112.jpg" alt=""></p>
<h4 id="其他参数及属性"><a href="#其他参数及属性" class="headerlink" title="其他参数及属性"></a><strong>其他参数及属性</strong></h4><ul>
<li>timeoutTimer</li>
<li>brokerId</li>
<li>purgeInterval：清理基准线，当<strong>DOP</strong>中已完成的<strong>DO</strong>数量达到该基准线后开始清理操作</li>
<li>reaperenbaled：是否允许清除<strong>DO</strong></li>
<li>watchersForKey：存放watcher与对应的key</li>
<li>expirationReaper：超时<strong>DO</strong>清道夫</li>
</ul>
<h4 id="tryCompleteElseWatch"><a href="#tryCompleteElseWatch" class="headerlink" title="tryCompleteElseWatch"></a><strong>tryCompleteElseWatch</strong></h4><p>该方法将一个<strong>DO</strong>塞进Watchers中并与多个Key进行关联。如果每次与Key进行关联时都执行一次<code>tryComplete</code>操作成本很大。kafka选择了一个折中的策略，保证在该方法内最多调用两次<code>tryComplete</code>方法：</p>
<ol>
<li>执行第一次<code>tryComplete</code>操作，成功就返回</li>
<li>遍历keys，如果<strong>DO</strong>未完成就将key与Watchers进行绑定，添加到<code>watchersForKey</code></li>
<li>执行第二次<code>tryComplete</code>，成功就返回</li>
<li>依然未成功的话就添加到<code>timeoutTimer</code>中<br>(进入这里面的<strong>DO</strong>将怎么处理？怎么才能再次触发<code>onComplete</code>操作呢？)<ul>
<li>放到<code>Timer</code>中的<strong>DO</strong>在超时的时候会执行<code>TimerTask#run()</code>方法</li>
<li><code>DelayedOperation</code>基础了<code>TimerTask</code>类，覆盖了父类的run方法：<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (forceComplete())</span><br><span class="line">    onExpiration()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<h3 id="DelayedJoin"><a href="#DelayedJoin" class="headerlink" title="DelayedJoin"></a>DelayedJoin</h3><p>GroupCoordinator中实现了<strong>DJ</strong>作为<strong>DO</strong>的各个方法，在分析这些方法前需要关注的是<em>GroupMetadata</em> 与 <em>MemberMetadata</em>的两个属性：</p>
<h4 id="notYetRejoinedMembers-AND-awaitingJoinCallback"><a href="#notYetRejoinedMembers-AND-awaitingJoinCallback" class="headerlink" title="notYetRejoinedMembers AND awaitingJoinCallback"></a><strong>notYetRejoinedMembers AND awaitingJoinCallback</strong></h4><ul>
<li><code>awaitingJoinCallback</code>是Mebmer的属性，初始为null;当member所在的group处于<code>PreparingReblance</code>状态下，member向coordinator发送了JoinGroup请求，那么该字段用于存放将<strong>JoinGroupResult</strong>反馈的回调方法，在重平衡结束后或者coordinator执行<strong>出境</strong>操作等会将该字段重新置为Null</li>
<li><code>notYetRejoinedMembers</code>存放该group中没有<code>awaitingJoinCallback</code>的member，在Rebalance期间，该集合中存储的是那些没有发送JoinGroup请求的member。</li>
</ul>
<h4 id="tryComplete"><a href="#tryComplete" class="headerlink" title="tryComplete"></a><strong>tryComplete</strong></h4><p>如果group中所有的成员都发送了<em>JoinGroupRequest</em> 就调用forceComplete方法(<strong>DO</strong>中的方法)：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tryCompleteJoin</span></span>(group: <span class="type">GroupMetadata</span>, forceComplete: () =&gt; <span class="type">Boolean</span>) = &#123;</span><br><span class="line">  group synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span> (group.notYetRejoinedMembers.isEmpty)</span><br><span class="line">      forceComplete()</span><br><span class="line">    <span class="keyword">else</span> <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>留个问题</strong> 如果某个member迟迟不发送JoinGroup请求的话，那总不能永久等待吧？member在什么情况下会被移出group呢？(关键点在于<strong>DelayedHeartbeat的onExpireHeartbeat方法</strong>)</p>
<h4 id="onExpireHeartbeat"><a href="#onExpireHeartbeat" class="headerlink" title="onExpireHeartbeat"></a><strong>onExpireHeartbeat</strong></h4><p>如果coordinator听不到member的心跳：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">member.latestHeartbeat + member.sessionTimeoutMs &gt; heartbeatDeadline</span><br></pre></td></tr></table></figure></p>
<p>heartbeat的超时时间由consumer的<code>session.timeout.ms</code>控制【默认为100s】<br>那么就会将member从group中移除：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onMemberFailure</span></span>(group: <span class="type">GroupMetadata</span>, member: <span class="type">MemberMetadata</span>) &#123;</span><br><span class="line">  trace(<span class="string">"Member %s in group %s has failed"</span>.format(member.memberId, group.groupId))</span><br><span class="line">  group.remove(member.memberId)</span><br><span class="line">  group.currentState <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Dead</span> | <span class="type">Empty</span> =&gt;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Stable</span> | <span class="type">AwaitingSync</span> =&gt; maybePrepareRebalance(group)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt; joinPurgatory.checkAndComplete(<span class="type">GroupKey</span>(group.groupId))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果当前group处于<code>PreparingRebalance</code>状态，那么将会将查下是否可以complete join操作。那么前面两个问题已经明朗了：</p>
<ol>
<li>添加到<strong>DOP</strong>的timeoutTimer中的<strong>DO</strong>只是放入一个定时器内，超时后移除，<strong>DO</strong>能否在超时前执行<code>onComplete</code>操作完全靠外部触发；</li>
<li>如果某个group存在多个member，如果其中存在member非正常退出(<em>即没有执行unsubscribe操作</em>)，那么coordinator必须依赖心跳超时来检查该member是否dead，在此期间内的joinGroup请求无法立即得到响应。</li>
</ol>
<h4 id="onCompleteJoin"><a href="#onCompleteJoin" class="headerlink" title="onCompleteJoin"></a><strong>onCompleteJoin</strong></h4><p><strong>疑惑</strong> 该方法的第一步看不懂：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// remove any members who haven't joined the group yet</span></span><br><span class="line">group.notYetRejoinedMembers.foreach &#123; failedMember =&gt;</span><br><span class="line">  group.remove(failedMember.memberId)</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> cut the socket connection to the client</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>能够执行<code>onCompleteJoin</code>说明<code>notYetRejoinedMembers</code>已经是空的了，这里的移除操作感觉多余了？？？？</p>
<p><strong>JoinGroupResult</strong>：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> joinResult = <span class="type">JoinGroupResult</span>(</span><br><span class="line">  members=<span class="keyword">if</span> (member.memberId == group.leaderId) &#123; group.currentMemberMetadata &#125; <span class="keyword">else</span> &#123; <span class="type">Map</span>.empty &#125;,</span><br><span class="line">  memberId=member.memberId,</span><br><span class="line">  generationId=group.generationId,</span><br><span class="line">  subProtocol=group.protocol,</span><br><span class="line">  leaderId=group.leaderId,</span><br><span class="line">  errorCode=<span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br></pre></td></tr></table></figure></p>
<p>group的leaderId采取先来后到的原则，新来的memberId作为leaderId。</p>
<p>对于非正常重启一个consumer所遇到的长时间等待可以用下图加深理解：<br><img src="http://olt6kofv9.bkt.clouddn.com/18-4-4/29650397.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2018/04/04/test-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/04/test-blog/" itemprop="url">test_blog</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-04T11:22:14+08:00">
                2018-04-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/04/04/test-blog/" class="leancloud_visitors" data-flag-title="test_blog">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="this-is-title"><a href="#this-is-title" class="headerlink" title="this is title"></a>this is title</h1><h2 id="this-is-the-second-title"><a href="#this-is-the-second-title" class="headerlink" title="this is the second title"></a>this is the second title</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//this is code</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"hello world"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2018/04/04/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/04/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-04T10:16:19+08:00">
                2018-04-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/04/04/hello-world/" class="leancloud_visitors" data-flag-title="Hello World">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2017/09/30/Effective Java读书笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/30/Effective Java读书笔记/" itemprop="url">《Effective Java》读书笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-30T09:43:10+08:00">
                2017-09-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2017/09/30/Effective Java读书笔记/" class="leancloud_visitors" data-flag-title="《Effective Java》读书笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="构造函数-VS-静态工厂方法"><a href="#构造函数-VS-静态工厂方法" class="headerlink" title="构造函数 VS 静态工厂方法"></a>构造函数 VS 静态工厂方法</h2><p>静态工厂方法与设计模式无关，只是一种创建实例的方式，如：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Boolean <span class="title">valueOf</span><span class="params">(<span class="keyword">boolean</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (b ? TRUE : FALSE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>与传统的构造函数相比，使用静态工厂方法的优势有三点</p>
<ol>
<li>静态工厂方法的<strong>命名更灵活</strong>，使用者不需要查看API也能通过静态工厂方法创建想要的实例</li>
<li>静态工厂方法创建的对象是<strong>单例的</strong>：<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Boolean fa = Boolean.valueOf(<span class="keyword">false</span>);</span><br><span class="line">Boolean fb = Boolean.valueOf(<span class="keyword">false</span>);</span><br><span class="line">assertTrue(fa == fb);</span><br><span class="line"></span><br><span class="line">Boolean ca = <span class="keyword">new</span> Boolean(<span class="keyword">false</span>);</span><br><span class="line">Boolean cb = <span class="keyword">new</span> Boolean(<span class="keyword">false</span>);</span><br><span class="line">assertTrue(ca != cb);</span><br><span class="line">assertTur(fa.equals(ca));</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>如果比较的两个对象都是单例的，那么通过 <code>==</code>而非<code>equals</code>方法来判断俩对象是否相同的效率更高</p>
<ol start="3">
<li>静态工厂方法能返回员返回类型的任意子类型的对象：<blockquote>
<p>EnumSet是一个抽象类，RegularEnumSet以及JumboEnumSet是其实现类，使用者可以通过以下静态工厂方法创建一个存放枚举的集合，而这个集合的实现类是什么，使用者并不关心。使用者关心的是EnumSet提供的API方法能否正常使用就可以了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;E extends Enum&lt;E&gt;&gt; <span class="function">EnumSet&lt;E&gt; <span class="title">noneOf</span><span class="params">(Class&lt;E&gt; elementType)</span> </span>&#123;</span><br><span class="line">    Enum&lt;?&gt;[] universe = getUniverse(elementType);</span><br><span class="line">    <span class="keyword">if</span> (universe == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ClassCastException(elementType + <span class="string">" not an enum"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (universe.length &lt;= <span class="number">64</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RegularEnumSet&lt;&gt;(elementType, universe);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> JumboEnumSet&lt;&gt;(elementType, universe);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
</ol>
<p>仅有的劣势：如果某个类只能通过静态工厂方法实例化，而缺乏public或者protected的构造方法因而无法扩展。</p>
<h2 id="如何构造多属性的对象"><a href="#如何构造多属性的对象" class="headerlink" title="如何构造多属性的对象"></a>如何构造多属性的对象</h2><p> 当我们需要构建一个包含最多6个，最少1个属性值的对象时，常用的方式有以下几种</p>
<h3 id="1-重构构造函数"><a href="#1-重构构造函数" class="headerlink" title="1. 重构构造函数"></a>1. 重构构造函数</h3><blockquote>
<p>提供多个包含不同属性值的构造函数</p>
</blockquote>
<p> 这种方式有很明显的缺陷：</p>
<ul>
<li>随着属性值数量的增大构造函数将变得非常庞大、臃肿,<strong>可读性较差</strong></li>
<li>在给构造函数传递参数时很容易搞错(<em>虽然现在强大如IDEA这样的编辑器会有提示参数名的功能</em>)。比如一个Human类中身高以及体重参数的类型都是Double，传递参数时如果将身高体重搞混了编译器并不会报错，但是这样的结果难免就是很奇葩了。</li>
</ul>
<h3 id="2-使用JavaBeans模式"><a href="#2-使用JavaBeans模式" class="headerlink" title="2. 使用JavaBeans模式"></a>2. 使用JavaBeans模式</h3><p>这种模式就是先通过无参构造函数创建一个空的对象，然后使用<strong>setter</strong>方法对属性进行赋值，这种方式虽然提高了可读性，但是存在较为严重的缺陷：无法保证状态一致性，并且无法创建不可变的对象，<strong>安全性较差</strong>。</p>
<h3 id="3-builder模式【推荐】"><a href="#3-builder模式【推荐】" class="headerlink" title="3. builder模式【推荐】"></a>3. builder模式【推荐】</h3><blockquote>
<p>builder模式不直接生成对象，而是通过调用构造器传入必要的参数获得一个内部builder对象，通过调用该对象的build方法获得一个不可变的对象。</p>
</blockquote>
<ul>
<li>使用builder模式创建对象提高了可读性以及可扩展性，</li>
<li>一般只有在参数可数大于4的情况下才会使用builder模式,尤其适用于参数可选的情况下，springKakfa在生成Container时我们就采用的builder模式。</li>
</ul>
<h2 id="类与接口"><a href="#类与接口" class="headerlink" title="类与接口"></a>类与接口</h2><blockquote>
<p><strong>封装</strong>是软件设计的基本原则之一，设计良好的模块会隐藏所有的实现细节，将API与实现隔离开来</p>
</blockquote>
<h3 id="尽可能地使每个类或者成员不被外界访问"><a href="#尽可能地使每个类或者成员不被外界访问" class="headerlink" title="尽可能地使每个类或者成员不被外界访问"></a>尽可能地使每个类或者成员不被外界访问</h3><ul>
<li><p>如果在一个发行的版本中某个类或者接口是共有的，那么你就有责任永远支持它已保证其兼容性！</p>
</li>
<li><p>成员的4种访问级别：</p>
<ul>
<li>public：在任何地方都可以访问该成员</li>
<li>protected：声明该成员的类的子类可以访问这个成员</li>
<li>package private：声明该成员的包内部的类可以访问这个成员【default】</li>
<li>private：声明该成员的顶层类内部可以访问</li>
</ul>
</li>
<li><p>如果子类覆盖了父类的某个方法，那么子类中对应方法的访问权限不能低于父类的。(如果覆盖了父类的protected方法，那么子类中该方法必须声明为protected或者public)</p>
</li>
<li>因为接口中所有方法都隐含着public访问级别，因此实现接口的方法必须声明为public。</li>
</ul>
<h3 id="通过对象引用实现策略模式"><a href="#通过对象引用实现策略模式" class="headerlink" title="通过对象引用实现策略模式"></a>通过对象引用实现策略模式</h3><blockquote>
<p>在C语言中可以通过函数指针来实现策略模式，比如<code>qsort</code>函数要求用一个指向<code>comparator</code>函数的指针作为参数。在Java中没有函数指针的概念，但是可以通过对象引用来实现同样的功能。</p>
</blockquote>
<p>下面的代码展示了最简单的策略使用：<br>首先定义一个具体的策略类，考虑到策略类会被频繁使用，并且该类是无状态的，所以使用单例模式来导出策略类的实例比较好，这样能减少不必要的对象创建开销<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringLengthComparator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">StringLengthComparator</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> StringLengthComparator SLC = <span class="keyword">new</span> StringLengthComparator();</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(String s1, String s2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s1.length() - s2.length();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后在一个客户端方法中将策略类当做参数传入：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringArrayUtil</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sortStringList</span><span class="params">(List&lt;String&gt; list, StringLengthComparator comparator)</span> </span>&#123;</span><br><span class="line">        Assert.assertTrue(list.size() &gt; <span class="number">2</span>);</span><br><span class="line">        <span class="keyword">if</span> (comparator.compare(list.get(<span class="number">0</span>), list.get(<span class="number">1</span>)) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            String tmp = list.get(<span class="number">0</span>);</span><br><span class="line">            list.set(<span class="number">0</span>, list.get(<span class="number">1</span>));</span><br><span class="line">            list.set(<span class="number">1</span>, tmp);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里传入的参数限定为具体策略类不方便扩展，所以可以定义一个接口：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Comparator</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(String s1, String s2)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>客户端的声明可以改为：<code>public void sortStringList(List&lt;String&gt; list, Comparator comparator)</code></p>
<p>具体的策略类往往使用匿名类声明，比如最常见的：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Arrays.sort(array, <span class="keyword">new</span> Comparator&lt;String&gt;() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(String s1, String s2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> s1.length() -s2.length();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>以这种方式使用匿名类时，每次执行调用的时候都会创建一个新的实例，可以考虑将函数对象存储到一个私有的静态final域里。</p>
</blockquote>
<h2 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h2><h3 id="泛型相关的术语"><a href="#泛型相关的术语" class="headerlink" title="泛型相关的术语"></a>泛型相关的术语</h3><p><img src="http://olt6kofv9.bkt.clouddn.com/18-3-15/88924589.jpg" alt=""></p>
<h3 id="参数化类型是不可变的"><a href="#参数化类型是不可变的" class="headerlink" title="参数化类型是不可变的"></a>参数化类型是不可变的</h3><blockquote>
<p>对于不同的类型Type1和Type2，<code>List&lt;Type1&gt;</code>既不是<code>List&lt;Type2&gt;</code>的子类型也不是其超类型。<br>我们可以将任意对象放到<code>List&lt;Object&gt;</code>中，但是却只能将字符串对象放到<code>List&lt;String&gt;</code>中</p>
</blockquote>
<p>以<code>Stack</code>为例：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Stack</span>&lt;<span class="title">E</span>&gt; <span class="keyword">extends</span> <span class="title">Vector</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">      <span class="function"><span class="keyword">public</span> E <span class="title">push</span><span class="params">(E item)</span> </span>&#123;</span><br><span class="line">        addElement(item);</span><br><span class="line">        <span class="keyword">return</span> item;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>由于子类型的对象可以转化成父类型，所以以下操作可行的：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Stack&lt;Number&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">stack.push(<span class="keyword">new</span> Integer(<span class="number">1</span>));</span><br></pre></td></tr></table></figure></p>
<p>但是如果我在自己的Stack中要实现一个pushAll的功能，将一组数据push到栈中：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyStack</span>&lt;<span class="title">E</span>&gt; <span class="keyword">extends</span> <span class="title">Stack</span>&lt;<span class="title">E</span>&gt;</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pushAll</span><span class="params">(Iterable&lt;E&gt; src)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (E e : src) &#123;</span><br><span class="line">            push(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>当传入数据的类型与Stack的类型不一致时，即使传入对象是Stack类型的子类型，编译期会提示错误<br><img src="http://olt6kofv9.bkt.clouddn.com/18-3-15/16267116.jpg" alt=""></p>
<h3 id="有限制通配符的妙用"><a href="#有限制通配符的妙用" class="headerlink" title="有限制通配符的妙用"></a>有限制通配符的妙用</h3><p>前面的pushAll方法其参数称为：<strong>E的Iterable接口</strong> ，通过有限制通配符将参数改为：<strong>E的某个子类型的Iterable接口</strong>：<code>public void pushAll(Iterable&lt;? extends E&gt; src)</code></p>
<p>接下来，需要实现一个popAll的方法将Stack中的数据pop到一个集合中，首先想到的实现是这样的：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">popAll</span><span class="params">(Collection&lt;E&gt; dest)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!<span class="keyword">this</span>.isEmpty()) &#123;</span><br><span class="line">        dest.add(pop());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这样的实现有个限制，就是只能导出到一个类似于<code>List&lt;Number&gt;</code>的集合中，如果尝试导出到<code>List&lt;Object&gt;</code>则会报错：<br><img src="http://olt6kofv9.bkt.clouddn.com/18-4-11/62430702.jpg" alt=""></p>
<p>同样的借助于有限制通配符将popAll的参数由<strong>E的集合类型</strong>转化成<strong>E的某种超类的集合</strong>：<br><code>public void popAll(Collection&lt;? super E&gt; dest)</code></p>
<h3 id="类型安全的异构容器"><a href="#类型安全的异构容器" class="headerlink" title="类型安全的异构容器"></a>类型安全的异构容器</h3><blockquote>
<p>泛型常用于容器，参数化的容器一般限制了参数类型的数量，如一个List只有一个类型参数，一个Map有两个类型参数。</p>
</blockquote>
<p>如果想要获得更多的灵活性，就需要对参数进行泛型化，而不是对容器进行泛型化。考虑下面这个类：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Favorites</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;Class&lt;?&gt;, Object&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(Class&lt;T&gt; type, T instance)</span> </span>&#123;</span><br><span class="line">        map.put(type, instance);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">T <span class="title">get</span><span class="params">(Class&lt;T&gt; type)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (T) map.get(type);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>在Java 1.5之后对Class进行了泛型化处理</li>
<li>对map的Key进行了泛型化，这样的话我们就能将不同类型的对象存到容器中，这就是<strong>异构</strong></li>
<li>Map的value类型是Object，因此容器并不能保证键值对之间的类型关系，如果传入原生态的Class，那么就可恶意的将一个String对象映射到其他类型，进而破坏Favorites的内部结构<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Class c = Integer.class;</span><br><span class="line">Favorites.put(c, <span class="string">"da"</span>);</span><br><span class="line">System.out.println(Favorites.get(Integer.class));</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>为了避免出现不可预料的运行时异常，在put过程应该<strong>严格把关</strong>，确保传入的类型与对象类型是一致的，这可以借助于Class的<strong>cast</strong>方法：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> T <span class="title">cast</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (obj != <span class="keyword">null</span> &amp;&amp; !isInstance(obj))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ClassCastException(cannotCastMsg(obj));</span><br><span class="line">    <span class="keyword">return</span> (T) obj;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中<code>isInstance</code>方法能判断对象是否为指定类的对象。</p>
<h2 id="异常与并发"><a href="#异常与并发" class="headerlink" title="异常与并发"></a>异常与并发</h2><h3 id="ConcurrentModificationException"><a href="#ConcurrentModificationException" class="headerlink" title="ConcurrentModificationException"></a>ConcurrentModificationException</h3><blockquote>
<p>这是Java提供的一种标准异常，适用于的场景为：在禁止并发修改的情况下检测到了对象的并发修改</p>
</blockquote>
<p>看下面一个简单的例子：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testConcurrentModificationException</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Set&lt;Integer&gt; set = <span class="keyword">new</span> HashSet&lt;&gt;(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>));</span><br><span class="line">    <span class="keyword">for</span> (Integer i : set) &#123;</span><br><span class="line">        System.out.println(i);</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">2</span>) &#123; </span><br><span class="line">            set.remove(<span class="number">3</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>输出的结果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">java.util.ConcurrentModificationException</span><br><span class="line">	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437)</span><br><span class="line">	at java.util.HashMap$KeyIterator.next(HashMap.java:1461)</span><br><span class="line">	at Test.testConcurrentModificationException(Test.java:549)</span><br></pre></td></tr></table></figure></p>
<p>只要i&lt;=2就会报这个错，因为<strong>企图在遍历列表的过程中，将一个元素从列表中删除是非法的</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2017/09/07/如何愉快的使用线程池/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/07/如何愉快的使用线程池/" itemprop="url">如何愉快的使用线程池</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-07T13:54:29+08:00">
                2017-09-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2017/09/07/如何愉快的使用线程池/" class="leancloud_visitors" data-flag-title="如何愉快的使用线程池">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>近来，在工作中多次使用了多线程编程，其中使用最多的就是<code>ExecutorService</code>了，在使用中遇到了诸多问题，最主要的问题是——<strong>何时关闭线程池？</strong></p>
<p>何时关闭线程池的问题可以转化为如何获知所有线程执行完毕。关于这个问题,stackoverflow上有<a href="https://stackoverflow.com/questions/1250643/how-to-wait-for-all-threads-to-finish-using-executorservice" target="_blank" rel="noopener">一篇很有意思的讨论</a></p>
<p>可以总结出以下几种方式：</p>
<h3 id="1-shutdown-awaitTermination的方式"><a href="#1-shutdown-awaitTermination的方式" class="headerlink" title="1. shutdown + awaitTermination的方式"></a>1. shutdown + awaitTermination的方式</h3><p>模式：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutorService taskExecutor = Executors.newFixedThreadPool(<span class="number">4</span>);</span><br><span class="line"><span class="keyword">while</span>(...) &#123;</span><br><span class="line">  taskExecutor.execute(<span class="keyword">new</span> MyTask());</span><br><span class="line">&#125;</span><br><span class="line">taskExecutor.shutdown();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  taskExecutor.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>调用了<code>shutdown</code>方法后，线程池不再接受新的任务，等待当前所有任务执行完毕后就会退出。这种方式适用于<strong>所有线程是一次性执行的</strong>，也就是说线程池中的线程只会被调用一次，如果需要执行的任务次数大于线程池的数量，调用shutdown方法会导致后续任务无法分配到线程执行。</p>
<h3 id="2-使用countDownLatch"><a href="#2-使用countDownLatch" class="headerlink" title="2. 使用countDownLatch"></a>2. 使用countDownLatch</h3><p>CountDownLatch是一个常用于线程同步的<strong>闭锁</strong>，这个适用于<strong>已知任务总执行次数</strong>的情况。<br>模式：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">CountDownLatch latch = <span class="keyword">new</span> CountDownLatch(totalNumberOfTasks);</span><br><span class="line">ExecutorService taskExecutor = Executors.newFixedThreadPool(<span class="number">4</span>);</span><br><span class="line"><span class="keyword">while</span>(...) &#123;</span><br><span class="line">  taskExecutor.execute(<span class="keyword">new</span> MyTask());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  latch.await();</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException E) &#123;</span><br><span class="line">   <span class="comment">// handle</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在每个线程中执行一次countDown操作。</p>
<h3 id="3-使用Future-get"><a href="#3-使用Future-get" class="headerlink" title="3. 使用Future.get()"></a>3. 使用Future.get()</h3><p>因为<code>future.get()</code>方法会等待线程执行完成，并获取返回值，所以当所有task对应的future都有返回值了，就可以关闭线程池了。<br>模式：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutorService pool = Executors.newFixedThreadPool(concurrent);</span><br><span class="line">List&lt;Future&gt; futures = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (Map&lt;String, Date&gt; dateSplit : dateSplits) &#123;</span><br><span class="line">    futures.add(pool.submit(<span class="keyword">new</span> MyCallableTask());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (Future future : futures) &#123;</span><br><span class="line">    future.get();</span><br><span class="line">&#125;</span><br><span class="line">pool.shutdown();</span><br></pre></td></tr></table></figure></p>
<h3 id="4-使用Future-CompletionService"><a href="#4-使用Future-CompletionService" class="headerlink" title="4. 使用Future + CompletionService"></a>4. 使用Future + CompletionService</h3><p><code>CompletionService</code>也是java.util.concurrent提供的。<br>模式：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutorService threadPool = Executors.newFixedThreadPool(N_THREADS);</span><br><span class="line">CompletionService&lt;List&lt;Object&gt;&gt; pool = <span class="keyword">new</span></span><br><span class="line">        ExecutorCompletionService&lt;~&gt;(threadPoolpool);</span><br><span class="line">List&lt;Future&lt;List&lt;Object&gt;&gt;&gt; futures = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">while</span>(...) &#123;</span><br><span class="line">    futures.add(pool.submit(<span class="keyword">new</span> CallableTask()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (future in futures) &#123;</span><br><span class="line">    future.get();</span><br><span class="line">&#125;</span><br><span class="line">treadPool.shutdown();</span><br></pre></td></tr></table></figure></p>
<p><code>com.best.oasis.express.util.kafka.utils.KafkaConsumerUtils</code>中就是使用这种模式来获取每个分区上对应时间段内的消息的。该模式适用于<strong>需要处理task返回值</strong>的情况</p>
<h3 id="5-使用CompletableFuture-需要JDK1-8"><a href="#5-使用CompletableFuture-需要JDK1-8" class="headerlink" title="5. 使用CompletableFuture (需要JDK1.8)"></a>5. 使用CompletableFuture (<em>需要JDK1.8</em>)</h3><p>用的不多，直接看模式：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">CompletableFuture&lt;?&gt;[] futures = tasks.stream()</span><br><span class="line">        .map(task -&gt; CompletableFuture.runAsync(task, pool))</span><br><span class="line">        .toArray(CompletableFuture[]::<span class="keyword">new</span>);</span><br><span class="line">CompletableFuture.allOf(futures).join();</span><br><span class="line">pool.shutdown();</span><br></pre></td></tr></table></figure></p>
<h3 id="6-使用ListenableFuture-需要Guava"><a href="#6-使用ListenableFuture-需要Guava" class="headerlink" title="6. 使用ListenableFuture (需要Guava)"></a>6. 使用ListenableFuture (需要<em>Guava</em>)</h3><p>上面几种模式都是等待线程执行完，而这个则是通过回调，在所有任务执行完后通知用户。<br>模式：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ListeningExecutorService service = MoreExecutors.listeningDecorator(Executors</span><br><span class="line">        .newFixedThreadPool(concurrent));</span><br><span class="line">List&lt;ListenableFuture&lt;Object&gt;&gt; futures = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (Map&lt;String, Date&gt; dateSplit : dateSplits) &#123;</span><br><span class="line">    ListenableFuture&lt;Object&gt; lf = service.submit(<span class="keyword">new</span> MyCallableTask());</span><br><span class="line">    futures.add(lf);</span><br><span class="line">&#125;</span><br><span class="line">ListenableFuture&lt;List&lt;Object&gt;&gt; lf = Futures.successfulAsList(futures);</span><br><span class="line">Futures.addCallback(lf, <span class="keyword">new</span> FutureCallback&lt;List&lt;Object&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(List&lt;Object&gt; result)</span> </span>&#123;</span><br><span class="line">        logger.info(<span class="string">"所有线程处理完毕"</span>);</span><br><span class="line">        service.shutdownNow();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Throwable t)</span> </span>&#123;</span><br><span class="line">        logger.info(<span class="string">"某个线程出错了"</span> + t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><code>successfulAsList</code>在所有Future都执行成功后返回一个由这些Future返回值组成的List，而且错误或者取消的Future都是在onSuccess中处理的，只是返回值为null。<strong>顺便一提：使用这个Future扩展形式的话，task中的异常是会被拦截的</strong></p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2017/04/28/Controller工作原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/28/Controller工作原理/" itemprop="url">Controller工作原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-28T10:55:56+08:00">
                2017-04-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2017/04/28/Controller工作原理/" class="leancloud_visitors" data-flag-title="Controller工作原理">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="start-up-controller"><a href="#start-up-controller" class="headerlink" title="start up controller"></a>start up controller</h1><blockquote>
<p>引入Controller的目的是为了减小ZK的压力以及降低整个分布式系统的复杂度。</p>
</blockquote>
<h2 id="注册监听"><a href="#注册监听" class="headerlink" title="注册监听"></a><span id="register">注册监听</span></h2><p>向zk注册监听两个实例：</p>
<ul>
<li>SessionExpirationListener：如果session时效了，zk会负责重连的，kafka这边不需要处理</li>
<li>LeaderChangeListener：监听集群的leader(即controller)发生改变，对于原leader需要执行resign的操作(<code>onControllerResignation</code>)将Leadership上交:<ul>
<li>de-register listeners：IsrChangeNotificationListener、ReassignedPartitionsListener、PreferredReplicaElectionListener、ReassignedPartitionsIsrChangeListeners</li>
<li>关闭的功能有：删除topic、自动重平衡管理、replica与partition的状态机等</li>
</ul>
</li>
</ul>
<h2 id="start-Elector"><a href="#start-Elector" class="headerlink" title="start Elector"></a>start Elector</h2><blockquote>
<p>首先要判断下zookeeper下面是否存在永久节点：<code>/controller</code>，该节点存储着controller的信息，如：<code>{&quot;version&quot;:1,&quot;brokerid&quot;:2,&quot;timestamp&quot;:&quot;1493345638688&quot;}</code></p>
</blockquote>
<h3 id="抢注leader"><a href="#抢注leader" class="headerlink" title="抢注leader"></a>抢注leader</h3><ul>
<li>从<em>/controller</em>目录下获取controllerId，成功获取到就返回<em>amILeader</em>。这样就能确保如果有broker注册controller(向zk的/controller目录下面创建临时节点)成功，其他broker就不再尝试注册</li>
<li>抢注leader：使用<code>ZKCheckedEphemeral</code>注册临时节点，下面是controller(broker)关闭的情况下，broker2注册controller成功的输出日志：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2017-04-28 10:13:58,689] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)</span><br><span class="line">[2017-04-28 10:13:58,692] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)</span><br><span class="line">[2017-04-28 10:13:58,693] INFO 2 successfully elected as leader (kafka.server.ZookeeperLeaderElector)</span><br><span class="line">[2017-04-28 10:13:59,592] INFO New leader is 2 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="onControllerFailover"><a href="#onControllerFailover" class="headerlink" title="onControllerFailover"></a>onControllerFailover</h3><p>在抢注成功后，该broker需要担负起Leader的职责，包括以下几个方面：</p>
<h4 id="1-改朝换代"><a href="#1-改朝换代" class="headerlink" title="1. 改朝换代"></a><strong>1. 改朝换代</strong></h4><p>读取 <code>/controller_epoch</code> 的值，获取当前controller的纪元(朝代)，然后增加1</p>
<h4 id="2-注册ZK监听程序"><a href="#2-注册ZK监听程序" class="headerlink" title="2. 注册ZK监听程序"></a><strong>2. 注册ZK监听程序</strong></h4><p>对ZK上的节点进行监听：</p>
<ul>
<li><strong>/admin/reassign_partitions</strong>：监听partition重分配的动作</li>
<li><strong>isr_change_notification</strong>：该节点用于通知parition的ISR变化</li>
<li><strong>/admin/preferred_replica_election</strong>:</li>
</ul>
<p>以上都是通过controller注册的，下面是通过特定的状态机向ZK注册监听的：</p>
<ul>
<li><strong>PartitionStateMachine</strong>：描述parition的状态机<ul>
<li><em>/brokers/topics</em>：<code>TopicChangeListener</code></li>
<li><em>/admin/delete_topics</em>：<code>DeleteTopicsListener</code></li>
</ul>
</li>
<li><strong>replicaStateMachine</strong>：描述replicas的状态机<ul>
<li><em>/brokers/ids</em>：<code>BrokerChangeListener</code></li>
</ul>
</li>
</ul>
<p>最后，对每个topic添加<code>PartitionModificationsListener</code>,zk路径为：<strong>/brokers/topics/topic_name</strong></p>
<h4 id="3-初始化controller上下文"><a href="#3-初始化controller上下文" class="headerlink" title="3. 初始化controller上下文"></a><strong>3. 初始化controller上下文</strong></h4><p>收集：brokers、topics、partitions-leader、ISR等信息；开启<code>ControllerChannelManager</code>和<code>partitionStateMachine</code></p>
<h4 id="4-进行分区重分配和分区leader选举操作"><a href="#4-进行分区重分配和分区leader选举操作" class="headerlink" title="4. 进行分区重分配和分区leader选举操作"></a><strong>4. 进行分区重分配和分区leader选举操作</strong></h4><p>详见</p>
<ul>
<li><a href="#PR">partition reassign</a></li>
<li><a href="#PRE">PreferredReplicaElection</a></li>
</ul>
<h4 id="5-发送metadata数据"><a href="#5-发送metadata数据" class="headerlink" title="5. 发送metadata数据"></a><strong>5. 发送metadata数据</strong></h4><p>发送数据到集群上所有的broker，包括live和shutdown的</p>
<h4 id="6-开启分区重平衡线程"><a href="#6-开启分区重平衡线程" class="headerlink" title="6. 开启分区重平衡线程"></a><strong>6. 开启分区重平衡线程</strong></h4><p><a href="#rebalance">分区的重平衡</a>是否开启由参数<code>auto.leader.rebalance.enable</code>控制，默认开启；线程的执行间隔由<code>leader.imbalance.check.interval.seconds</code>控制，默认为300</p>
<h3 id="所有触发Elect的情况"><a href="#所有触发Elect的情况" class="headerlink" title="所有触发Elect的情况"></a>所有触发Elect的情况</h3><h3 id="partition-reassign"><a href="#partition-reassign" class="headerlink" title="partition reassign"></a><span id="PR">partition reassign</span></h3><p>在Kafka-manager上对topic——testReassign(包含3个分区)进行手动partition分配：</p>
<p>这是重分配之前的分区情况：</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/17-4-28/46811832-file_1493361667528_90d4.png" alt=""></p>
<p>调整0和1的分区Replicas：</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/17-5-10/19313383-file_1494385172260_105ee.png" alt=""></p>
<p>执行<strong>Reassign Partitions</strong>，观察日志：</p>
<p><em>PartitionsReassignedListener</em>感知到了ZK上<em>/admin/reassign_partitions</em>节点上内容发生了变化<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PartitionsReassignedListener on 2]: Partitions reassigned listener fired for path /admin/reassign_partitions. Record partitions to be reassigned &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;testReassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,&#123;&quot;topic&quot;:&quot;testReassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;,&#123;&quot;topic&quot;:&quot;testReassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,1]&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>由于分区2未进行重分配，其原来的replicas与现在的一致，因此无视该Reassign请求：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kafka.common.KafkaException: Partition [testReassign,2] to be reassigned is already assigned to replicas 1,2. Ignoring request for partition reassignment</span><br></pre></td></tr></table></figure></p>
<p>接下来对分区0和1进行重分配，就拿1来说：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Controller 2]: Handling reassignment of partition [testReassign,1] to new replicas 2,3 </span><br><span class="line">[Controller 2]: New replicas 2,3 for partition [testReassign,1] being reassigned not yet caught up with the leader </span><br><span class="line">[Controller 2]: Updated path /brokers/topics/testReassign with &#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;2&quot;:[1,2],&quot;0&quot;:[2,3],&quot;1&quot;:[2,3,1]&#125;&#125; for replica assignment </span><br><span class="line">[Controller 2]: Updated assigned replicas for partition [testReassign,1] being reassigned to 2,3,1  </span><br><span class="line">[[Controller 2]: Updating leader epoch for partition [testReassign,1]. </span><br><span class="line">[Controller 2]: Updated leader epoch for partition [testReassign,1] to 1 </span><br><span class="line">[Replica state machine on controller 2]: Invoking state change to NewReplica for replicas [Topic=testReassign,Partition=1,Replica=2] </span><br><span class="line">[Controller 2]: Waiting for new replicas 2,3 for partition [testReassign,1] being reassigned to catch up with the leader</span><br></pre></td></tr></table></figure></p>
<p><strong>疑问</strong>：往<strong>/brokers/topics/testReassign</strong>中写入的为啥是[2,3,1]而不是[2,3]呢？留个坑：在<em>KafkaController.onPartitionReassignment</em>方法中找答案吧~<br>该方法在多处被调用<br><img src="http://olt6kofv9.bkt.clouddn.com/17-4-28/73504112-file_1493368426005_d1af.png" alt=""></p>
<h3 id="partition-state-machine"><a href="#partition-state-machine" class="headerlink" title="partition state machine"></a><strong>partition state machine</strong></h3><p>下图简单描述了partition的4种状态：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-9/22464449-file_1494308280134_3a39.png" alt=""></p>
<p>状态机通过一个map类型的<code>partitionState</code>来存放所有分区的当前状态。</p>
<h4 id="start-up"><a href="#start-up" class="headerlink" title="start up"></a><strong>start up</strong></h4><p>此时cluster的controller已经产生，controller通过读取ZK的partition和ISR节点信息判断每个partition的当前状态。</p>
<p>具体是判断每个分区对应的<em>LeaderIsrAndControllerEpoch</em>信息与当前的Epoch是否吻合：</p>
<ul>
<li>如果吻合，则是OnLine状态</li>
<li>存在但不吻合，则是OffLine状态</li>
<li>不存在信息，则是New状态</li>
</ul>
<blockquote>
<p>划分好了当前的状态后，下面要将new和offline的转化成Online</p>
</blockquote>
<p>如何转化呢？当然是选举leader咯，关键代码在<code>OfflinePartitionLeaderSelector</code>这个类里，下面这个表是选举时遇到的几种情况<br>ISR|replicas|result<br>—|—|—<br>(1,2)|(1,2)|leader = 1<br>null|(2,3)|unclean enabled ? leader=2 : <em>NoReplicaOnlineException</em><br>null|null|<em>NoReplicaOnlineException</em></p>
<p>如当前有个repilca设置为1的topic：”jjhtest”，当前leader与isr的分配为：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-8/33844974-file_1494227473975_176f5.png" alt=""></p>
<p>接下来我重启broker3，会发生什么呢？</p>
<ol>
<li><p>由于每个分区的replicas都是固定的(不考虑手动分配)，关闭broker3将导致分区1,4,7的replicas中无可用的broker，这3个分区将无法分配到broker，进入<strong>offline</strong>状态</p>
<blockquote>
<p>如果分区的replica&gt;1，在<code>shutDownBroker</code>方法中：broker作为leader的分区状态转移是online ==&gt; online，leader选举的selector为：<code>ControlledShutdownPartitionLeaderSelector</code></p>
</blockquote>
</li>
<li><p>broker3恢复后，以上3个分区由<strong>offline</strong>状态向<strong>online</strong>转变，触发leader的选举</p>
</li>
<li><p>选举的过程：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-8/38963849-file_1494228534238_a5f5.png" alt=""></p>
<h4 id="partitionReplicaAssignment"><a href="#partitionReplicaAssignment" class="headerlink" title="partitionReplicaAssignment"></a><span id="PRA"><strong>partitionReplicaAssignment</strong></span></h4><p>该变量是个<em>Mutable</em> Map，存放的是每个分区对应的replicas信息。这个信息有两个来源：</p>
</li>
<li><p><strong>TopicChangeListener</strong>监听ZK上的topic变化，比如新增了topic，那么就会从ZK获取分区的replicas分配信息</p>
</li>
<li><a href="#PML"><strong>PartitionModificationsListener</strong></a>监听parition的变化，比如新增了分区，会从ZK获取新增分区的replicas信息。</li>
</ol>
<h4 id="electLeaderForPartition"><a href="#electLeaderForPartition" class="headerlink" title="electLeaderForPartition"></a><strong>electLeaderForPartition</strong></h4><p>当partition的状态由offline或者online ==&gt; online时，会通过不同的<em>PartitionLeaderSelector</em>选举leader(<em>LeaderSelector缩写为LS</em>)：</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/17-5-10/21966269-file_1494383478257_ab7f.png" alt=""></p>
<p>每次成功执行了leader select，都会更新<strong>leader cache</strong>(<code>controllerContext.partitionLeadershipInfo</code>)</p>
<h3 id="preferred-replica-election"><a href="#preferred-replica-election" class="headerlink" title="preferred replica election"></a><span id="PRE">preferred replica election</span></h3><blockquote>
<p>在controller启动的过程中会执行一次<strong>preferredReplicaElection</strong>(简称<strong>PRE</strong>)，并创建一个间隔为<code>leader.imbalance.check.interval.seconds</code>(default:300)的定时任务执行<strong>PartitionRebalance</strong>操作，该操作会触发<strong>PRE</strong></p>
</blockquote>
<p><strong>kafka平衡策略的实现依赖于以下几方面的因素</strong>：</p>
<ol>
<li>在不考虑手动分配分区的情况下，每个分区分配的<strong>Replicas</strong>是写死的，包括顺序都是固定的。</li>
<li>在出现broker挂掉的情况下，<strong>leader的重新选举能保证消息不丢失(未触发脏选举)，而PRE(首选备份选举)能保证broker恢复后分区的leader能均衡分布在集群上。</strong></li>
</ol>
<h4 id="小实验"><a href="#小实验" class="headerlink" title="小实验"></a>小实验</h4><p>有一个分区数为6，备份数为2的topic：”TheOne”，当前的分配状态为：</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/17-5-8/87366899-file_1494230972294_516e.png" alt=""></p>
<ul>
<li><p>关闭broker3后分区4和5的leader重新选举：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-8/90590726-file_1494231294553_16ebf.png" alt=""></p>
</li>
<li><p>broker3恢复后，又重新加入到了各个topic的ISR中(IsrChangeNotificationListener被触发)：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-8/47635529-file_1494231488624_78ec.png" alt=""></p>
</li>
<li><p>一段时间后，触发PRE：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Controller 1]: Starting preferred replica leader election for partitions [TheOne,5] (kafka.controller.KafkaController)</span><br><span class="line">[Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [TheOne,5] (kafka.controller.PartitionStateMachine)</span><br><span class="line">[PreferredReplicaPartitionLeaderSelector]: Current leader 1 for partition [TheOne,5] is not the preferred replica. Trigerring preferred replica leader election (kafka.controller.PreferredReplicaPartitionLeaderSelector)</span><br><span class="line">[Controller 1]: Partition [TheOne,5] completed preferred replica leader election. New leader is 3 (kafka.controller.KafkaController)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>leader &amp; ISR又恢复了初始状态，达到了均衡。</p>
<p>PRE的选举操作由<strong>preferredReplicaPartitionLeaderSelector</strong>完成，状态为<strong>online</strong> ==&gt; <strong>online</strong></p>
<blockquote>
<p><strong>思考</strong>：动态扩展broker如何实现？动态扩展broker能否实现分区的均衡扩展呢？</p>
</blockquote>
<h3 id="partition-reblance"><a href="#partition-reblance" class="headerlink" title="partition reblance"></a><span id="rebalance">partition reblance</span></h3><ul>
<li>分区重平衡的间隔上面提到过了，默认为5分钟。</li>
<li><p>触发重平衡由失衡率(imbalanceRatio)决定:<br>$$ ratio = Sum(partitonsNotLeaded)/Sum(partitonsShouldLeaded)$$<br>当ratio大于<em>leader.imbalance.per.broker.percentage</em>(默认10%)时会触发重平衡</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DEBUG: topics not in preferred replica Map() (kafka.controller.KafkaController)</span><br><span class="line">TRACE: leader imbalance ratio for broker 2 is 0.000000 (kafka.controller.KafkaController)</span><br><span class="line">DEBUG: topics not in preferred replica Map() (kafka.controller.KafkaController)</span><br><span class="line">TRACE: leader imbalance ratio for broker 1 is 0.000000 (kafka.controller.KafkaController)</span><br><span class="line">DEBUG: topics not in preferred replica Map([TheOne,4] -&gt; List(3, 2), [TheOne,5] -&gt; List(3, 1),...) (kafka.controller.KafkaController)</span><br><span class="line">TRACE: leader imbalance ratio for broker 3 is 0.951613 (kafka.controller.KafkaController)</span><br></pre></td></tr></table></figure>
</li>
<li><p>重平衡的过程主要是执行<strong>PRE</strong></p>
</li>
</ul>
<h3 id="Replica-state-machine"><a href="#Replica-state-machine" class="headerlink" title="Replica state machine"></a><strong>Replica state machine</strong></h3><p>与partition相比多了3种状态</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/17-5-9/61651338-file_1494313792247_3a25.png" alt=""></p>
<h4 id="create-topic"><a href="#create-topic" class="headerlink" title="create topic"></a><span id="createTopic"><strong>create topic</strong></span></h4><p>新建topic涉及到partition以及replica状态的改变：</p>
<ul>
<li><strong>NoExistent ==&gt; New</strong><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Invoking state change to NewPartition for partitions [TT,6],...(kafka.controller.PartitionStateMachine)</span><br><span class="line">Invoking state change to NewReplica for replicas [Topic=TT,Partition=6,Replica=2],[Topic=TT,Partition=6,Replica=3],... (kafka.controller.ReplicaStateMachine)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>在新建topic后，ZK就会为该topic分配replicas</p>
<ul>
<li><strong>New ==&gt; Online</strong><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#partition state machine</span><br><span class="line">Invoking state change to OnlinePartition for partitions [TT,6],...</span><br><span class="line">Live assigned replicas for partition [TT,6] are: [List(2, 3)]</span><br><span class="line">Initializing leader and isr for partition [TT,6] to (Leader:2,ISR:2,3,LeaderEpoch:0,ControllerEpoch:30)</span><br><span class="line">#replica state machine</span><br><span class="line">Invoking state change to OnlineReplica for replicas [Topic=TT,Partition=6,Replica=2],[Topic=TT,Partition=6,Replica=3],...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="ReplicaDeletionStarted"><a href="#ReplicaDeletionStarted" class="headerlink" title="ReplicaDeletionStarted"></a><strong>ReplicaDeletionStarted</strong></h4><p>该状态一般在进行手动分区分配时产生的，以<a href="#PR">parition reassign实验</a>试验中的分区1为例：replicas：[3,1] ==&gt; [2,3]</p>
<ul>
<li>broker2成为<strong>NewReplica</strong>：<code>Invoking state change to NewReplica for replicas [Topic=testReassign,Partition=1,Replica=2]</code></li>
<li>broker2进入<strong>OnlineReplica</strong>：<code>Invoking state change to OnlineReplica for replicas [Topic=testReassign,Partition=1,Replica=2]</code></li>
<li><p>broker1成为<strong>OfflineReplica</strong>，从ISR中移除：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Invoking state change to OfflineReplica for replicas [Topic=testReassign,Partition=1,Replica=1]</span><br><span class="line">Removing replica 1 from ISR 3,1 for partition [testReassign,1]</span><br></pre></td></tr></table></figure>
</li>
<li><p>broker1进入<strong>ReplicaDeletionStarted</strong>状态:<code>Invoking state change to ReplicaDeletionStarted for replicas [Topic=testReassign,Partition=1,Replica=1]</code></p>
</li>
<li>broker1进入<strong>ReplicaDeletionSuccessful</strong> ：<code>nvoking state change to ReplicaDeletionSuccessful for replicas [Topic=testReassign,Partition=1,Replica=1]</code></li>
<li>broker1进入<strong>NonExistentReplica</strong>：<code>Invoking state change to NonExistentReplica for replicas [Topic=testReassign,Partition=1,Replica=1]</code></li>
</ul>
<h1 id="ZKListener-详解"><a href="#ZKListener-详解" class="headerlink" title="ZKListener 详解"></a>ZKListener 详解</h1><blockquote>
<p>下面对Controller中使用到的一些监听ZK节点及子节点变化的listener进行梳理</p>
</blockquote>
<table>
<thead>
<tr>
<th>listener</th>
<th>监听的节点</th>
<th>用处</th>
<th>处理逻辑</th>
</tr>
</thead>
<tbody>
<tr>
<td>BrokerChangeListener</td>
<td>/brokers/ids 子节点</td>
<td>在ReplicaStateMachine中使用</td>
<td><a href="#BCL">BCL</a></td>
</tr>
<tr>
<td>ISRChangeNotificationListener</td>
<td>/isr_change_notification 子节点</td>
<td>Controller用于监听各个partition的ISR变化</td>
<td><a href="#ICNL">ICNL</a></td>
</tr>
<tr>
<td>PartitionModificationsListener</td>
<td>/brokers/topics/topic_name</td>
<td>监听对应topic分区的变化(只允许增加)</td>
<td><a href="#PML">PML</a></td>
</tr>
<tr>
<td>LeaderChangeListener</td>
<td>/controller</td>
<td>监听集群leader的变化</td>
<td><a href="#register">注册监听</a></td>
</tr>
</tbody>
</table>
<h2 id="Broker-Change-Listener"><a href="#Broker-Change-Listener" class="headerlink" title="Broker Change Listener"></a><span id="BCL">Broker Change Listener</span></h2><p>/brokers/ids目录下有多个以broker_id命名的子节点，每个子节点上存储着与broker相关的信息，如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;jmx_port&quot;:9999,&quot;timestamp&quot;:&quot;1493886226618&quot;,&quot;endpoints&quot;:[&quot;PLAINTEXT://ip:9092&quot;],&quot;host&quot;:&quot;ip&quot;,&quot;version&quot;:3,&quot;port&quot;:9092&#125;</span><br></pre></td></tr></table></figure></p>
<p>当这些子节点的内容发生变化时，controller感知到后会统计出变化的情况：</p>
<ul>
<li>broker2 shutdown：<code>Newly added brokers: , deleted brokers: 2, all live brokers: 1,3</code></li>
<li>broker2 startup： <code>Newly added brokers: 2, deleted brokers: , all live brokers: 1,2,3</code></li>
</ul>
<h3 id="delete-broker"><a href="#delete-broker" class="headerlink" title="delete broker"></a>delete broker</h3><h4 id="shutdownBroker"><a href="#shutdownBroker" class="headerlink" title="shutdownBroker"></a><strong>shutdownBroker</strong></h4><blockquote>
<p>该方法与<strong>KafkaServer</strong>中的<em>shutdown</em>方法的区别在于前者用于处理<code>ControlledShutdown</code>请求，将分区的职责转移出去，而后者是关闭brokerServer的一堆服务。</p>
</blockquote>
<p>需要处理的分区为：<br>$ List(partition) = { partition | id ∈ Replicas(partition) &amp;&amp; replicaFactor(partition) &gt; 1 } $</p>
<ol>
<li>对于作为leader的分区，执行leader的重选举(<code>ControlledShutdownLS</code>)</li>
<li>对于作为普通replica的分区：<ul>
<li>关闭<strong>ReplicaRequest</strong>,如broker3是<em>TheOne-1</em>的replica，在关闭broker3时controller会向3发送关闭replica请求的<strong>请求</strong>：<code>The stop replica request (delete = false) sent to broker 3 is [Topic=TheOne,Partition=1,Replica=3](kafka.controller.ControllerBrokerRequestBatch)</code><ul>
<li>broker3收到<strong>StopReplica</strong>请求后调用<em>ReplicaManager</em>的<em>stopReplicas</em>移除了相关的replica的Fetcher：<code>[ReplicaFetcherManager on broker 3] Removed fetcher for partitions TheOne-1</code></li>
</ul>
</li>
<li>将broker3的Replica状态置为<strong>OfflineReplica</strong></li>
</ul>
</li>
</ol>
<h4 id="onBrokerFailure"><a href="#onBrokerFailure" class="headerlink" title="onBrokerFailure"></a><strong>onBrokerFailure</strong></h4><p>步骤如下：</p>
<ol>
<li>在<em>shutdownBroker</em>的处理的分区中有一项要求是：replicaFactor &gt; 1，而对于replicaFactor = 1的分区来说，如果其leader所在的broker关闭了，那么该分区的状态将置为<strong>OfflinePartition</strong>。筛选的依据是其leader是否为关闭的broker(其他类型的分区在上面的流程中要么leader重新选举过了，要么只是改变ISR)</li>
<li>尝试使用<code>OfflinePartitionLS</code>将offline和new类型的partition转化为<strong>Online</strong></li>
<li><p>集体性的将该broker的replica的状态置为<strong>OfflineReplica</strong></p>
<blockquote>
<p><del>感觉重复操作了，因为shutdownBroker阶段也有这操作。</del>不同点在于shutdownBroker只处理replica&gt;1分区的replica状态。</p>
</blockquote>
</li>
<li><p>如果第一步操作未执行，那么将向其他broker发送一个<strong>UpdateMetadataRequest</strong></p>
<h3 id="onBrokerStartup"><a href="#onBrokerStartup" class="headerlink" title="onBrokerStartup"></a>onBrokerStartup</h3><p>主要处理的依然是分区、replica、ISR以及leader的转化：</p>
</li>
<li><p>发送<strong>UpdateMetadataRequest</strong>（具体原因不明）</p>
</li>
<li>将该broker的replica状态置为<strong>OnlineReplica</strong></li>
<li>尝试使用<code>OfflinePartitionLS</code>将offline和new类型的partition转化为<strong>Online</strong>，这一步的主要目的是为了将<strong>onBrokerFailue</strong>阶段呗置为<strong>OffLinePartition</strong>的分区恢复成<strong>Online</strong>：<code>[OfflinePartitionLeaderSelector]: No broker in ISR is alive for [jjhtest,1]. Pick the leader from the alive assigned replicas: 3
 [OfflinePartitionLeaderSelector]: No broker in ISR is alive for [jjhtest,1]. Elect leader 3 from live brokers 3. There is potential data loss.
 [OfflinePartitionLeaderSelector]: Selected new leader and ISR {&quot;leader&quot;:3,&quot;leader_epoch&quot;:23,&quot;isr&quot;:[3]} for offline partition [jjhtest,1]</code></li>
<li>判断是否需要执行分区重分配的操作</li>
</ol>
<h2 id="ISR-Change-Notification-Listener"><a href="#ISR-Change-Notification-Listener" class="headerlink" title="ISR Change Notification Listener"></a><span id="ICNL">ISR Change Notification Listener</span></h2><p>集群正常运行时/isr_change_notification节点下面是没有子节点的，当ISR发生变化的时候，如重启某个broker，那些ISR原来包含该broker的分区需要调整ISR，因此会创建这些partition的子节点。</p>
<p>下面是broker3挂掉后，TheOne这个topic的ISR：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-5-11/41268388-file_1494473080893_14c67.png" alt=""></p>
<p>在broker3恢复后，ISNL感知到了某些partition的ISR发生了变化，进行下面3步操作：</p>
<ol>
<li>根据子节点的内容更新对应parition的leader以及ISR cache (从ZK上获取)</li>
<li>向集群所有节点发送<strong>MetadataRequest</strong>：<code>DEBUG Sending MetadataRequest to Brokers:ArrayBuffer(1, 2, 3) for TopicAndPartitions:Set([TheOne, 2], [TheOne, 4],...)</code></li>
<li>删除子节点</li>
</ol>
<h2 id="Partition-Modifications-Listener"><a href="#Partition-Modifications-Listener" class="headerlink" title="Partition Modifications Listener"></a><span id="PML">Partition Modifications Listener</span></h2><blockquote>
<p>Contorller为每个topic都设置了一个<em>PartitionModificationsListener</em>，用于监听新增分区的操作</p>
</blockquote>
<p>下面是将<em>TheOne</em>的分区数增加3，PML所感知到的信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[AddPartitionsListener on 1]: Partition modification trigered &#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;8&quot;:[3,2],&quot;4&quot;:[3,2],&quot;5&quot;:[3,1],&quot;6&quot;:[1,3],&quot;1&quot;:[1,3],&quot;0&quot;:[1,2],&quot;2&quot;:[2,3],&quot;7&quot;:[2,1],&quot;3&quot;:[2,1]&#125;&#125; for path /brokers/topics/TheOne</span><br><span class="line">[AddPartitionsListener on 1]: New partitions to be added Map([TheOne,7] -&gt; List(2, 1), [TheOne,6] -&gt; List(1, 3), [TheOne,8] -&gt; List(3, 2))</span><br></pre></td></tr></table></figure></p>
<p>新增了Partition，ZK已经为这些partition分配了replicas，在ZK上每个topic的分区上存储着的信息如：<code>{&quot;controller_epoch&quot;:30,&quot;leader&quot;:3,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[3,2]}</code></p>
<p>拿到这些消息后，Controller要做的第一件事就是将新增的分区的replicas信息添加到<a href="#PRA">partitionReplicaAssignment</a>中，接下来才是处理这些新建的分区的状态、leader等，这个操作与<a href="#createTopic">create topic</a>的处理如出一辙，区别在于前者是对新增的分区进行处理，后者是对这个topic的所有分区进行处理</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2017/04/24/ISR消息管理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/24/ISR消息管理/" itemprop="url">ISR消息管理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-24T17:09:57+08:00">
                2017-04-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2017/04/24/ISR消息管理/" class="leancloud_visitors" data-flag-title="ISR消息管理">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h1><ul>
<li>leader是在何时更新Highwater的？</li>
<li>kafka-manager上出现Lag为负值是什么原因造成的？</li>
<li>Log中的消息被删除时，ISR之间是如何协调的？</li>
</ul>
<blockquote>
<p>下面所有的讨论都是基于一个包含3个broker的kafka集群而言的</p>
</blockquote>
<h1 id="Replica"><a href="#Replica" class="headerlink" title="Replica"></a>Replica</h1><h2 id="leader-vs-followers"><a href="#leader-vs-followers" class="headerlink" title="leader vs followers"></a>leader vs followers</h2><p>如果将所有的topic的<code>replicas</code>设置为2(_consumer_offsets除外)，那么对于每个partition而言其Log存在于两个broker上，其中一个作为<strong>leader</strong>，另一个作为<strong>followers</strong></p>
<p>下图是一个包含3个分区的topic的replicas以及leader分布情况：<br><img src="http://olt6kofv9.bkt.clouddn.com/17-4-25/97528352-file_1493085463058_12a88.png" alt=""></p>
<h3 id="leader与followers的职责"><a href="#leader与followers的职责" class="headerlink" title="leader与followers的职责"></a>leader与followers的职责</h3><p>之前在<a href="http://blog.leanote.com/post/584933245@qq.com/Kafka-consumer%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90" target="_blank" rel="noopener">kafka-consumer剖析</a>的文章中介绍过<em>high watermark</em>(简称<strong>HW</strong>)以及<em>log end offset</em>(简称<strong>LEO</strong>)的概念。作为leader，其主要职责是：</p>
<ul>
<li>将消息及offset写入Local log &amp; offset</li>
<li>在followers完全备份了消息后(leader应该是会收到通知)，更新HW</li>
<li>leader并不需要维护LEO的值，因为在Log中有一个<code>nextOffsetMetadata</code>属性就是每个Log维护的最新offset的信息</li>
</ul>
<p>反观followers，其职责则体现在与leader的交互上：</p>
<ul>
<li>备份消息，然后通知leader</li>
<li>更新LEO的值</li>
</ul>
<h2 id="创建Replica"><a href="#创建Replica" class="headerlink" title="创建Replica"></a>创建Replica</h2><p>因为每个partition都有对应的replicas，所以创建replica的操作是在<em>Partition</em>中执行的，具体的方法是<code>getOrCreateReplica</code>：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (isReplicaLocal(replicaId)) &#123;</span><br><span class="line">  <span class="comment">//创建Log文件</span></span><br><span class="line">  <span class="keyword">val</span> config = <span class="type">LogConfig</span>.fromProps(...)</span><br><span class="line">  <span class="keyword">val</span> log = logManager.createLog(<span class="type">TopicAndPartition</span>(topic, partitionId), config)</span><br><span class="line">  <span class="comment">//获取checkPoint文件</span></span><br><span class="line">  <span class="keyword">val</span> checkpoint = replicaManager.highWatermarkCheckpoints(log.dir.getParentFile.getAbsolutePath)</span><br><span class="line">  <span class="keyword">val</span> offsetMap = checkpoint.read</span><br><span class="line">  <span class="keyword">val</span> offset = offsetMap.getOrElse(<span class="type">TopicAndPartition</span>(topic, partitionId), <span class="number">0</span>L).min(log.logEndOffset)</span><br><span class="line">  <span class="keyword">val</span> localReplica = <span class="keyword">new</span> <span class="type">Replica</span>(replicaId, <span class="keyword">this</span>, time, offset, <span class="type">Some</span>(log))</span><br><span class="line">  addReplicaIfNotExists(localReplica)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> remoteReplica = <span class="keyword">new</span> <span class="type">Replica</span>(replicaId, <span class="keyword">this</span>, time)</span><br><span class="line">  addReplicaIfNotExists(remoteReplica)</span><br><span class="line">&#125;</span><br><span class="line">getReplica(replicaId).get</span><br></pre></td></tr></table></figure></p>
<p>其中：</p>
<ul>
<li><em>getReplica</em>与<em>addReplicaIfNotExists</em>都对<code>assignedReplicaMap</code>进行操作</li>
<li>checkpoint里维护着该log.dir下面所有topic-parition与offset的对应关系</li>
<li>log.dir是Log文件所在目录，log.dir.getParentFile就是<em>server.properties</em>中定义的<code>log.dirs</code></li>
</ul>
<h1 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h1><blockquote>
<p>创建Replica的调用路径是<em>ReplicaManager.becomeLeaderOrFollower-&gt;makeFollowers</em>。ReplicaManager是broker范畴的，管理着broker上面所有的partition</p>
</blockquote>
<h2 id="OffsetCheckPoint"><a href="#OffsetCheckPoint" class="headerlink" title="OffsetCheckPoint"></a>OffsetCheckPoint</h2><p>在多磁盘的环境中，log.dirs会定义在多个磁盘上，这样就能将Partiton分布在不同的磁盘上。</p>
<p>每个磁盘中的Log目录下都维护着3个OffsetCheckPoint文件：<br>: recovery-point-offset-checkpoint<br>: replication-offset-checkpoint：维护parition与offset信息<br>: cleaner-offset-checkpoint</p>
<p>replication-offset-checkpoint的内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">21</span><br><span class="line">__consumer_offsets 16 0</span><br><span class="line">__consumer_offsets 49 124</span><br><span class="line">admin.benchmark 9 24564634</span><br><span class="line">test 13 16709</span><br><span class="line">theOne 18 0</span><br><span class="line">__consumer_offsets 4 6</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>其中：</p>
<ul>
<li>第一行是<code>CurrentVersion</code>的值，默认是0</li>
<li>第二行是当前Log目录下面分区的数量</li>
<li>后面紧跟着的21行是分区以及对应的offset信息，格式为：<code>$topic $partition $offset</code></li>
</ul>
<p>在ReplicaManager中会将所有log目录下面的replication-offset-checkpoint组成一个名为<code>highWatermarkCheckpoints</code>的Map:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val highWatermarkCheckpoints = </span><br><span class="line">  config.logDirs.map(dir =&gt; (new File(dir).getAbsolutePath, </span><br><span class="line">  new OffsetCheckpoint(new File(dir,ReplicaManager.HighWatermarkFilename)))).toMap</span><br></pre></td></tr></table></figure></p>
<p>其更新操作是由一个定时任务控制的，每隔<code>replica.high.watermark.checkpoint.interval.ms</code>(默认5s)的时间会将所有log目录下的raplica的HW值写入到对应的replication-offset-checkpoint文件中。该定时任务是在<code>becomeLeaderOrFollower</code>方法中被开启的。</p>
<h2 id="appendMessages"><a href="#appendMessages" class="headerlink" title="appendMessages"></a>appendMessages</h2><p>在<a href="http://blog.leanote.com/post/584933245@qq.com/%E6%B6%88%E6%81%AF%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98%E5%8F%8A%E5%A4%87%E4%BB%BD%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90" target="_blank" rel="noopener">Log分析</a>曾提到过<code>appendMessages</code>方法，该方法内先将消息写入local(即Leader的Log)，然后判断ack是否=-1来决定是否需要创建延迟的producer请求(不立即反馈，需要等待备份完成，由<code>delayedProducePurgatory</code>进行管理)。<del>该请求是从leader发往followers的</del>，内容包含：</p>
<ul>
<li>delayedMs：即producer配置的<em>request.timeout</em></li>
<li>requiredOffset: Leader在写入消息后的<em>InextOffsetMetadata</em>值(用<code>LogAppendInfo.lastOffset + 1</code>表示)</li>
</ul>
<p>在ack = -1的策略下，leader必须在所有的followers都将消息备份的情况下，才会向producer发送反馈，而判断消息是否已完整备份的方法是<code>Partition.checkEnoughReplicasReachOffset</code>：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def checkEnoughReplicasReachOffset(requiredOffset: Long): (Boolean, Errors) = &#123;</span><br><span class="line">  leaderReplicaIfLocal() match &#123;</span><br><span class="line">    case Some(leaderReplica) =&gt;</span><br><span class="line">      val curInSyncReplicas = inSyncReplicas</span><br><span class="line">      //对ISR中所有replicas的LEO校验是否已跟上leader的步伐(包括leader)</span><br><span class="line">      def numAcks = curInSyncReplicas.count &#123; r =&gt;</span><br><span class="line">        if (!r.isLocal)</span><br><span class="line">          if (r.logEndOffset.messageOffset &gt;= requiredOffset) &#123;</span><br><span class="line">            true</span><br><span class="line">          &#125;</span><br><span class="line">          else</span><br><span class="line">            false</span><br><span class="line">        else</span><br><span class="line">          true </span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      val minIsr = leaderReplica.log.get.config.minInSyncReplicas</span><br><span class="line">      </span><br><span class="line">      if (leaderReplica.highWatermark.messageOffset &gt;= requiredOffset) &#123;</span><br><span class="line">        if (minIsr &lt;= curInSyncReplicas.size)</span><br><span class="line">          (true, Errors.NONE)</span><br><span class="line">        else</span><br><span class="line">          (true, Errors.NOT_ENOUGH_REPLICAS_AFTER_APPEND)</span><br><span class="line">      &#125; else</span><br><span class="line">        (false, Errors.NONE)</span><br><span class="line">    case None =&gt;</span><br><span class="line">      (false, Errors.NOT_LEADER_FOR_PARTITION)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>第一个返回参数表示是否有足够多的replicas达到了<em>requiredOffset</em></p>
</blockquote>
<ul>
<li>如果<em>HW &lt; requiredOffset</em>，表明leader还未收到足够多的replicas的response</li>
<li><em>HW &gt;= requiredOffset</em>表明HW已经更新过了，此时如果ISR的数量小于misISR，那么就报<code>NOT_ENOUGH_REPLICAS_AFTER_APPEND</code>错</li>
<li>如果当前ReplicaManger发现local不再是leader了，说明这个broker出现问题了，报<code>NOT_LEADER_FOR_PARTITION</code>错</li>
</ul>
<h3 id="Increment-HW"><a href="#Increment-HW" class="headerlink" title="Increment HW"></a><span id="jump">Increment HW</span></h3><p>HW是由leader维护的，其更新时机主要有两种：</p>
<ol>
<li>ISR发生变化，包括：</li>
</ol>
<ul>
<li>新的broker成为leader</li>
<li>shrink ISR</li>
<li>expand ISR</li>
</ul>
<ol start="2">
<li>Replicas已备份完新的消息，判别的方法有两种：<ol>
<li>ISR中Replicas中的最小LEO值大于HW</li>
<li>HW的<em>LogOffsetMetadata</em>存在一个老的log segment中，表明上一次Producer请求已处理完成(即消息已经在完成备份)</li>
</ol>
</li>
</ol>
<h3 id="ShrinkIsr"><a href="#ShrinkIsr" class="headerlink" title="ShrinkIsr"></a>ShrinkIsr</h3><h3 id="becomeLeaderOrFollower"><a href="#becomeLeaderOrFollower" class="headerlink" title="becomeLeaderOrFollower"></a>becomeLeaderOrFollower</h3><h2 id="ReplcaFetcher"><a href="#ReplcaFetcher" class="headerlink" title="ReplcaFetcher"></a>ReplcaFetcher</h2><h3 id="启动过程ReplicaManger相关日志记录"><a href="#启动过程ReplicaManger相关日志记录" class="headerlink" title="启动过程ReplicaManger相关日志记录"></a>启动过程ReplicaManger相关日志记录</h3><p>以broker3启动日志为例：<br>在Log加载、处理完后，首先是unblock操作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 producer requests. (kafka.server.ReplicaManager)</span><br><span class="line">[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 producer requests. (kafka.server.ReplicaManager)</span><br><span class="line">[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 fetch requests. (kafka.server.ReplicaManager)</span><br><span class="line">[2017-04-25 13:35:57,270] DEBUG [Replica Manager on Broker 3]: Request key __consumer_offsets-25 unblocked 0 fetch requests. (kafka.server.ReplicaManager)</span><br></pre></td></tr></table></figure></p>
<p>应该是解封在关闭阶段被阻塞的一些request</p>
<p>下面开启ReplicaFetcherThread：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2017-04-25 13:35:57,367] INFO [ReplicaFetcherThread-0-1], Starting  (kafka.server.ReplicaFetcherThread)</span><br><span class="line">[2017-04-25 13:35:57,384] INFO [ReplicaFetcherThread-3-1], Starting  (kafka.server.ReplicaFetcherThread)</span><br><span class="line">[2017-04-25 13:35:57,384] INFO [ReplicaFetcherThread-1-2], Starting  (kafka.server.ReplicaFetcherThread)</span><br><span class="line">[2017-04-25 13:35:57,398] INFO [ReplicaFetcherThread-2-2], Starting  (kafka.server.ReplicaFetcherThread)</span><br><span class="line">[2017-04-25 13:35:57,406] INFO [ReplicaFetcherThread-1-1], Starting  (kafka.server.ReplicaFetcherThread)</span><br><span class="line">[2017-04-25 13:35:57,414] INFO [ReplicaFetcherThread-0-2], Starting  (kafka.server.ReplicaFetcherThread)</span><br><span class="line">[2017-04-25 13:35:57,423] INFO [ReplicaFetcherThread-2-1], Starting  (kafka.server.ReplicaFetcherThread)</span><br><span class="line">[2017-04-25 13:35:57,435] INFO [ReplicaFetcherThread-3-2], Starting  (kafka.server.ReplicaFetcherThread)</span><br></pre></td></tr></table></figure></p>
<p>数字编号的含义是什么呢？？</p>
<p>接下来是添加Fetcher请求，向Remote Broker请求那些本地不是leader的分区的数据。摘取片段如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO [ReplicaFetcherManager on broker 3] Added fetcher for partitions List([topic1-8, initOffset 6538 to broker BrokerEndPoint(1,10.45.4.9,9092)] ,[topic2-9, initOffset 0 to broker BrokerEndPoint(1,10.45.4.9,9092)]...)</span><br></pre></td></tr></table></figure></p>
<h3 id="ReplicaFetcherThread"><a href="#ReplicaFetcherThread" class="headerlink" title="ReplicaFetcherThread"></a>ReplicaFetcherThread</h3><blockquote>
<p>每个ReplicaManager不仅要维护一个producer的炼狱(<em>delayedProducePurgatory</em>)，还维护了一个fetcher的炼狱(<em>delayedFetchPurgatory</em>)。fetch这个操作不仅仅是由consumer触发的，followers备份消息也是通过向leader fetch实现的。</p>
</blockquote>
<h4 id="ReplicaFetcherManager"><a href="#ReplicaFetcherManager" class="headerlink" title="ReplicaFetcherManager"></a><strong>ReplicaFetcherManager</strong></h4><ul>
<li><em>ReplicaFetcherManager</em>管理着ReplicaFetcherThread的创建和关闭工作。</li>
<li>该manager的命名格式为：<em>“ReplicaFetcherManager on broker $ID”</em></li>
</ul>
<h4 id="thread"><a href="#thread" class="headerlink" title="thread"></a><strong>thread</strong></h4><ul>
<li>fetcher的数量由<code>num.replica.fetchers</code>控制：<br><code>(31 * topic.hashCode() + partitionId) % numFetchers</code>，当该参数设置为4，那么 $fetcherId∈[0,1,2,3]$，对于3节点replicas=2的集群，每个broker需要$4*2=8$个fetcherThread：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val partitionsPerFetcher = partitionAndOffsets.groupBy&#123; case(topicAndPartition, brokerAndInitialOffset) =&gt;</span><br><span class="line">  BrokerAndFetcherId(brokerAndInitialOffset.broker, getFetcherId(topicAndPartition.topic, topicAndPartition.partition))&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>上面的代码中将<strong>partition-broker</strong>这个map按照<em>broker-&gt;fetcherId</em>的对应关系进行分组。</p>
<ul>
<li>fetcherThread的命名方式为：<em>ReplicaFetcherThread-fetcherId-brokerId</em></li>
<li>ReplicaFetcherThread是个定时执行的线程，执行间隔由<code>replica.fetch.backoff.ms</code>控制，默认为1s</li>
<li>每个ReplicaFethcerThread的fetch目标是一个broker上的若干topic-partition</li>
</ul>
<h4 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a><strong>工作流程</strong></h4><p><img src="http://olt6kofv9.bkt.clouddn.com/17-4-26/115323-file_1493196133164_114f9.png" alt=""></p>
<h4 id="processPartitionData"><a href="#processPartitionData" class="headerlink" title="processPartitionData"></a><strong>processPartitionData</strong></h4><p>当前：<br>: broker2作为test-1的follower<br>: 其LEO值为17314<br>: leader的<em>HW=17314</em><br>: FetchRequest中的<em>fetchOffset = 17314</em></p>
<ul>
<li><p>收到一条消息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17314 for partition test-1. Received 45 messages and leader hw 17314</span><br></pre></td></tr></table></figure>
</li>
<li><p>对log执行append操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17315 after appending 45 bytes of messages for partition test-1</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>与<code>appendMessagesToLeader</code>时不相同的是，follower执行append操作不会再进行offset配置操作，用得就是leader传过来的offset</p>
</blockquote>
<ul>
<li>设置follower的HW值<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ReplicaFetcherThread-3-1], Follower 2 set replica high watermark for partition [test,1] to 17314</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>1s后再次fetch到消息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17315 for partition test-1. Received 45 messages and leader hw 17315</span><br><span class="line">[ReplicaFetcherThread-3-1], Follower 2 has replica log end offset 17316 after appending 45 bytes of messages for partition test-1</span><br><span class="line">[ReplicaFetcherThread-3-1], Follower 2 set replica high watermark for partition [test,1] to 17315</span><br></pre></td></tr></table></figure></p>
<p>由上面的日志可以注意到：</p>
<ol>
<li>LEO的值由17314 =&gt; 17315，这是因为前面执行的<em>Log.append</em>操作使得<em>nextOffsetMetadata</em>的值+1</li>
<li>leader的HW值变成了17315,这是由于leader检测到了followers(就是broker2)LEO的最小值已经变成了17315，所以更新了HW值,具体实现在<a href="#jump">Increment HW</a>中有介绍</li>
</ol>
<h4 id="handleOffsetOutOfRange"><a href="#handleOffsetOutOfRange" class="headerlink" title="handleOffsetOutOfRange"></a><strong>handleOffsetOutOfRange</strong></h4><blockquote>
<p>当ReplicaFether的fetchOffset不在leader的offset之内(即大于最大值或者小于最小值)，那么就会收到<code>OffsetOutOfRangeException</code></p>
</blockquote>
<p>假设当前test-1这个partition的leader为1，新增broker2作为follower</p>
<p><img src="http://olt6kofv9.bkt.clouddn.com/17-4-27/31207104-file_1493259488522_397.png" alt=""></p>
<p>broker2在catch up的过程中broker1挂了，broker2被选为leader。(需要<code>unclean.leader.election.enable</code>配置为1，才能允许这样的<em>脏选举</em>)</p>
<h5 id="情况一、Replica-fetchOffset-gt-leader-LEO"><a href="#情况一、Replica-fetchOffset-gt-leader-LEO" class="headerlink" title="情况一、Replica fetchOffset &gt; leader LEO"></a><strong>情况一、Replica fetchOffset &gt; leader LEO</strong></h5><p><img src="http://olt6kofv9.bkt.clouddn.com/17-4-27/51702971-file_1493259483396_ca38.png" alt=""><br>broker1在恢复后，成为了follower，向leader发送ReplicaFetcherRequest，其中<em>fetchOffset=7</em>，该值大于leader的LEO，所以需要将broker1上该分区的Log执行truncate操作使得LEO值与leader保持一致。</p>
<blockquote>
<p>值得注意的是，这样的truncate操作后，follower与leader的消息并不完全一致的，上例中，offset为3和4的消息就是不一致的</p>
</blockquote>
<h5 id="情况二、Replica-fetchOffset-lt-leader-start-offset"><a href="#情况二、Replica-fetchOffset-lt-leader-start-offset" class="headerlink" title="情况二、Replica fetchOffset &lt; leader start offset"></a><strong>情况二、Replica fetchOffset &lt; leader start offset</strong></h5><p><img src="http://olt6kofv9.bkt.clouddn.com/17-4-27/69901377-file_1493260935890_14937.png" alt=""></p>
<p>broker1在恢复之前，broker2添加了很多消息，并且也删除了一个消息，导致其最小的offset大于broker1中的LEO，这种情况下，broker1中的所有消息都没有意义了(因为ISR中的leader不再维护这些消息)，所以就删除掉所有的segment，然后fetch leader的第一条消息</p>
<blockquote>
<p>Kafka_0.10.1.1版本对于收到<em>OffsetOutOfRangeException</em>时follower的LEO处于leader开始与LEO之间的情况没有对策。</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://flytoair.github.io/2017/04/01/SocketServer工作原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="fbZhu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天外飞猪的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/01/SocketServer工作原理/" itemprop="url">SocketServer工作原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-01T16:33:36+08:00">
                2017-04-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2017/04/01/SocketServer工作原理/" class="leancloud_visitors" data-flag-title="SocketServer工作原理">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>kafka的socket server是基于java NIO,使用<code>Reactor</code>模式开发的。socketserver主要用于处理kafka server对外提交网络请求的操作,用于检查连接数,把请求添加到请求的队列中,对KafkaApis提供操作支持.</p>
</blockquote>
<p>其线程模型为：</p>
<ul>
<li>一个Acceptor线程接受/处理所有的新连接</li>
<li>N个Processor线程,每个Processor都有自己的selector,从每个连接中读取请求。数量由<code>num.network.threads</code>控制，默认为3</li>
<li>M个Handler线程处理请求,并将产生的请求返回给Processor线程用于写回客户端。数量由<code>queued.max.requests</code>控制，默认为500</li>
</ul>
<p><img src="http://olt6kofv9.bkt.clouddn.com/17-3-30/36381884-file_1490844340908_13db4.png" alt=""></p>
<h1 id="Acceptor"><a href="#Acceptor" class="headerlink" title="Acceptor"></a>Acceptor</h1><p><del>每个broker只有一个acceptor进程</del>每个<code>endPoint</code>(由server.properties中的<code>listeners</code>定义)对应一个Acceptor，采用轮询的方式来处理新的连接，步骤如下：</p>
<h2 id="1-新建ServerSocketChannel"><a href="#1-新建ServerSocketChannel" class="headerlink" title="1. 新建ServerSocketChannel"></a><strong>1. 新建ServerSocketChannel</strong></h2><p>使用endPoint的<code>ip:port</code><strong>strong text</strong>作为channel监听的socket地址：<br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> serverChannel = <span class="type">ServerSocketChannel</span>.open()</span><br><span class="line">serverChannel.configureBlocking(<span class="literal">false</span>)</span><br><span class="line"><span class="keyword">if</span> (recvBufferSize != <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>)</span><br><span class="line">  serverChannel.socket().setReceiveBufferSize(recvBufferSize)</span><br><span class="line">serverChannel.socket.bind(socketAddress)</span><br></pre></td></tr></table></figure></p>
<p>recvBufferSize的大小由<code>socket.receive.buffer.bytes</code>控制，默认是100KB，目前生产环境上配置的是1MB</p>
<h2 id="2-selector绑定channel，开始工作"><a href="#2-selector绑定channel，开始工作" class="headerlink" title="2. selector绑定channel，开始工作"></a><strong>2. selector绑定channel，开始工作</strong></h2><ul>
<li><p>首先注册一个OP_ACCEPT事件<br><code>serverChannel.register(selector, SelectionKey.OP_ACCEPT)</code></p>
</li>
<li><p>等待客户端的连接<br><code>val ready = nioSelector.select(500)</code></p>
</li>
<li><p>采用Round Robin的方式将连接分配给processor进行处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if(ready &gt; 0) &#123;</span><br><span class="line">    val keys = selector.selectedKeys()</span><br><span class="line">    val iter = keys.iterator()</span><br><span class="line">    while(iter.hasNext &amp;&amp; isRunning) &#123;</span><br><span class="line">      var key: SelectionKey = null</span><br><span class="line">      key = iter.next</span><br><span class="line">      iter.remove()</span><br><span class="line">      if(key.isAcceptable)</span><br><span class="line">        accept(key, processors(currentProcessor))</span><br><span class="line"></span><br><span class="line">      // round robin to the next processor thread</span><br><span class="line">      currentProcessor = (currentProcessor + 1) % processors.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>接受客户端的连接，并将获得的SocketChannel交给processor</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def accept(key: SelectionKey, processor: Processor) &#123;</span><br><span class="line">  //根据SelectionKey获取对应的serverSocketChannel</span><br><span class="line">  val serverSocketChannel = key.channel().asInstanceOf[ServerSocketChannel]</span><br><span class="line">  //调用accept方法建立与客户端的连接</span><br><span class="line">  val socketChannel = serverSocketChannel.accept()</span><br><span class="line">  socketChannel.configureBlocking(false)</span><br><span class="line">  socketChannel.socket().setTcpNoDelay(true)</span><br><span class="line">  socketChannel.socket().setSendBufferSize(sendBufferSize)</span><br><span class="line"></span><br><span class="line">  processor.accept(socketChannel)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>sendBufferSize由<code>socket.send.buffer.bytes</code>控制</p>
<p>至此，Acceptor的工作就完成了，接下来处理下一个连接请求。<br><img src="http://olt6kofv9.bkt.clouddn.com/17-3-30/94490923-file_1490860792124_8fc3.png" alt=""></p>
<h1 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a>Processor</h1><p>Processor有两个重要的概念：</p>
<ul>
<li>newConnections：Processor在新收到一个从Acceptor转发过来的SocketChannel时先存放到该链表中，在运行时从链表中拿出一个Channel与客户端的ID(<em>这个ID由本地ip:port+远程ip:port构造</em>)进行绑定，并注册<code>OP_READ</code></li>
<li>inflightResponses：用于存放正在发送或者等待发送的服务器响应报文，如果成功发送了就移除掉</li>
</ul>
<blockquote>
<p>processor的运行模式跟客户端的NetworkClient相似<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while(isRunning)&#123;</span><br><span class="line">    configureNewConnections()</span><br><span class="line">    processNewResponses()</span><br><span class="line">    poll()</span><br><span class="line">    processCompletedReceives()</span><br><span class="line">    processCompletedSends()</span><br><span class="line">    processDisconnected()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>其中Selector的poll方法是跟客户端共用的。</p>
<h2 id="processCompletedReceives"><a href="#processCompletedReceives" class="headerlink" title="processCompletedReceives"></a><strong>processCompletedReceives</strong></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private def processCompletedReceives() &#123;</span><br><span class="line">    selector.completedReceives.asScala.foreach &#123; receive =&gt;</span><br><span class="line">        val openChannel = selector.channel(receive.source)</span><br><span class="line">        val session = &#123;</span><br><span class="line">          val channel = if (openChannel != null) openChannel else selector.closingChannel(receive.source)</span><br><span class="line">          RequestChannel.Session(new KafkaPrincipal(KafkaPrincipal.USER_TYPE, channel.principal.getName), channel.socketAddress)</span><br><span class="line">        &#125;</span><br><span class="line">        val req = RequestChannel.Request(processor = id, connectionId = receive.source, session = session,</span><br><span class="line">          buffer = receive.payload, startTimeMs = time.milliseconds, listenerName = listenerName,</span><br><span class="line">          securityProtocol = securityProtocol)</span><br><span class="line">        requestChannel.sendRequest(req)</span><br><span class="line">        selector.mute(receive.source)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>completedReceives</code>中存放的是客户端的请求(NetworkReceive)</li>
<li>receive.source指的是客户端ID</li>
<li>selector的mute操作会将取消对应channel的<code>OP_READ</code>注册，这一点与客户端不同，client在与server建立连接后，channel上的<code>OP_READ</code>状态会一直保持着，因为client无法预知server的数据会在什么时候到来。而server则是在建立连接后，注册<code>OP_READ</code>，在收到client的请求后取消读。</li>
<li>server根据收到的请求，组建了一个响应的包,然后放到了<code>requestChannel</code>中。</li>
</ul>
<h2 id="processCompletedSends"><a href="#processCompletedSends" class="headerlink" title="processCompletedSends"></a><strong>processCompletedSends</strong></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private def processCompletedSends() &#123;</span><br><span class="line">    selector.completedSends.asScala.foreach &#123; send =&gt;</span><br><span class="line">      val resp = inflightResponses.remove(send.destination).getOrElse &#123;</span><br><span class="line">        throw new IllegalStateException(s&quot;Send for $&#123;send.destination&#125; completed, but not in `inflightResponses`&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      resp.request.updateRequestMetrics()</span><br><span class="line">      selector.unmute(send.destination)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>completedSends</code>存放的是已经成功发送的响应，因此需要将其从<em>inflightResponses</em>中移除</li>
<li><em>selector.unmute</em>方法能将channel注册<code>OP_READ</code></li>
</ul>
<p><strong>那么问题来了：放到<em>requestChannel</em>中的request和response是如何被处理的呢？</strong></p>
<p>我们先来看下<code>requestChannel</code>：<br>每个服务端只存在一个<em>RequestChannel</em>，用于连接Processor与Handler，他定义了两个阻塞队列：</p>
<ul>
<li>requestQueue：缓存服务端发送的request，大小为<code>queued.max.requests</code></li>
<li>responseQueues：每个processor都有一个队列<br><img src="http://olt6kofv9.bkt.clouddn.com/17-4-1/72599292-file_1491031322900_42ac.png" alt=""></li>
</ul>
<h1 id="KafkaRequestHandler"><a href="#KafkaRequestHandler" class="headerlink" title="KafkaRequestHandler"></a><strong>KafkaRequestHandler</strong></h1><p>SocketServer在初始化阶段会创建一个Handler的线程池：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, time,</span><br><span class="line">          config.numIoThreads)</span><br></pre></td></tr></table></figure></p>
<p>KafkaRequestHandler的数量由<code>num.io.threads</code>控制，默认为8。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val runnables = new Array[KafkaRequestHandler](numThreads)</span><br><span class="line">for(i &lt;- 0 until numThreads) &#123;</span><br><span class="line">  runnables(i) = new KafkaRequestHandler(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis, time)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>他的run方法也非常简单，就是从RequestChannel中获取request，然后调用<em>KafkaApis</em>对请求进行处理：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">req = requestChannel.receiveRequest(300)</span><br><span class="line">apis.handle(req)</span><br></pre></td></tr></table></figure></p>
<p><em>KafkaApis</em>在处理完成后，会调用<code>sendResponse</code>方法。</p>
<h2 id="ChannelBuilders"><a href="#ChannelBuilders" class="headerlink" title="ChannelBuilders"></a>ChannelBuilders</h2><p>SocketServer中的<code>selector</code>初始化过程：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private val selector = new KSelector(</span><br><span class="line">  maxRequestSize,</span><br><span class="line">  connectionsMaxIdleMs,</span><br><span class="line">  metrics,</span><br><span class="line">  time,</span><br><span class="line">  &quot;socket-server&quot;,</span><br><span class="line">  metricTags,</span><br><span class="line">  false,</span><br><span class="line">  ChannelBuilders.create(protocol, Mode.SERVER, LoginType.SERVER, channelConfigs, null, true))</span><br></pre></td></tr></table></figure></p>
<p>其中ChannelBuilders是根据安全策略来创建<code>KafkaChannel</code>的</p>
<h3 id="create-channel-using-protocole"><a href="#create-channel-using-protocole" class="headerlink" title="create channel using protocole"></a>create channel using protocole</h3><p>首先来看下第一个参数：<code>protocol</code>，该参数是由socketServer初始化时创建Processor的时候根据<code>EndPoint</code>中声明的protocolType决定的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">endpoints.values.foreach &#123; endpoint =&gt;</span><br><span class="line">  val protocol = endpoint.protocolType</span><br><span class="line">  val processorEndIndex = processorBeginIndex + numProcessorThreads</span><br><span class="line"></span><br><span class="line">  for (i &lt;- processorBeginIndex until processorEndIndex)</span><br><span class="line">    processors(i) = newProcessor(i, connectionQuotas, protocol)</span><br></pre></td></tr></table></figure></p>
<p>如这个<code>listeners=PLAINTEXT://10.45.4.9:9093,SASL_PLAINTEXT://10.45.4.9:9094</code>定义了两个EndPoint：第一个endpoint的protocol是PLAINTEXT即明文无安全验证，第二个是使用SASL_PLAINTEXT安全验证的。每个endpoint都会创建一定数量(由<code>num.network.threads</code>控制)的processor。</p>
<p>下面再来看看创建Channel的主要逻辑：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">switch (securityProtocol) &#123;</span><br><span class="line">    case SSL:</span><br><span class="line">        ...</span><br><span class="line">    case SASL_SSL:</span><br><span class="line">    case SASL_PLAINTEXT:</span><br><span class="line">        requireNonNullMode(mode, securityProtocol);</span><br><span class="line">        if (loginType == null)</span><br><span class="line">            throw new IllegalArgumentException(&quot;`loginType` must be non-null if `securityProtocol` is `&quot; + securityProtocol + &quot;`&quot;);</span><br><span class="line">        if (mode == Mode.CLIENT &amp;&amp; clientSaslMechanism == null)</span><br><span class="line">            throw new IllegalArgumentException(&quot;`clientSaslMechanism` must be non-null in client mode if `securityProtocol` is `&quot; + securityProtocol + &quot;`&quot;);</span><br><span class="line">        channelBuilder = new SaslChannelBuilder(mode, loginType, securityProtocol, clientSaslMechanism, saslHandshakeRequestEnable);</span><br><span class="line">        break;</span><br><span class="line">    case PLAINTEXT:</span><br><span class="line">        channelBuilder = new PlaintextChannelBuilder();</span><br><span class="line">        break;</span><br><span class="line">    default:</span><br><span class="line">        throw new IllegalArgumentException(&quot;Unexpected securityProtocol &quot; + securityProtocol);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里主要关心的是SASL_PLAINTEXT以及PLAINTEXT的处理。</p>
<p>当<strong>security.protocol=SASL_PLAINTEXT时Client端创建的Channel中的saslMechanism不能为空</strong>。可以看到上面socketServer创建Channel时传过来的saslMechanism字段为空。我们再来看看producer/consumer创建Channel时的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">String clientSaslMechanism = (String) configs.get(SaslConfigs.SASL_MECHANISM);</span><br><span class="line">return ChannelBuilders.create(securityProtocol, Mode.CLIENT, LoginType.CLIENT, configs, clientSaslMechanism, true);</span><br></pre></td></tr></table></figure></p>
<p>客户端<code>sasl.mechanism</code>默认值为GSSAPI。KafkaChannel构造函数包含四个属性： </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private final String id;</span><br><span class="line">private final TransportLayer transportLayer;</span><br><span class="line">private final Authenticator authenticator;</span><br><span class="line">private final int maxReceiveSize;</span><br></pre></td></tr></table></figure>
<p>当security.protocol=SASL_PLAINTEXT || PLAINTEXT时，用的都是<code>PlaintextTransportLayer</code>，该类中包含一个Principal：<code>ANONYMOUS</code>，如果kafka开启了权限控制，那么当不安全(security.protocol=PLAINTEXT)的client连接不安全的endpoint时需要为<code>ANONYMOUS</code>用户分配访问权限。<br>Channel的验证工具有两种对应于client和server</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Note left of SaslClient: SEND_HANDSHAKE_REQUEST</span><br><span class="line">Note right of SaslServer: HANDSHAKE_REQUEST</span><br><span class="line">SaslClient-&gt;SaslServer: SaslHandshakeRequest (ApiVersion=*,correlationId=*,clientMechanism=*)</span><br><span class="line">Note left of SaslClient: RECEIVE_HANDSHAKE_RESPONSE</span><br><span class="line">Note left of SaslServer: check clientMechanism is enabled</span><br><span class="line">SaslServer-&gt;SaslClient: SaslHandshakeResponse</span><br><span class="line">Note right of SaslClient: check error code in response</span><br><span class="line">Note left of SaslClient: FAILED or INITAL</span><br><span class="line">Note right of SaslServer: AUTHENICATE</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="http://olt6kofv9.bkt.clouddn.com/18-4-4/21649873.jpg"
                alt="fbZhu" />
            
              <p class="site-author-name" itemprop="name">fbZhu</p>
              <p class="site-description motion-element" itemprop="description">人为什么越长大越孤单？
答:内心中有秘密,无法诉说
</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">fbZhu</span>

  
</div>






  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  








  
  





  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_sphere.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("cea0JXdngKbekqyUcytEll8T-gzGzoHsz", "4K0JxrNpvBK8dDrqbkSm4axL");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  

</body>
</html>
